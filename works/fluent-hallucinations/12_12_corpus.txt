Introduction: Why Data Science Needs Feminism

Christine Mann Darden first passed through the gates of NASA’s Langley Research Cen-
ter in Hampton, Virginia, in the summer of 1967. Her newly minted master’s degree in
applied math had earned her a position as a data analyst there. In the city of Hampton,
and across the United States, tensions were running high. In Los Angeles, a massive
protest against the Vietnam War ended only when more than a thousand armed police
officers attacked the peaceful protestors. One month later, even more violence engulfed
the city of Detroit after a police raid spiraled out of control. The 1967 Detroit Riot, or
the 1967 Detroit Rebellion, as it is increasingly known, resulted in over forty deaths
and one thousand injuries.
   The gates of Langley might have shielded Darden from those physical confronta-
tions, but her work there was no less removed from the national stage. By 1967, the
space race was well underway, and the United States was losing. The Soviet Union had
already sent a man into space and a rocket to the moon. The only thing standing in
the way of a Soviet victory was to put those two pieces together. Meanwhile, the United
States had suffered a series of defeats—­and, in January of that year, an outright disas-
ter, when a sudden fire during a launch test of the Apollo 1 spacecraft killed all three
astronauts on board.
   While the nation mourned, everyone at NASA threw themselves back into their
work—­including Darden, who began her data analyst job in the immediate aftermath
of the Apollo 1 disaster. Two years later, it would be her precise analysis of the physics of
rocket reentry that would help to ensure the successful return of the Apollo 11 mission
from the moon, effectively winning the space race for the United States. But it would
be Darden herself, as a Black woman with technical expertise, working at a federal
agency in which sexism and racism openly prevailed, who demonstrated that the ideo-
logical mission of the United States—­as a land based on the ideals of liberty, equality,
and opportunity for all—­was far from accomplished (figure 0.1).
    The 1960s, after all, were years of social protest and transformation as well as explo-
ration into outer space. Darden herself had participated in several lunch-­counter sit-­
ins at the Hampton Institute, the historically Black college that she attended for her
undergraduate studies.1 By the time that Darden joined NASA, its Virginia facility had
been officially desegregated for several years. But it had yet to reckon with the unof-
ficial segregation of both race and gender that remained in place—­particularly among
its women employees known as computers.
    Darden’s arrival at Langley coincided with the early days of digital computing.
Although Langley could claim one of the most advanced computing systems of the
time—­an IBM 704, the first computer to support floating-­point math—­its resources
were still limited. For most data analysis tasks, Langley’s Advanced Computing Division
relied upon human computers like Darden herself. These computers were all women,
trained in math or a related field, and tasked with performing the calculations that
determined everything from the best wing shape for an airplane, to the best flight path
to the moon. But despite the crucial roles they played in advancing this and other
NASA research, they were treated like unskilled temporary workers. They were brought
into research groups on a project-­by-­project basis, often without even being told any-
thing about the source of the data they were asked to analyze. Most of the engineers,
who were predominantly men, never even bothered to learn the computers’ names.
   These women computers have only recently begun to receive credit for their crucial
work, thanks to scholars of the history of computing—­and to journalists like Margot
Lee Shetterly, whose book, Hidden Figures: The American Dream and the Untold Story of the
Black Women Who Helped Win the Space Race, along with its film adaptation, is respon-
sible for bringing Christine Darden’s story into the public eye.2 Her story, like those
of her colleagues, is one of hard work under discriminatory conditions. Each of these
women computers was required to advocate for herself—­and some, like Darden, chose
also to advocate for others. It is because of both her contributions to data science and
her advocacy for women that we have chosen to begin our book, Data Feminism, with
Darden’s story. For feminism begins with a belief in the “political, social, and economic
equality of the sexes,” as the Merriam-­Webster Dictionary defines the term—­as does, for
the record, Beyoncé.3 And any definition of feminism also necessarily includes the
activist work that is required to turn that belief into reality. In Data Feminism, we bring
these two aspects of feminism together, demonstrating a way of thinking about data,
their analysis, and their display, that is informed by this tradition of feminist activism
as well as the legacy of feminist critical thought.
   As for Darden, she did not only apply her skills of data analysis to spaceflight tra-
jectories; she also applied them to her own career path. After working at Langley for a
number of years, she began to notice two distinct patterns in her workplace: men with
math credentials were placed in engineering positions, where they could be promoted
through the ranks of the civil service, while women with the same degrees were sent
to the computing pools, where they languished until they retired or quit. She did not
want to become one of those women, nor did she want others to experience the same
fate. So she gathered up her courage and decided to approach the chief of her division
to ask him why. As Darden, now seventy-­five, told Shetterly in an interview for Hidden
Figures, his response was sobering: “Well, nobody’s ever complained,” he told Darden.
“The women seem to be happy doing that, so that’s just what they do.”
   In today’s world, Darden might have gotten her boss fired—­or at least served with
an Equal Employment Opportunity Commission complaint. But at the time that
Darden posed her question, stereotypical remarks about “what women do” were par
for the course. In fact, challenging assumptions about what women could or couldn’t
do—­especially in the workplace—­was the central subject of Betty Friedan’s best-­selling
book, The Feminine Mystique. Published in 1963, The Feminine Mystique is often credited
with starting feminism’s so-­called second wave.4 Fed up with the enforced return to
domesticity following the end of World War II, and inspired by the national conversa-
tion about equality of opportunity prompted by the civil rights movement, women
across the United States began to organize around a wide range of issues, including
reproductive rights and domestic violence, as well as the workplace inequality and
restrictive gender roles that Darden faced at Langley.
    That said, Darden’s specific experience as a Black woman with a full-­time job was
quite different than that of a white suburban housewife—­the central focus of The Femi-
nine Mystique. And when critics rightly called out Friedan for failing to acknowledge the
range of experiences of women in the United States (and abroad), it was women like
Darden, among many others, whom they had in mind. In Feminist Theory: From Margin
to Center, another landmark feminist book published in 1984, bell hooks puts it plainly:
“[Friedan] did not discuss who would be called in to take care of the children and
maintain the home if more women like herself were freed from their house labor and
given equal access with white men to the professions. She did not speak of the needs of
women without men, without children, without homes. She ignored the existence of
all non-­white women and poor white women. She did not tell readers whether it was
more fulfilling to be a maid, a babysitter, a factory worker, a clerk, or a prostitute than
to be a leisure-­class housewife.”
    In other words, Friedan had failed to consider how those additional dimensions
of individual and group identity—­like race and class, not to mention sexuality, abil-
ity, age, religion, and geography, among many others—­intersect with each other to
determine one’s experience in the world. Although this concept—­intersectionality—­did
not have a name when hooks described it, the idea that these dimensions cannot be
examined in isolation from each other has a much longer intellectual history.6 Then,
as now, key scholars and activists were deeply attuned to how the racism embedded
in US culture, coupled with many other forms of oppression, made it impossible to
claim a common experience—­or a common movement—­for all women everywhere.
Instead, what was needed was “the development of integrated analysis and practice
based upon the fact that the major systems of oppression are interlocking.”7 These
words are from the Combahee River Collective Statement, written in 1978 by the
famed Black feminist activist group out of Boston. In this book, we draw heavily from
intersectionality and other concepts developed through the work of Black feminist
scholars and activists because they offer some of the best ways for negotiating this
multidimensional terrain.
   Indeed, feminism must be intersectional if it seeks to address the challenges of the
present moment. We write as two straight, white women based in the United States,
with four advanced degrees and five kids between us. We identify as middle-­class and
cisgender—­meaning that our gender identity matches the sex that we were assigned at
birth. We have experienced sexism in various ways at different points of our lives—­
being women in tech and academia, birthing and breastfeeding babies, and trying to
advocate for ourselves and our bodies in a male-­dominated health care system. But we
haven’t experienced sexism in ways that other women certainly have or that nonbi-
nary people have, for there are many dimensions of our shared identity, as the authors
of this book, that align with dominant group positions. This fact makes it impossible
for us to speak from experience about some oppressive forces—­racism, for example. But
it doesn’t make it impossible for us to educate ourselves and then speak about racism
and the role that white people play in upholding it. Or to challenge ableism and the
role that abled people play in upholding it. Or to speak about class and wealth inequali-
ties and the role that well-­educated, well-­off people play in maintaining those. Or to
believe in the logic of co-­liberation. Or to advocate for justice through equity. Indeed, a
central aim of this book is to describe a form of intersectional feminism that takes the
inequities of the present moment as its starting point and begins its own work by ask-
ing: How can we use data to remake the world?8
   This is a complex and weighty task, and it will necessarily remain unfinished. But
its size and scope need not stop us—­or you, the readers of this book—­from taking addi-
tional steps toward justice. Consider Christine Darden, who, after speaking up to her
division chief, heard nothing from him but radio silence. But then, two weeks later,
she was indeed promoted and transferred to a group focused on sonic boom research.
In her new position, Darden was able to begin directing her own research projects and
collaborate with colleagues of all genders as a peer. Her self-­advocacy serves as a model:
a sustained attention to how systems of oppression intersect with each other, informed
by the knowledge that comes from direct experience. It offers a guide for challenging
power and working toward justice.
Christine Darden would go on to conduct groundbreaking research on sonic boom
minimization techniques, author more than sixty scientific papers in the field of com-
putational fluid dynamics, and earn her PhD in mechanical engineering—­all while “juggling the duties of Girl Scout mom, Sunday school teacher, trips to music lessons,
and homemaker,” Shetterly reports. But even as she ascended the professional ranks,
she could tell that her scientific accomplishments were still not being recognized as
readily as those of her male counterparts; the men, it seemed, received promotions far
more quickly.
    Darden consulted with Langley’s Equal Opportunity Office, where a white woman
by the name of Gloria Champine had been compiling a set of statistics about gen-
der and rank. The data confirmed Darden’s direct experience: that women and men—­
even those with identical academic credentials, publication records, and performance
reviews—­were promoted at vastly different rates. Champine recognized that her data
could support Darden in her pursuit of a promotion and, furthermore, that these data
could help communicate the systemic nature of the problem at hand. Champine visu-
alized the data in the form of a bar chart, and presented the chart to the director of
Darden’s division.9 He was “shocked at the disparity,” Shetterly reports, and Darden
received the promotion she had long deserved.10 Darden would advance to the top
rank in the federal civil service, the first Black woman at Langley to do so. By the time
that she retired from NASA, in 2007, Darden was a director herself.11
    Although Darden’s rise into the leadership ranks at NASA was largely the result of her
own knowledge, experience, and grit, her story is one that we can only tell as a result
of the past several decades of feminist activism and critical thought. It was a national
feminist movement that brought women’s issues to the forefront of US cultural poli-
tics, and the changes brought about by that movement were vast. They included both
the shifting gender roles that pointed Darden in the direction of employment at NASA
and the creation of reporting mechanisms like the one that enabled her to continue
her professional rise. But Darden’s success in the workplace was also, presumably, the
result of many unnamed colleagues and friends who may or may not have considered
themselves feminists. These were the people who provided her with community and
support—­and likely a not insignificant number of casserole dinners—­as she ascended
the government ranks. These types of collective efforts have been made increas-
ingly legible, in turn, because of the feminist scholars and activists whose decades
of work have enabled us to recognize that labor—­emotional as much as physical—­as
such today.
    As should already be apparent, feminism has been defined and used in many ways.
Here and throughout the book, we employ the term feminism as a shorthand for the
diverse and wide-­ranging projects that name and challenge sexism and other forces
of oppression, as well as those which seek to create more just, equitable, and livable
futures. Because of this broadness, some scholars prefer to use the term feminisms,
which clearly signals the range of—­and, at times, the incompatibilities among—­these
various strains of feminist activism and political thought. For reasons of readability, we
choose to use the term feminism here, but our feminism is intended to be just as expan-
sive. It includes the work of regular folks like Darden and Champine, public intellectu-
als like Betty Friedan and bell hooks, and organizing groups like the Combahee River
Collective, which have taken direct action to achieve the equality of the sexes. It also
includes the work of scholars and other cultural critics—­like Kimberlé Crenshaw and
Margot Lee Shetterly, among many more—­who have used writing to explore the social,
political, historical, and conceptual reasons behind the inequality of the sexes that we
face today.
  In the process, these writers and activists have given voice to the many ways in
which today’s status quo is unjust.12 These injustices are often the result of histori-
cal and contemporary differentials of power, including those among men, women,
and nonbinary people, as well as those among white women and Black women, aca-
demic researchers and Indigenous communities, and people in the Global North and
the Global South. Feminists analyze these power differentials so that they can change
them. Such a broad focus—­one that incorporates race, class, ability, and more—­would
have sounded strange to Friedan or to the white women largely credited for leading the
fight for women’s suffrage in the nineteenth century.13 But the reality is that women of
color have long insisted that any movement for gender equality must also consider the
ways in which privilege and oppression are intersectional.
  Because the concept of intersectionality is essential for this whole book, let’s get
a bit more specific. The term was coined by legal theorist Kimberlé Crenshaw in the
late 1980s.14 In law school, Crenshaw had come across the antidiscrimination case of
DeGraffenreid v. General Motors. Emma DeGraffenreid was a Black working mother who
had sought a job at a General Motors factory in her town. She was not hired and sued
GM for discrimination. The factory did have a history of hiring Black people: many
Black men worked in industrial and maintenance jobs there. They also had a history of
hiring women: many white women worked there as secretaries. These two pieces of evi-
dence provided the rationale for the judge to throw out the case. Because the company
did hire Black people and did hire women, it could not be discriminating based on race
or gender. But, Crenshaw wanted to know, what about discrimination on the basis of
race and gender together? This was something different, it was real, and it needed to be
named. Crenshaw not only named the concept, but would go on to explain and elabo-
rate the idea of intersectionality in award-­winning books, papers, and talks.15
  Key to the idea of intersectionality is that it does not only describe the intersect-
ing aspects of any particular person’s identity (or positionalities, as they are sometimes termed).16 It also describes the intersecting forces of privilege and oppression at work
in a given society. Oppression involves the systematic mistreatment of certain groups of
people by other groups. It happens when power is not distributed equally—­when one
group controls the institutions of law, education, and culture, and uses its power to
systematically exclude other groups while giving its own group unfair advantages (or
simply maintaining the status quo).17 In the case of gender oppression, we can point to
the sexism, cissexism, and patriarchy that is evident in everything from political repre-
sentation to the wage gap to who speaks more often (or more loudly) in a meeting.18 In
the case of racial oppression, this takes the form of racism and white supremacy. Other
forms of oppression include ableism, colonialism, and classism. Each has its particular
history and manifests differently in different cultures and contexts, but all involve a
dominant group that accrues power and privilege at the expense of others. Moreover,
these forces of power and privilege on the one hand and oppression on the other mesh
together in ways that multiply their effects.
    The effects of privilege and oppression are not distributed evenly across all individu-
als and groups, however. For some, they become an obvious and unavoidable part of
daily life, particularly for women and people of color and queer people and immigrants:
the list goes on. If you are a member of any or all of these (or other) minoritized groups,
you experience their effects everywhere, shaping the choices you make (or don’t get to
make) each day. These systems of power are as real as rain. But forces of oppression can
be difficult to detect when you benefit from them (we call this a privilege hazard later in
the book). And this is where data come in: it was a set of intersecting systems of power
and privilege that Darden was intent on exposing when she posed her initial question
to her division chief. And it was that same set of intersecting systems of power and
privilege that Darden sought to challenge when she approached Champine. Darden
herself didn’t need any more evidence of the problem she faced; she was already living
it every day.19 But when her experience was recorded as data and aggregated with oth-
ers’ experiences, it could be used to challenge institutional systems of power and have
far broader impact than on her career trajectory alone.
    In this way, Darden models what we call data feminism: a way of thinking about
data, both their uses and their limits, that is informed by direct experience, by a com-
mitment to action, and by intersectional feminist thought. The starting point for data
feminism is something that goes mostly unacknowledged in data science: power is not
distributed equally in the world. Those who wield power are disproportionately elite,
straight, white, able-­bodied, cisgender men from the Global North.20 The work of data
feminism is first to tune into how standard practices in data science serve to reinforce
these existing inequalities and second to use data science to challenge and change the
distribution of power.21 Underlying data feminism is a belief in and commitment to
co-­liberation: the idea that oppressive systems of power harm all of us, that they under-
mine the quality and validity of our work, and that they hinder us from creating true
and lasting social impact with data science.
  We wrote this book because we are data scientists and data feminists. Although we
speak as a “we” in this book, and share certain identities, experiences, and skills, we
have distinct life trajectories and motivations for our work on this project. If we were
sitting with you right now, we would each introduce ourselves by answering the ques-
tion: What brings you here today? Placing ourselves in that scenario, here is what we
would have to say.
Catherine: I am a hacker mama. I spent fifteen years as a freelance software developer
and experimental artist, now professor, working on projects ranging from serendipi-
tous news-­recommendation systems to countercartography to civic data literacy to
making breast pumps not suck. I’m here writing this book because, for one, the hype
around big data and AI is deafeningly male and white and technoheroic and the time is
now to reframe that world with a feminist lens. The second reason I’m here is that my
recent experience running a large, equity-­focused hackathon taught me just how much
people like me—­basically, well-­meaning liberal white people—­are part of the problem
in struggling for social justice. This book is one attempt to expose such workings of
power, which are inside us as much as outside in the world.22
Lauren: I often describe myself as a professional nerd. I worked in software develop-
ment before going to grad school to study English, with a particular focus on early
American literature and culture. (Early means very early—­like, the eighteenth cen-
tury.) As a professor at an engineering school, I now work on research projects that
translate this history into contemporary contexts. For instance, I’m writing a book
about the history of data visualization, employing machine-­learning techniques to
analyze abolitionist newspapers, and designing a haptic recreation of a hundred-­year-­
old visualization scheme that looks like a quilt. Through projects like these, I show
how the rise of the concept of “data” (which, as it turns out, really took off in the
eighteenth century) is closely connected to the rise of our current concepts of gen-
der and race. So one of my reasons for writing this book is to show how the issues of
racism and sexism that we see in data science today are by no means new. The other
reason is to help translate humanistic thinking into practice and, in so doing, create
more opportunities for humanities scholars to engage with activists, organizers, and
communities.23
     We both strongly believe that data can do good in the world. But for it to do so, we
must explicitly acknowledge that a key way that power and privilege operate in the
world today has to do with the word data itself. The word dates to the mid-­seventeenth
century, when it was introduced to supplement existing terms such as evidence and fact.
Identifying information as data, rather than as either of those other two terms, served
a rhetorical purpose.24 It converted otherwise debatable information into the solid basis
for subsequent claims. But what information needs to become data before it can be
trusted? Or, more precisely, whose information needs to become data before it can be
considered as fact and acted upon?25 Data feminism must answer these questions, too.
     The story that begins with Christine Darden entering the gates of Langley, passes
through her sustained efforts to confront the structural oppression she encountered
there, and concludes with her impressive array of life achievements, is a story about
the power of data. Throughout her career, in ways large and small, Darden used data
to make arguments and transform lives. But that’s not all. Darden’s feel-­good biog-
raphy is just as much a story about the larger systems of power that required data—­
rather than the belief in her lived experience—­to perform that transformative work. An
institutional mistrust of Darden’s experiential knowledge was almost certainly a factor
in Champine’s decision to create her bar chart. Champine likely recognized, as did
Darden herself, that she would need the bar chart to be believed.
     In this way, the alliance between Darden and Champine, and their work together,
underscores the flaws and compromises that are inherent in any data-­driven project.
The process of converting life experience into data always necessarily entails a reduc-
tion of that experience—­along with the historical and conceptual burdens of the term.
That Darden and Champine were able to view their work as a success despite these
inherent constraints underscores even more the importance of listening to and learn-
ing from people whose lives and voices are behind the numbers. No dataset or analysis
or visualization or model or algorithm is the result of one person working alone. Data
feminism can help to remind us that before there are data, there are people—­people
who offer up their experience to be counted and analyzed, people who perform that
counting and analysis, people who visualize the data and promote the findings of any
particular project, and people who use the product in the end. There are also, always,
people who go uncounted—­for better or for worse. And there are problems that cannot
be represented—­or addressed—­by data alone. And so data feminism, like justice, must
remain both a goal and a process, one that guides our thoughts and our actions as we
move forward toward our goal of remaking the world.
It took five state-­of-­the-­art IBM System/360 Model 75 machines to guide the Apollo 11
astronauts to the moon. Each was the size of a car and cost $3.5 million dollars. Fast
forward to the present. We now have computers in the form of phones that fit in our
pockets and—­in the case of the 2019 Apple iPhone XR—­can perform more than 140
million more instructions per second than a standard IBM System/360.26 That rate of
change is astounding; it represents an exponential growth in computing capacity (fig-
ure 0.2a). We’ve witnessed an equally exponential growth in our ability to collect and
record information in digital form—­and in the ability to have information collected
about us (figure 0.2b).
(a) The time-­series chart included in the original paper on Moore’s law, published in 1965, which
posited that the number of transistors that could fit on an integrated circuit (and therefore con-
tribute to computing capacity) would double every year. Courtesy of Gordon Moore. (b) Several
years ago, researchers concluded that transistors were approaching their smallest size and that
Moore’s law would not hold. Nevertheless, today’s computing power is what enabled Dr. Katie
Bouman, a postdoctoral fellow at MIT, to contribute to a project that involved processing and
compositing approximately five petabytes of data captured by the Event Horizon Telescope to cre-
ate the first ever image of a black hole. After the publication of this photo in April 2019 showing
her excitement—­as one of the scientists on the large team that worked for years to capture the
image—­Bouman was subsequently trolled and harassed online. Courtesy of Tamy Emma Pepin/
Twitter.
     But the act of collecting and recording data about people is not new at all. From the
registers of the dead that were published by church officials in the early modern era to
the counts of Indigenous populations that appeared in colonial accounts of the Ameri-
cas, data collection has long been employed as a technique of consolidating knowledge
about the people whose data are collected, and therefore consolidating power over
their lives.27 The close relationship between data and power is perhaps most clearly
visible in the historical arc that begins with the logs of people captured and placed
aboard slave ships, reducing richly lived lives to numbers and names. It passes through
the eugenics movement, in the late nineteenth and early twentieth centuries, which
sought to employ data to quantify the superiority of white people over all others. It
continues today in the proliferation of biometrics technologies that, as sociologist Sim-
one Browne has shown, are disproportionately deployed to surveil Black bodies.28
     When Edward Snowden, the former US National Security Agency contractor, leaked
his cache of classified documents to the press in 2013, he revealed the degree to which
the federal government routinely collects data on its citizens—­often with minimal
regard to legality or ethics.29 At the municipal level, too, governments are starting to
collect data on everything from traffic movement to facial expressions in the interests
of making cities “smarter.”30 This often translates to reinscribing traditional urban pat-
terns of power such as segregation, the overpolicing of communities of color, and the
rationing of ever-­scarcer city services.31
     But the government is not alone in these data-­collection efforts; corporations do it
too—­with profit as their guide. The words and phrases we search for on Google, the
times of day we are most active on Facebook, and the number of items we add to our
Amazon carts are all tracked and stored as data—­data that are then converted into
corporate financial gain. The most trivial of everyday actions—­searching for a way
around traffic, liking a friend’s cat video, or even stepping out of our front doors in
the morning—­are now hot commodities. This is not because any of these actions are
exceptionally interesting (although we do make an exception for Catherine’s cats) but
because these tiny actions can be combined with other tiny actions to generate targeted
advertisements and personalized recommendations—­in other words, to give us more
things to click on, like, or buy.32
     This is the data economy, and corporations, often aided by academic researchers,
are currently scrambling to see what behaviors—­both online and off—­remain to be
turned into data and then monetized. Nothing is outside of datafication, as this process
is sometimes termed—­not your search history, or Catherine’s cats, or the butt that
Lauren is currently using to sit in her seat. To wit: Shigeomi Koshimizu, a Tokyo-­based
professor of engineering, has been designing matrices of sensors that collect data at 360
different positions around a rear end while it is comfortably ensconced in a chair.33 He
proposes that people have unique butt signatures, as unique as their fingerprints. In the
future, he suggests, our cars could be outfitted with butt-­scanners instead of keys or car
alarms to identify the driver.
   Although datafication may occasionally verge into the realm of the absurd, it
remains a very serious issue. Decisions of civic, economic, and individual impor-
tance are already and increasingly being made by automated systems sifting through
large amounts of data. For example, PredPol, a so-­called predictive policing company
founded in 2012 by an anthropology professor at the University of California, Los
Angeles, has been employed by the City of Los Angeles for nearly a decade to determine
which neighborhoods to patrol more heavily, and which neighborhoods to (mostly)
ignore. But because PredPol is based on historical crime data and US policing practices
have always disproportionately surveilled and patrolled neighborhoods of color, the
predictions of where crime will happen in the future look a lot like the racist practices
of the past.34 These systems create what mathematician and writer Cathy O’Neil, in
Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy,
calls a “pernicious feedback loop,” amplifying the effects of racial bias and of the crimi-
nalization of poverty that are already endemic to the United States.
   O’Neil’s solution is to open up the computational systems that produce these racist
results. Only by knowing what goes in, she argues, can we understand what comes out.
This is a key step in the project of mitigating the effects of biased data. Data feminism
additionally requires that we trace those biased data back to their source. PredPol and
the “three most objective data points” that it employs certainly amplify existing biases,
but they are not the root cause.35 The cause, rather, is the long history of the crimi-
nalization of Blackness in the United States, which produces biased policing practices,
which produce biased historical data, which are then used to develop risk models for
the future.36 Tracing these links to historical and ongoing forces of oppression can help
us answer the ethical question, Should this system exist?37 In the case of PredPol, the
answer is a resounding no.
   Understanding this long and complicated chain reaction is what has motivated
Yeshimabeit Milner, along with Boston-­based activists, organizers, and mathemati-
cians, to found Data for Black Lives, an organization dedicated to “using data science
to create concrete and measurable change in the lives of Black communities.”38 Groups
like the Stop LAPD Spying coalition are using explicitly feminist and antiracist meth-
ods to quantify and challenge invasive data collection by law enforcement.39 Data
journalists are reverse-­engineering algorithms and collecting qualitative data at scale
about maternal harm.40 Artists are inviting participants to perform ecological maps and
using AI for making intergenerational family memoirs (figure 0.3a).41
     All these projects are data science. Many people think of data as numbers alone,
but data can also consist of words or stories, colors or sounds, or any type of infor-
mation that is systematically collected, organized, and analyzed (figures 0.3b, 0.3c).42
The science in data science simply implies a commitment to systematic methods of
observation and experiment. Throughout this book, we deliberately place diverse data
science examples alongside each other. They come from individuals and small groups,
and from across academic, artistic, nonprofit, journalistic, community-­based, and for-­
profit organizations. This is due to our belief in a capacious definition of data sci-
ence, one that seeks to include rather than exclude and does not erect barriers based
on formal credentials, professional affiliation, size of data, complexity of technical
methods, or other external markers of expertise. Such markers, after all, have long
been used to prevent women from fully engaging in any number of professional fields,
even as those fields—­which include data science and computer science, among many
others—­were largely built on the knowledge that women were required to teach them-
selves.43 An attempt to push back against this gendered history is foundational to data
feminism, too.
     Throughout its own history, feminism has consistently had to work to convince the
world that it is relevant to people of all genders. We make the same argument: that
data feminism is for everybody. (And here we borrow a line from bell hooks.)44 You will
notice that the examples we use are not only about women, nor are they created only
by women. That’s because data feminism isn’t only about women. It takes more than one
gender to have gender inequality and more than one gender to work toward justice.
Likewise, data feminism isn’t only for women. Men, nonbinary, and genderqueer people
are proud to call themselves feminists and use feminist thought in their work. More-
over, data feminism isn’t only about gender. Intersectional feminists have keyed us into
how race, class, sexuality, ability, age, religion, geography, and more are factors that
together influence each person’s experience and opportunities in the world. Finally,
data feminism is about power—­about who has it and who doesn’t. Intersectional feminism
examines unequal power. And in our contemporary world, data is power too. Because
the power of data is wielded unjustly, it must be challenged and changed.

We define data science expansively in this book—­here are three examples. (a) Not the Only One by
Stephanie Dinkins (2017), is a sculpture that features a Black family through the use of artificial
intelligence. The AI is trained and taught by the underrepresented voices of Black and brown indi-
viduals in the tech sector. (b) Researcher Margaret Mitchell and colleagues, in “Seeing through the
Human Reporting Bias” (2016), have worked on systems to infer what is not said in human speech
for the purposes of image classification. For example, people say “green bananas” but not “yel-
low bananas” because yellow is implied as the default color of the banana. Similarly, people say
“woman doctor” but do not say “man doctor,” so it is the words that are not spoken that encode
the bias. (c) A gender analysis of Hollywood film dialogue, “Film Dialogue from 2,000 Screenplays
Broken Down by Gender and Age,” by Hanah Anderson and Matt Daniels, created for The Pudding,
a data journalism start-­up (2017).
people. Indeed, a central goal of this book is to show how governments and corpora-
tions have long employed data and statistics as management techniques to preserve an
unequal status quo. Working with data from a feminist perspective requires knowing
and acknowledging this history. To frame the trouble with data in another way: it’s
not a coincidence that the institution that employed Christine Darden and enabled
her professional rise is the same that wielded the results of her data analysis to assert
the technological superiority of the United States over its communist adversaries and
to plant an American flag on the moon. But this flawed history does not mean ceding
control of the future to the powers of the past. Data are part of the problem, to be sure.
But they are also part of the solution. Another central goal of this book is to show how
the power of data can be wielded back.
   To guide us in this work, we have developed seven core principles. Individually
and together, these principles emerge from the foundation of intersectional feminist
thought. Each of the following chapters is structured around a single principle. The
seven principles of data feminism are as follows:

1. Examine power. Data feminism begins by analyzing how power operates in the
   world.
2. Challenge power. Data feminism commits to challenging unequal power structures
   and working toward justice.
3. Elevate emotion and embodiment. Data feminism teaches us to value multiple
     forms of knowledge, including the knowledge that comes from people as living,
     feeling bodies in the world.
4. Rethink binaries and hierarchies. Data feminism requires us to challenge the gen-
     der binary, along with other systems of counting and classification that perpetuate
     oppression.
5. Embrace pluralism. Data feminism insists that the most complete knowledge comes
     from synthesizing multiple perspectives, with priority given to local, Indigenous,
     and experiential ways of knowing.
6. Consider context. Data feminism asserts that data are not neutral or objective. They
     are the products of unequal social relations, and this context is essential for conduct-
     ing accurate, ethical analysis.
7. Make labor visible. The work of data science, like all work in the world, is the work
     of many hands. Data feminism makes this labor visible so that it can be recognized
     and valued.

     Each of the following chapters takes up one of these principles, drawing upon exam-
ples from the field of data science, expansively defined, to show how that principle
can be put into action. Along the way, we introduce key feminist concepts like the
matrix of domination (Patricia Hill Collins; see chapter 1), situated knowledge (Donna
Haraway; see chapter 3), and emotional labor (Arlie Hochschild; see chapter 8), as well
as some of our own ideas about what data feminism looks like in theory and practice.
To this end, we introduce you to people at the cutting edge of data and justice. These
include engineers and software developers, activists and community organizers, data
journalists, artists, and scholars. This range of people, and the range of projects they
have helped to create, is our way of answering the question: What makes a project
feminist? As will become clear, a project may be feminist in content, in that it challenges
power by choice of subject matter; in form, in that it challenges power by shifting the
aesthetic and/or sensory registers of data communication; and/or in process, in that it
challenges power by building participatory, inclusive processes of knowledge produc-
tion. What unites this broad scope of data-­based work is a commitment to action and
a desire to remake the world.
     Our overarching goal is to take a stand against the status quo—­against a world that
benefits us, two white college professors, at the expense of others. To work toward
this goal, we have chosen to feature the voices of those who speak from the margins,
whether because of their gender, sexuality, race, ability, class, geographic location, or
any combination of those (and other) subject positions. We have done so, moreover,
because of our belief that those with direct experience of inequality know better than
we do about what actions to take next. For this reason, we have attempted to prioritize
the work of people in closer proximity to issues of inequality over those who study
inequality from a distance. In this book, we pay particular attention to inequalities
at the intersection of gender and race. This reflects our location in the United States,
where the most entrenched issues of inequality have racism at their source. Our val-
ues statement, included as an appendix to this book, discusses the rationale for these
authorial choices in more detail.
   Any book involves making choices about whose voices and whose work to include
and whose voices and work to omit. We ask that those who find their perspectives
insufficiently addressed or their work insufficiently acknowledged view these gaps as
additional openings for conversation. Our sincere hope is to contribute in a small way
to a much larger conversation, one that began long before we embarked upon this writ-
ing process and that will continue long after these pages are through.
   This book is intended to provide concrete steps to action for data scientists seeking
to learn how feminism can help them work toward justice, and for feminists seeking to
learn how their own work can carry over to the growing field of data science. It is also
addressed to professionals in all fields in which data-­driven decisions are being made,
as well as to communities that want to resist or mobilize the data that surrounds them.
It is written for everyone who seeks to better understand the charts and statistics that
they encounter in their day-­to-­day lives, and for everyone who seeks to communicate
the significance of such charts and statistics to others.
   Our claim, once again, is that data feminism is for everyone. It’s for people of all
genders. It’s by people of all genders. And most importantly: it’s about much more than
gender. Data feminism is about power, about who has it and who doesn’t, and about
how those differentials of power can be challenged and changed using data. We invite
you, the readers of this book, to join us on this journey toward justice and toward
remaking our data-­driven world.
Principle: Examine Power

Data feminism begins by analyzing how power operates in the world.

When tennis star Serena Williams disappeared from Instagram in early September 2017,
her six million followers assumed they knew what had happened. Several months ear-
lier, in March of that year, Williams had accidentally announced her pregnancy to the
world via a bathing suit selfie and a caption that was hard to misinterpret: “20 weeks.”
Now, they thought, her baby had finally arrived.
    But then they waited, and waited some more. Two weeks later, Williams finally
reappeared, announcing the birth of her daughter and inviting her followers to watch
a video that welcomed Alexis Olympia Ohanian Jr. to the world.1 The video was a mon-
tage of baby bump pics interspersed with clips of a pregnant Williams playing tennis
and having cute conversations with her husband, Reddit cofounder Alexis Ohanian,
and then, finally, the shot that her fans had been waiting for: the first clip of baby
Olympia. Williams was narrating: “So we’re leaving the hospital,” she explains. “It’s
been a long time. We had a lot of complications. But look who we got!” The scene fades
to white, and the video ends with a set of stats: Olympia’s date of birth, birth weight,
and number of grand slam titles: 1. (Williams, as it turned out, was already eight weeks
pregnant when she won the Australian Open earlier that year.)
    Williams’s Instagram followers were, for the most part, enchanted. But soon, the
enthusiastic congratulations were superseded by a very different conversation. A num-
ber of her followers—­many of them Black women like Williams herself—­fixated on the
comment she’d made as she was heading home from the hospital with her baby girl.
Those “complications” that Williams experienced—­other women had had them too.
In Williams’s case, the complications had been life-­threatening, and her self-­advocacy
in the hospital played a major role in her survival.
     On Williams’s Instagram feed, dozens of women began posting their own experi-
ences of childbirth gone horribly wrong. A few months later, Williams returned to
social media—­Facebook, this time—­to continue the conversation (figure 1.1). Citing a
2017 statement from the US Centers for Disease Control and Prevention (CDC), Wil-
liams wrote that “Black women are over 3 times more likely than white women to die
from pregnancy-­or childbirth-­related causes.”2
     These disparities were already well-­known to Black-­women-­led reproductive jus-
tice groups like SisterSong, the Black Mamas Matter Alliance, and Raising Our Sisters
Everywhere (ROSE), some of whom had been working on the maternal health cri-
sis for decades. Williams helped to shine a national spotlight on them. The main-
stream media also recently had begun to pay more attention to the crisis as well. A few
months earlier, Nina Martin of the investigative journalism outfit ProPublica, work-
ing with Renee Montagne of NPR, had reported on the same phenomenon.3 “Noth-
ing Protects Black Women from Dying in Pregnancy and Childbirth,” the headline
read. In addition to the study cited by Williams, Martin and Montagne cited a sec-
ond study from 2016, which showed that neither education nor income level—­the
factors usually invoked when attempting to account for healthcare outcomes that
diverge along racial lines—­impacted the fates of Black women giving birth.4 On the
contrary, the data showed that Black women with college degrees suffered more severe
complications of pregnancy and childbirth than white women without high school
diplomas.
   So what were these complications, more precisely? And how many women had
actually died as a result? Nobody was counting. A 2014 United Nations report, coau-
thored by SisterSong, described the state of data collection on maternal mortality in
the United States as “particularly weak.”5 The situation hadn’t improved in 2017, when
ProPublica began its reporting. In 2018, USA Today investigated these racial disparities,
and found what was an even more fundamental problem: there was still no national
system for tracking complications sustained in pregnancy and childbirth, even though
similar systems had long been in place for tracking any number of other health issues,
such as teen pregnancy, hip replacements, or heart attacks.6 They also found that there
was still no reporting mechanism for ensuring that hospitals follow national safety
standards, as is required for both hip surgery and cardiac care. “Our maternal data
is embarrassing,” stated Stacie Geller, a professor of obstetrics and gynecology at the
University of Illinois, when asked for comment. The chief of the CDC’s Maternal and
Infant Health branch, William Callaghan, makes the significance of this “embarrass-
ing” data more clear: “What we choose to measure is a statement of what we value in
health,” he explains.7 We might edit his statement to add that it’s a measure of who we
value in health, too.8
   Why did it take the near-­death of an international sports superstar for the media
to begin paying attention to an issue that less famous Black women had been expe-
riencing and organizing around for decades? Why did it take reporting by the pre-
dominantly white mainstream press for US cities and states to begin collecting data on
the issue?9 Why are those data still not viewed as big enough, statistically significant
enough, or of high enough quality for those cities and states, and other public institu-
tions, to justify taking action? And why didn’t those institutions just #believeblack-
women in the first place?10
   The answers to these questions are directly connected to larger issues of power and
privilege. Williams recognized as much when asked by Glamour magazine about the
fact that she had to demand that her medical team perform additional tests in order
to diagnose her own postnatal complications—­and because she was Serena Williams,
twenty-­three-­time grand slam champion, they complied.11 “If I wasn’t who I am, it
could have been me,” she told Glamour, referring to the fact that the privilege she
experienced as a tennis star intersected with the oppression she experienced as a Black
woman, enabling her to avoid becoming a statistic herself. As Williams asserted, “that’s
not fair.”12
     Needless to say, Williams is right. It’s absolutely not fair. So how do we mitigate this
unfairness? We begin by examining systems of power and how they intersect—­like
how the influences of racism, sexism, and celebrity came together first to send Wil-
liams into a medical crisis and then, thankfully, to keep her alive. The complexity of
these intersections is the reason that examine power is the first principle of data femi-
nism, and the focus of this chapter. Examining power means naming and explaining
the forces of oppression that are so baked into our daily lives—­and into our datasets,
our databases, and our algorithms—­that we often don’t even see them. Seeing oppres-
sion is especially hard for those of us who occupy positions of privilege. But once we
identify these forces and begin to understand how they exert their potent force, then
many of the additional principles of data feminism—­like challenging power (chapter 2),
embracing emotion (chapter 3), and making labor visible (chapter 7)—­become easier to
undertake.
But first, what do we mean by power? We use the term power to describe the current
configuration of structural privilege and structural oppression, in which some groups
                               because various systems have been designed by
experience unearned advantages—­
people like them and work for people them—­and other groups experience systematic
disadvantages—­because those same systems were not designed by them or with people
like them in mind. These mechanisms are complicated, and there are “few pure vic-
tims and oppressors,” notes influential sociologist Patricia Hill Collins. In her landmark
text, Black Feminist Thought, first published in 1990, Collins proposes the concept of the
matrix of domination to explain how systems of power are configured and experienced.13
It consists of four domains: the structural, the disciplinary, the hegemonic, and the
interpersonal. Her emphasis is on the intersection of gender and race, but she makes
clear that other dimensions of identity (sexuality, geography, ability, etc.) also result in
unjust oppression, or unearned privilege, that become apparent across the same four
domains.
     The structural domain is the arena of laws and policies, along with schools and insti-
tutions that implement them. This domain organizes and codifies oppression. Take,
for example, the history of voting rights in the United States. The US Constitution did
not originally specify who was authorized to vote, so various states had different poli-
cies that reflected their local politics. Most had to do with owning property, which,
conveniently, only men could do. But with the passage of the Fourteenth Amendment
in 1868, which granted the rights of US citizenship to those who had been enslaved,
the nature of those rights—­including voting—­were required to be spelled out at the
national level for the first time. More specifically, voting was defined as a right reserved
for “male citizens.” This is a clear instance of codified oppression in the structural
domain.
   It would take until the passage of the Nineteenth Amendment in 1920 for most (but
not all) women to be granted the right to vote.14 Even still, many state voting laws con-
tinued to include literacy tests, residency requirements, and other ways to indirectly
exclude people who were not property-­owning white men. These restrictions persist
today, in the form of practices like dropping names from voter rolls, requiring photo
IDs, and limits to early voting—­the burdens of which are felt disproportionately by
low-­income people, people of color, and others who lack the time or resources to jump
through these additional bureaucratic hoops.15 This is the disciplinary domain that Col-
lins names: the domain that administers and manages oppression through bureaucracy
and hierarchy, rather than through laws that explicitly encode inequality on the basis
of someone’s identity.16
   Neither of these domains would be possible without the hegemonic domain, which
deals with the realm of culture, media, and ideas. Discriminatory policies and prac-
tices in voting can only be enacted in a world that already circulates oppressive
ideas about, for example, who counts as a citizen in the first place. Consider an anti-
suffragist pamphlet from the 1910s that proclaims, “You do not need a ballot to clean
out your sink spout.”17 Pamphlets like these, designed to be literally passed from hand
to hand, reinforced preexisting societal views about the place of women in society.
Today, we have animated GIFs instead of paper pamphlets, but the hegemonic func-
tion is the same: to consolidate ideas about who is entitled to exercise power and who
is not.
     The final part of the matrix of domination is the interpersonal domain, which influ-
ences the everyday experience of individuals in the world. How would you feel if you
were a woman who read that pamphlet, for example? Would it have more or less of
an impact if a male family member gave it to you? Or, for a more recent example, how
would you feel if you took time off from your hourly job to go cast your vote, only to
discover when you got there that your name had been purged from the official voting
roll or that there was a line so long that it would require that you miss half a day’s pay,
or stand for hours in the cold, or ... the list could go on. These are examples of how it
feels to know that systems of power are not on your side and, at times, are actively seek-
ing to take away the small amount of power that you do possess.18
     The matrix of domination works to uphold the undue privilege of dominant groups
while unfairly oppressing minoritized groups. What does this mean? Beginning in this
chapter and continuing throughout the book, we use the term minoritized to describe
groups of people who are positioned in opposition to a more powerful social group.
While the term minority describes a social group that is comprised of fewer people,
minoritized indicates that a social group is actively devalued and oppressed by a domi-
nant group, one that holds more economic, social, and political power. With respect
to gender, for example, men constitute the dominant group, while all other genders
constitute minoritized groups. This remains true even as women actually constitute a
majority of the world population. Sexism is the term that names this form of oppres-
sion. In relation to race, white people constitute the dominant group (racism); in rela-
tion to class, wealthy and educated people constitute the dominant group (classism);
and so on.19
     Using the concept of the matrix of domination and the distinction between domi-
nant and minoritized groups, we can begin to examine how power unfolds in and
around data. This often means asking uncomfortable questions: who is doing the work
of data science (and who is not)? Whose goals are prioritized in data science (and whose
are not)? And who benefits from data science (and who is either overlooked or actively
harmed)?20 These questions are uncomfortable because they unmask the inconvenient
truth that there are groups of people who are disproportionately benefitting from data
science, and there are groups of people who are disproportionately harmed. Asking
these who questions allows us, as data scientists ourselves, to start to see how privilege is
baked into our data practices and our data products.21
It is important to acknowledge the elephant in the server room: the demograph-
ics of data science (and related occupations like software engineering and artificial
intelligence research) do not represent the population as a whole. According to the
most recent data from the US Bureau of Labor Statistics, released in 2018, only 26 per-
cent of those in “computer and mathematical occupations” are women.22 And across
all of those women, only 12 percent are Black or Latinx women, even though Black and
Latinx women make up 22.5 percent of the US population.23 A report by the research
group AI Now about the diversity crisis in artificial intelligence notes that women com-
prise only 15 percent of AI research staff at Facebook and 10 percent at Google.24 These
numbers are probably not a surprise. The more surprising thing is that those numbers
are getting worse, not better. According to a research report published by the Ameri-
can Association of University Women in 2015, women computer science graduates in
the United States peaked in the mid-­1980s at 37 percent, and we have seen a steady
decline in the years since then to 26 percent today (figure 1.2).25 As “data analysts”(low-­status number crunchers) have become rebranded as “data scientists” (high status researchers), women are being pushed out in order to make room for more highly val-
ued and more highly compensated men.26
     There are not disparities only along gender lines in the higher education pipeline.
The same report noted specific underrepresentation for Native American women, mul-
tiracial women, white women, and all Black and Latinx people. So is it really a surprise
that each day brings a new example of data science being used to disempower and
oppress minoritized groups? In 2018, it was revealed that Amazon had been developing
an algorithm to screen its first-­round job applicants. But because the model had been
trained on the resumes of prior applicants, who were predominantly male, it developed
an even stronger preference for male applicants. It downgraded resumes with the word
women and graduates of women’s colleges. Ultimately, Amazon had to cancel the proj-
ect.27 This example reinforces the work of Safiya Umoja Noble, whose book, Algorithms
of Oppression, has shown how both gender and racial biases are encoded into some of
the most pervasive data-­driven systems—­including Google search, which boasts over
five billion unique web searches per day. Noble describes how, as recently as 2016,
comparable searches for “three Black teenagers” and “three white teenagers” turned up
wildly different representations of those teens. The former returned mugshots, while
the latter returned wholesome stock photography.28
     The problems of gender and racial bias in our information systems are complex, but
some of their key causes are plain as day: the data that shape them, and the models
designed to put those data to use, are created by small groups of people and then scaled
up to users around the globe. But those small groups are not at all representative of the
globe as a whole, nor even of a single city in the United States. When data teams are
primarily composed of people from dominant groups, those perspectives come to exert
outsized influence on the decisions being made—­to the exclusion of other identities
and perspectives. This is not usually intentional; it comes from the ignorance of being
on top. We describe this deficiency as a privilege hazard.
     How does this come to pass? Let’s take a minute to imagine what life is like for some-
one who epitomizes the dominant group in data science: a straight, white, cisgender
man with formal technical credentials who lives in the United States. When he looks
for a home or applies for a credit card, people are eager for his business. People smile
when he holds his girlfriend’s hand in public. His body doesn’t change due to child-
birth or breastfeeding, so he does not need to think about workplace accommodations.
He presents his social security number in jobs as a formality, but it never hinders his
application from being processed or brings him unwanted attention. The ease with
which he traverses the world is invisible to him because it has been designed for people
just like him. He does not think about how life might be different for everyone else. In
fact, it is difficult for him to imagine that at all.
   This is the privilege hazard: the phenomenon that makes those who occupy the most
privileged positions among us—­those with good educations, respected credentials, and
professional accolades—­so poorly equipped to recognize instances of oppression in the
world.29 They lack what Anita Gurumurthy, executive director of IT for Change, has
called “the empiricism of lived experience.”30 And this lack of lived experience—­this
evidence of how things truly are—­profoundly limits their ability to foresee and prevent
harm, to identify existing problems in the world, and to imagine possible solutions.
   The privilege hazard occurs at the level of the individual—­in the interpersonal
                                   but it is much more harmful in aggregate
domain of the matrix of domination—­
because it reaches the hegemonic, disciplinary and structural domains as well. So it
matters deeply that data science and artificial intelligence are dominated by elite white
men because it means there is a collective privilege hazard so great that it would be a
profound surprise if they could actually identify instances of bias prior to unleashing
them onto the world. Social scientist Kate Crawford has advanced the idea that the
biggest threat from artificial intelligence systems is not that they will become smarter
than humans, but rather that they will hard-­code sexism, racism, and other forms of
discrimination into the digital infrastructure of our societies.31
   What’s more, the same cis het white men responsible for designing those systems
lack the ability to detect harms and biases in their systems once they’ve been released
into the world.32 In the case of the “three teenagers” Google searches, for example,
it was a young Black teenager that pointed out the problem and a Black scholar who
wrote about the problem. The burden consistently falls upon those more intimately
familiar with the privilege hazard—­in data science as in life—­to call out the creators of
those systems for their limitations.
   For example, Joy Buolamwini, a Ghanaian-­American graduate student at MIT, was
working on a class project using facial-­analysis software.33 But there was a problem—­
the software couldn’t “see” Buolamwini’s dark-­skinned face (where “seeing” means
that it detected a face in the image, like when a phone camera draws a square around a
person’s face in the frame). It had no problem seeing her lighter-­skinned collaborators.
She tried drawing a face on her hand and putting it in front of the camera; it detected
that. Finally, Buolamwini put on a white mask, essentially going in “whiteface”
(figure 1.3).34 The system detected the mask’s facial features perfectly.
   Digging deeper into the code and benchmarking data behind these systems, Buol-
amwini discovered that the dataset on which many of facial-­recognition algorithms
are tested contains 78 percent male faces and 84 percent white faces. When she did
an intersectional breakdown of another test dataset—­looking at gender and skin type
together—­only 4 percent of the faces in that dataset were women and dark-­skinned.
In their evaluation of three commercial systems, Buolamwini and computer scientist
Timnit Gebru showed that darker-­skinned women were up to forty-­four times more
likely to be misclassified than lighter-­skinned males.35 It’s no wonder that the software
failed to detect Buolamwini’s face: both the training data and the benchmarking data
relegate women of color to a tiny fraction of the overall dataset.36
     This is the privilege hazard in action—­that no coder, tester, or user of the software
had previously identified such a problem or even thought to look. Buolamwini’s work
has been widely covered by the national media (by the New York Times, by CNN, by
the Economist, by Bloomberg BusinessWeek, and others) in articles that typically contain
a hint of shock.37 This is a testament to the social, political, and technical importance
of the work, as well as to how those in positions of power—­not just in the field of data
science, but in the mainstream media, in elected government, and at the heads of
corporations—­are so often surprised to learn that their “intelligent technologies” are
not so intelligent after all. (They need to read data journalist Meredith Broussard’s book
Artificial Unintelligence).38 For another example, think back to the introduction of this
book, where we quoted Shetterly as reporting that Christine Darden’s white male man-
ager was “shocked at the disparity” between the promotion rates of men and women.
We can speculate that Darden herself wasn’t shocked, just as Buolamwini and Gebru
likely were not entirely shocked at the outcome of their study either. When sexism, rac-
ism, and other forms of oppression are publicly unmasked, it is almost never surprising
to those who experience them.
   For people in positions of power and privilege, issues of race and gender and class
and ability—­to name only a few—­are OPP: other people’s problems. Author and anti-
racist educator Robin DiAngelo describes instances like the “shock” of Darden’s boss or
the surprise in the media coverage of Buolamwini’s various projects as a symptom of
the “racial innocence” of white people.39 In other words, those who occupy positions
of privilege in society are able to remain innocent of that privilege. Race becomes some-
thing that only people of color have. Gender becomes something that only women and
nonbinary people have. Sexual orientation becomes something that all people except
heterosexual people have. And so on. A personal anecdote might help illustrate this
point. When we published the first draft of this book online, Catherine told a colleague
about it. His earnestly enthusiastic response was, “Oh great! I’ll show it to my female
graduate students!” To which Catherine rejoined, “You might want to show it to your
other students, too.”
   If things were different—­if the 79 percent of engineers at Google who are male were
specifically trained in structural oppression before building their data systems (as social
workers are before they undertake social work)—­then their overrepresentation might
be very slightly less of a problem.40 But in the meantime, the onus falls on the indi-
viduals who already feel the adverse effects of those systems of power to prove, over
and over again, that racism and sexism exist—­in datasets, in data systems, and in data
science, as in everywhere else.
   Buolamwini and Gebru identified how pale and male faces were overrepresented in
facial detection training data. Could we just fix this problem by diversifying the data
set? One solution to the problem would appear to be straightforward: create a more
representative set of training and benchmarking data for facial detection models. In
fact, tech companies are starting to do exactly this. In January 2019, IBM released a
database of one million faces called Diversity in Faces (DiF).41 In another example, jour-
nalist Amy Hawkins details how CloudWalk, a startup in China in need of more images
of faces of people of African descent, signed a deal with the Zimbabwean government
for it to provide the images the company was lacking.42 In return for sharing its data,
Zimbabwe will receive a national facial database and “smart” surveillance infrastruc-
ture that it can install in airports, railways, and bus stations.
     It might sound like an even exchange, but Zimbabwe has a dismal record on human
rights. Making things worse, CloudWalk provides facial recognition technologies to
the Chinese police—­a conflict of interest so great that the global nonprofit Human
Rights Watch voiced its concern about the deal.43 Face harvesting is happening in
the US as well. Researchers Os Keyes, Nikki Stevens and Jacqueline Wernimont have
shown how immigrants, abused children, and dead people are some of the groups
whose faces have been used to train software—­without their consent.44 So is a diverse
database of faces really a good idea? Voicing his concerns in response to the announce-
ment of Buolamwini and Gebru’s 2018 study on Twitter, an Indigenous Marine veteran
shot back, “I hope facial recognition software has a problem identifying my face too.
That’d come in handy when the police come rolling around with their facial recogni-
tion truck at peaceful demonstrations of dissent, cataloging all dissenters for ‘safety
and security.’”45
     Better detection of faces of color cannot be characterized as an unqualified good.
More often than not, it is enlisted in the service of increased oppression, greater sur-
veillance, and targeted violence. Buolamwini understands these potential harms and
has developed an approach that works across all four domains of the matrix of domi-
nation to address the underlying issues of power that are playing out in facial analysis
technology. Buolamwini and Gebru first quantified the disparities in the dataset—­a
technical audit, which falls in the disciplinary domain of the matrix of domination.
Then, Buolamwini went on to launch the Algorithmic Justice League, an organization
that works to highlight and intervene in instances of algorithmic bias. On behalf of
the AJL, Buolamwini has produced viral poetry projects and given TED talks—­taking
action in the hegemonic domain, the realm of culture and ideas. She has advised on
legislation and professional standards for the field of computer vision and called for
a moratorium on facial analysis in policing on national media and in Congress.46
These are actions operating in the structural domain of the matrix of domination—­the
realm of law and policy. Throughout these efforts, the AJL works with students and
researchers to help guide and shape their own work—­the interpersonal domain. Taken
together, Buolamwini’s various initiatives demonstrate how any “solution” to bias
in algorithms and datasets must tackle more than technical limitations. In addition,
they present a compelling model for the data scientist as public intellectual—­who,
yes, works on technical audits and fixes, but also works on cultural, legal, and political
efforts too.
     While equitable representation—­in datasets and data science workforces—­is impor-
tant, it remains window dressing if we don’t also transform the institutions that pro-
duce and reproduce those biased outcomes in the first place. As doctoral health student
One of the downstream effects of the privilege hazard—­the risks incurred when people
from dominant groups create most of our data products—­is not only that datasets
are biased or unrepresentative, but that they never get collected at all. Mimi Onu-
oha—­an artist, designer, and educator—­has long been asking who questions about data
science. Her project, The Library of Missing Datasets (figure 1.4), is a list of datasets that
one might expect to already exist in the world, because they help to address pressing
social issues, but that in reality have never been created. The project exists as a web-
site and as an art object. The latter consists of a file cabinet filled with folders labeled
with phrases like: “People excluded from public housing because of criminal records,”
“Mobility for older adults with physical disabilities or cognitive impairments,” and
“Total number of local and state police departments using stingray phone trackers
(IMSI-­catchers).” Visitors can tab through the folders and remove any particular folder
of interest, only to reveal that it is empty. They all are. The datasets that should be there
are “missing.”
     By compiling a list of the datasets that are missing from our “otherwise data-­
saturated” world, Onuoha explains, “we find cultural and colloquial hints of what
is deemed important” and what is not. “Spots that we’ve left blank reveal our hid-
den social biases and indifferences,” she continues. And by calling attention to these
datasets as “missing,” she also calls attention to how the matrix of domination encodes
these “social biases and indifferences” across all levels of society.48 Along similar lines,
foundations like Data2X and books like Invisible Women have advanced the idea of
a systematic “gender data gap” due to the fact that the majority of research data in
scientific studies is based around men’s bodies. The downstream effects of the gender
data gap range from annoying—­cell phones slightly too large for women’s hands, for
example—­to fatal. Until recently, crash test dummies were designed in the size and
shape of men, an oversight that meant that women had a 47 percent higher chance of
car injury than men.49
     The who question in this case is: Who benefits from data science and who is over-
looked? Examining those gaps can sometimes mean calling out missing datasets, as
Onuoha does; characterizing them, as Invisible Women does; and advocating for filling
them, as Data2X does. At other times, it can mean collecting the missing data yourself.
Lacking comprehensive data about women who die in childbirth, for example, Pro-
Publica decided to resort to crowdsourcing to learn the names of the estimated seven
hundred to nine hundred US women who died in 2016.50 As of 2019, they’ve identified
only 140. Or, for another example: in 1998, youth living in Roxbury—­a neighborhood
known as “the heart of Black culture in Boston”51—­were sick and tired of inhaling pol-
luted air. They led a march demanding clean air and better data collection, which led
to the creation of the AirBeat community monitoring project.52
     Scholars have proposed various names for these instances of ground-­up data collec-
tion, including counterdata or agonistic data collection, data activism, statactivism, and
citizen science (when in the service of environmental justice).53 Whatever it’s called,
it’s been going on for a long time. In 1895, civil rights activist and pioneering data
journalist Ida B. Wells assembled a set of statistics on the epidemic of lynching that
was sweeping the United States.54 She accompanied her data with a meticulous exposé
of the fraudulent claims made by white people—­typically, that a rape, theft, or assault
of some kind had occurred (which it hadn’t in most cases) and that lynching was a
justified response. Today, an organization named after Wells—­the Ida B. Wells Society
for Investigative Reporting—­continues her mission by training up a new generation of
journalists of color in the skills of data collection and analysis.55
   A counterdata initiative in the spirit of Wells is taking place just south of the US
border, in Mexico, where a single woman is compiling a comprehensive dataset on
femicides—­gender-­related killings of women and girls.56 María Salguero, who also goes
by the name Princesa, has logged more than five thousand cases of femicide since
2016.57 Her work provides the most accessible information on the subject for journal-
ists, activists, and victims’ families seeking justice.
   The issue of femicide in Mexico rose to global visibility in the mid-­2000s with wide-
spread media coverage about the deaths of poor and working-­class women in Ciudad
Juárez. A border town, Juárez is the site of more than three hundred maquiladoras:
factories that employ women to assemble goods and electronics, often for low wages
and in substandard working conditions. Between 1993 and 2005, nearly four hundred
of these women were murdered, with around a third of those murders exhibiting signs
of exceptional brutality or sexual violence. Convictions were made in only three of
those deaths. In response, a number of activist groups like Ni Una Más (Not One More)
and Nuestras Hijas de Regreso a Casa (Our Daughters Back Home) were formed, largely
motivated by mothers demanding justice for their daughters, often at great personal
risk to themselves.58
   These groups succeeded in gaining the attention of the Mexican government, which
established a Special Commission on Femicide. But despite the commission and the
fourteen volumes of information about femicide that it produced, and despite a 2009
ruling against the Mexican state by the Inter-­American Human Rights Court, and
despite a United Nations Symposium on Femicide in 2012, and despite the fact that
sixteen Latin American countries have now passed laws defining femicide—­despite all
of this, deaths in Juárez have continued to rise.59 In 2009 a report pointed out that one
of the reasons that the issue had yet to be sufficiently addressed was the lack of data.60
Needless to say, the problem remains.
   How might we explain the missing data around femicides in relation to the four
domains of power that constitute Collins’s matrix of domination? As is true in so many
cases of data collected (or not) about women and other minoritized groups, the collec-
tion environment is compromised by imbalances of power.
   The most grave and urgent manifestation of the matrix of domination is within the
interpersonal domain, in which cis and trans women become the victims of violence
and murder at the hands of men. Although law and policy (the structural domain)
have recognized the crime of femicide, no specific policies have been implemented to
ensure adequate information collection, either by federal agencies or local authorities.
Thus the disciplinary domain, in which law and policy are enacted, is characterized by
a deferral of responsibility, a failure to investigate, and victim blaming. This persists in
a somewhat recursive fashion because there are no consequences imposed within the
structural domain. For example, the Special Commission’s definition of femicide as a
“crime of the state” speaks volumes to how the government of Mexico is deeply com-
plicit through inattention and indifference.61
     Of course, this inaction would not have been tolerated without the assistance of
                     the realm of media and culture—­
the hegemonic domain—­                              which presents men as
strong and women as subservient, men as public and women as private, trans people
as deviating from “essential” norms, and nonbinary people as nonexistent altogether.
Indeed, government agencies have used their public platforms to blame victims. Fol-
lowing the femicide of twenty-­two-­year-­old Mexican student Lesvy Osorio in 2017,
                           Dominguez documented how the Public Prosecutor’s
researcher Maria Rodriguez-­
Office of Mexico City shared on social media that the victim was an alcoholic and
drug user who had been living out of wedlock with her boyfriend.62 This led to justi-
fied public backlash, and to the hashtag #SiMeMatan (If they kill me), which prompted
sarcastic tweets such as “#SiMeMatan it’s because I liked to go out at night and drink
a lot of beer.”63
     It is into this data collection environment, characterized by extremely asymmetrical
power relations, that María Salguero has inserted her femicides map. Salguero manu-
ally plots a pin on the map for every femicide that she collects through media reports
or through crowdsourced contributions (figure 1.5a). One of her goals is to “show that
these victims [each] had a name and that they had a life,” and so Salguero logs as
many details as she can about each death. These include name, age, relationship with
the perpetrator, mode and place of death, and whether the victim was transgender, as
well as the full content of the news report that served as the source. Figure 1.5b shows
a detailed view for a single report from an unidentified transfemicide, including the
date, time, location, and media article about the killing. It can take Salguero three to
four hours a day to do this unpaid work. She takes occasional breaks to preserve her
mental health, and she typically has a backlog of a month’s worth of femicides to add
to the map.
     Although media reportage and crowdsourcing are imperfect ways of collecting data,
this particular map, created and maintained by a single person, fills a vacuum created
by her national government. The map has been used to help find missing women, and
Salguero herself has testified before Mexico’s Congress about the scope of the problem.
Salguero is not affiliated with an activist group, but she makes her data available to
activist groups for their efforts. Parents of victims have called her to give their thanks
for making their daughters visible, and Salguero affirms this function as well: “This
map seeks to make visible the sites where they are killing us, to find patterns, to bolster
arguments about the problem, to georeference aid, to promote prevention and try to
avoid femicides.”
     It is important to make clear that the example of missing data about femicides in
Mexico is not an isolated case, either in terms of subject matter or geographic loca-
tion. The phenomenon of missing data is a regular and expected outcome in all societ-
ies characterized by unequal power relations, in which a gendered, racialized order is
maintained through willful disregard, deferral of responsibility, and organized neglect
for data and statistics about those minoritized bodies who do not hold power. So too
are examples of individuals and communities using strategies like Salguero’s to fill in
the gaps left by these missing datasets—­in the United States as around the world.64 If
“quantification is representation,” as data journalist Jonathan Stray asserts, then this
offers one way to hold those in power accountable. Collecting counterdata demon-
strates how data science can be enlisted on behalf of individuals and communities that
need more power on their side.65
Far too often, the problem is not that data about minoritized groups are missing but the
reverse: the databases and data systems of powerful institutions are built on the exces-
sive surveillance of minoritized groups. This results in women, people of color, and
poor people, among others, being overrepresented in the data that these systems are
premised upon. In Automating Inequality, for example, Virginia Eubanks tells the story
of the Allegheny County Office of Children, Youth, and Families in western Pennsyl-
vania, which employs an algorithmic model to predict the risk of child abuse in any
particular home.66 The goal of the model is to remove children from potentially abusive
households before it happens; this would appear to be a very worthy goal. As Eubanks
shows, however, inequities result. For wealthier parents, who can more easily access
private health care and mental health services, there is simply not that much data to
pull into the model. For poor parents, who more often rely on public resources, the
system scoops up records from child welfare services, drug and alcohol treatment pro-
grams, mental health services, Medicaid histories, and more. Because there are far more
data about poor parents, they are oversampled in the model, and so their children
are overtargeted as being at risk for child abuse—­a risk that results in children being
removed from their families and homes. Eubanks argues that the model “confuse[s]
parenting while poor with poor parenting.”
  This model, like many, was designed under two flawed assumptions: (1) that more
data is always better and (2) that the data are a neutral input. In practice, however,
the reality is quite different. The higher proportion of poor parents in the database,
with more complete data profiles, the more likely the model will be to find fault with
poor parents. And data are never neutral; they are always the biased output of unequal
social, historical, and economic conditions: this is the matrix of domination once
again.67 Governments can and do use biased data to marshal the power of the matrix
of domination in ways that amplify its effects on the least powerful in society. In this
case, the model becomes a way to administer and manage classism in the disciplinary
domain—­with the consequence that poor parents’ attempts to access resources and
improve their lives, when compiled as data, become the same data that remove their
children from their care.
     So this raises our next who question: Whose goals are prioritized in data science (and
whose are not)? In this case, the state of Pennsylvania prioritized its bureaucratic goal
of efficiency, which is an oft-­cited reason for coming up with a technical solution to
a social and political dilemma. Viewed from the perspective of the state, there were
simply not enough employees to handle all of the potential child abuse cases, so it
needed a mechanism for efficiently deploying limited staff—­or so the reasoning goes.
This is what Eubanks has described as a scarcity bias: the idea that there are not enough
resources for everyone so we should think small and allow technology to fill the gaps.
Such thinking, and the technological “solutions” that result, often meet the goals
of their creators—­in this case, the Allegheny County Office of Children, Youth, and
Families—­but not the goals of the children and families that it purports to serve.
     Corporations also place their own goals ahead of those of the people their prod-
ucts purport to serve, supported by their outsize wealth and the power that comes
with it. For example, in 2012, the New York Times published an explosive article by
Charles Duhigg, “How Companies Learn Your Secrets,”68 which soon became the stuff
of legend in data and privacy circles. Duhigg describes how Andrew Pole, a data scien-
tist working at Target, was approached by men from the marketing department who
asked, “If we wanted to figure out if a customer is pregnant, even if she didn’t want
us to know, can you do that?”69 He proceeded to synthesize customers’ purchasing
histories with the timeline of those purchases to give each customer a so-­called preg-
nancy prediction score (figure 1.6).70 Evidently, pregnancy is the second major life
event, after leaving for college, that determines whether a casual shopper will become
a customer for life.
     Target turned around and put Pole’s pregnancy detection model into action in an
automated system that sent discount coupons to possibly pregnant customers. Win-­
win—­or so the company thought, until a Minneapolis teenager’s dad saw the coupons
for baby clothes that she was getting in the mail and marched into his local Target to
read the manager the riot act. Why was his daughter getting coupons for pregnant
women when she was only a teen?!
     It turned out that the young woman was indeed pregnant. Pole’s model informed
Target before the teenager informed her family. By analyzing the purchase dates of
approximately twenty-­five common products, such as unscented lotion and large bags
of cotton balls, the model found a set of purchase patterns that were highly correlated
Figure 1.6
Screenshot from a video of statistician Andrew Pole’s presentation at Predictive Analytics World
about Target’s pregnancy detection model in October 2010, titled “How Target Gets the Most out
of Its Guest Data to Improve Marketing ROI.” He discusses the model at 47:50. Image by Andrew
Pole for Predictive Analytics World.
with pregnancy status and expected due date. But the win-­win quickly became a lose-­lose, as Target lost the trust of its customers in a PR disaster and the Minneapolis
teenager lost far worse: her control over information related to her own body and
her health.
   This story has been told many times: first by Pole, the statistician; then by Duhigg,
the New York Times journalist; then by many other commentators on personal privacy
and corporate overreach. But it is not only a story about privacy: it is also a story about
gender injustice—­about how corporations approach data relating to women’s bodies
and lives, and about how corporations approach data relating to minoritized popula-
tions more generally. Whose goals are prioritized in this case? The corporation’s, of
course. For Target, the primary motivation was maximizing profit, and quarterly finan-
cial reports to the board are the measurement of success. Whose goals are not priori-
tized? The teenager’s and those of every other pregnant woman out there.
   How did we get to the point where data science is used almost exclusively in the
service of profit (for a few), surveillance (of the minoritized), and efficiency (amidst
scarcity)? It’s worth stepping back to make an observation about the organization of
the data economy: data are expensive and resource-­intensive, so only already powerful
             corporations, governments, and elite research universities—­
institutions—­                                                          have the
means to work with them at scale. These resource requirements result in data science
that serves the primary goals of the institutions themselves. We can think of these goals
as the three Ss: science (universities), surveillance (governments), and selling (corpora-
tions). This is not a normative judgment (e.g., “all science is bad”) but rather an obser-
vation about the organization of resources. If science, surveillance, and selling are the
main goals that data are serving, because that’s who has the money, then what other
goals and purposes are going underserved?
     Let’s take “the cloud” as an example. As server farms have taken the place of paper
archives, storing data has come to require large physical spaces. A project by the Center
for Land Use Interpretation (CLUI) makes this last point plain (figure 1.7). In 2014,
CLUI set out to map and photograph data centers around the United States, often
in those seemingly empty in-­between areas we now call exurbs. In so doing, it called
attention to “a new kind of physical information architecture” sprawling across the
United States: “windowless boxes, often with distinct design features such as an appli-
qué of surface graphics or a functional brutalism, surrounded by cooling systems.” The
environmental impacts of the cloud—­in the form of electricity and air conditioning—­
are enormous. A 2017 Greenpeace report estimated that the global IT sector, which
is largely US-­based, accounted for around 7 percent of the world’s energy use. This is
more than some of largest countries in the world, including Russia, Brazil, and Japan.71
Unless that energy comes from renewable sources (which the Greenpeace report shows
that it does not), the cloud has a significant accelerating impact on global climate
change.
     So the cloud is not light and it is not airy. And the cloud is not cheap. The cost of
constructing Facebook’s newest data center in Los Lunas, New Mexico, is expected to
reach $1 billion.72 The electrical cost of that center alone is estimated at $31 million
per year.73 These numbers return us to the question about financial resources: Who has
the money to invest in centers like these? Only powerful corporations like Facebook
and Target, along with wealthy governments and elite universities, have the resources
to collect, store, maintain, analyze, and mobilize the largest amounts of data. Next,
who is in charge of these well-­resourced institutions? Disproportionately men, even
more disproportionately white men, and even more than that, disproportionately rich
white men. Want the data on that? Google’s Board of Directors is comprised of 82
percent white men. Facebook’s board is 78 percent male and 89 percent white. The
2018 US Congress was 79 percent male—­actually a better percentage than in previous
years—­and with a median net worth of five times more than the average American
household.74 These are the people who experience the most privilege within the matrix
Figure 1.7
Photographs from Networked Nation: The Landscape of the Internet in America, an exhibition by the
Center for Land Use Interpretation staged in 2013. The photos show four data centers located in
North Bergen, NJ; Dalles, OR; Ashburn, VA; and Lockport, NY (counterclockwise from top right).
They show how the “cloud” is housed in remote locations and office parks around the country.
Images by the Center for Land Use Interpretation.
of domination, and they are also the people who benefit the most from the current
status quo.75
   In the past decade or so, many of these men at the top have described data as “the
new oil.”76 It’s a metaphor that resonates uncannily well—­even more than they likely
intended. The idea of data as some sort of untapped natural resource clearly points to
the potential of data for power and profit once they are processed and refined, but it
also helps highlight the exploitative dimensions of extracting data from their source—­
people—­as well as their ecological cost. Just as the original oil barons were able to use
their riches to wield outsized power in the world (think of John D. Rockefeller, J. Paul
Getty, or, more recently, the Koch brothers), so too do the Targets of the world use
their corporate gain to consolidate control over their customers. But unlike crude oil,
which is extracted from the earth and then sold to people, data are both extracted from
people and sold back to them—­in the form of coupons like the one the Minneapolis
teen received in the mail, or far worse.77
   This extractive system creates a profound asymmetry between who is collecting,
storing, and analyzing data, and whose data are collected, stored, and analyzed.78 The
goals that drive this process are those of the corporations, governments, and well-­
resourced universities that are dominated by elite white men. And those goals are
neither neutral nor democratic—­in the sense of having undergone any kind of par-
ticipatory, public process. On the contrary, focusing on those three Ss—­science, surveil-
lance, and selling—­to the exclusion of other possible objectives results in significant
oversights with life-­altering consequences. Consider the Target example as the flip side
of the missing data on maternal health outcomes. Put crudely, there is no profit to be
made collecting data on the women who are dying in childbirth, but there is signifi-
cant profit in knowing whether women are pregnant.
   How might we prioritize different goals and different people in data science? How
might data scientists undertake a feminist analysis of power in order to tackle bias at its
source? Kimberly Seals Allers, a birth justice advocate and author, is on a mission to do
exactly that in relation to maternal and infant care in the United States. She followed
the Serena Williams story with great interest and watched as Congress passed the Pre-
venting Maternal Deaths Act of 2018. This bill funded the creation of maternal health
review committees in every state and, for the first time, uniform and comprehensive
data collection at the federal level. But even as more data have begun to be collected
about maternal mortality, Seals Allers has remained frustrated by the public conversa-
tion: “The statistics that are rightfully creating awareness around the Black maternal
mortality crisis are also contributing to this gloom and doom deficit narrative. White people are like, ‘how can we save Black women?’ And that’s not the solution that we
need the data to produce.”79
     Seals Allers—­and her fifteen-­year-­old son, Michael—­are working on their own data-­
driven contribution to the maternal and infant health conversation: a platform and
app called Irth—­from birth, but with the b for bias removed (figure 1.8). One of the
major contributing factors to poor birth outcomes, as well as maternal and infant mor-
tality, is biased care. Hospitals, clinics, and caregivers routinely disregard Black wom-
en’s expressions of pain and wishes for treatment.80 As we saw, Serena Williams’s own
story almost ended in this way, despite the fact that she is an international tennis star.
To combat this, Irth operates like an intersectional Yelp for birth experiences. Users
post ratings and reviews of their prenatal, postpartum, and birth experiences at spe-
cific hospitals and in the hands of specific caregivers. Their reviews include important
details like their race, religion, sexuality, and gender identity, as well as whether they
felt that those identities were respected in the care that they received. The app also
has a taxonomy of bias and asks users to tick boxes to indicate whether and how they
may have experienced different types of bias. Irth allows parents who are seeking care
to search for a review from someone like them—­from a racial, ethnic, socioeconomic,
and/or gender perspective—­to see how they experienced a certain doctor or hospital.
   Seals Allers’s vision is that Irth will be both a public information platform, for indi-
viduals to find better care, and an accountability tool, to hold hospitals and providers
responsible for systemic bias. Ultimately, she would like to present aggregated stories
and data analyses from the platform to hospital networks to push for change grounded
in women’s and parents’ lived experiences. “We keep telling the story of maternal mor-
tality from the grave,” she says. “We have to start preventing those deaths by sharing
the stories of people who actually lived.”81
   Irth illustrates the fact that “doing good with data” requires being deeply attuned to
the things that fall outside the dataset—­and in particular to how datasets, and the data
science they enable, too often reflect the structures of power of the world they draw
from. In a world defined by unequal power relations, which shape both social norms
and laws about how data are used and how data science is applied, it remains impera-
tive to consider who gets to do the “good” and who, conversely, gets someone else’s
“good” done to them.
Data feminism begins by examining how power operates in the world today. This con-
sists of asking who questions about data science: Who does the work (and who is pushed
out)? Who benefits (and who is neglected or harmed)? Whose priorities get turned into
products (and whose are overlooked)? These questions are relevant at the level of indi-
viduals and organizations, and are absolutely essential at the level of society. The cur-
rent answer to most of these questions is “people from dominant groups,” which has
resulted in a privilege hazard so acute that it explains the near-­daily revelations about
another sexist or racist data product or algorithm. The matrix of domination helps us to
understand how the privilege hazard—­the result of unequal distributions of power—­
plays out in different domains. Ultimately, the goal of examining power is not only to
understand it, but also to be able to challenge and change it. In the next chapter, we
explore several approaches for challenging power with data science.

Data feminism commits to challenging unequal power structures and working toward justice.

In 1971, the Detroit Geographic Expedition and Institute (DGEI) released a provocative
map, Where Commuters Run Over Black Children on the Pointes-­Downtown Track. The map
(figure 2.1) uses sharp black dots to illustrate the places in the community where the
children were killed. On one single street corner, there were six Black children killed
by white drivers over the course of six months. On the map, the dots blot out that
entire block.
    The people who lived along the deadly route had long recognized the magnitude
of the problem, as well as its profound impact on the lives of their friends and neigh-
bors. But gathering data in support of this truth turned out to be a major challenge.
No one was keeping detailed records of these deaths, nor was anyone making even
more basic information about what had happened publicly available. “We couldn’t
get that information,” explains Gwendolyn Warren, the Detroit-­based organizer who
headed the unlikely collaboration: an alliance between Black young adults from the
surrounding neighborhoods and a group led by white male academic geographers from
nearby universities.1 Through the collaboration, the youth learned cutting-­edge map-
ping techniques and, guided by Warren, leveraged their local knowledge in order to
produce a series of comprehensive reports, covering topics such as the social and eco-
nomic inequities among neighborhood children and proposals for new, more racially
equitable school district boundaries.
    Compare the DGEI map with another map of Detroit made thirty years earlier,
Residential Security Map (figure 2.2). Both maps use straightforward cartographic tech-
niques: an aerial view, legends and keys, and shading. But the similarities end there.
The maps differ in terms of visual style, of course. But more profound is how they
diverge in terms of the worldviews of their makers and the communities they seek to
support. The latter map was made by the Detroit Board of Commerce, which consisted
Where Commuters Run Over Black Children on the Pointes-­Downtown Track (1971) is one image from
a report, “Field Notes No. 3: The Geography of Children” which documented the racial inequities
of Detroit children. The map was created by Gwendolyn Warren, the administrative director of
the Detroit Geographic Expedition and Institute (DGEI), in a collaboration between Black young
adults in Detroit and white academic geographers that lasted from 1968–­1971. The group worked
together to map aspects of the urban environment related to children and education. Warren also
worked to set up a free school at which young adults could take college classes in geography for
credit. Courtesy of Gwendolyn Warren and the Detroit Geographical Expedition and Institute.

of only white men, in collaboration with the Federal Home Loan Bank Board, which
consisted mostly of white men. Far from emancipatory, this map was one of the earliest
instances of the practice of redlining, a term used to describe how banks rated the risk of
granting loans to potential homeowners on the basis of neighborhood demographics
(specifically race and ethnicity), rather than individual creditworthiness.
     Redlining gets its name because the practice first involved drawing literal red lines
on a map. (Sometimes the areas were shaded red instead, as in the map in figure 2.2.)
All of Detroit’s Black neighborhoods fall into red areas on this map because housing
discrimination and other forms of structural oppression predated the practice.2 But
denying home loans to the people who lived in these neighborhoods reinforced those
existing inequalities and, as decades of research have shown, were directly responsible
for making them worse.3
     Early twentieth-­century redlining maps had an aura very similar to the “big data”
approaches of today. These high-­tech, scalable “solutions” were deployed across the
nation, and they were one method among many that worked to ensure that wealth
remained attached to the racial category of whiteness.4 At the same time that these
maps were being made, the insurance industry, for example, was implementing simi-
         driven methods for granting (or denying) policies to customers based on
lar data-­
their demographics. Zoning laws that were explicitly based on race had already been
declared unconstitutional; but within neighborhoods, so-­called covenants were nearly
as exclusionary and completely legal.5 This is a phenomenon that political philosopher
Cedric Robinson famously termed racial capitalism, and it continues into the present in
the form of algorithmically generated credit scores that are consistently biased and in
the consolidation of “the 1 percent” through the tax code, to give only two examples
of many.6 What’s more, the benefits of whiteness accrue: “Whiteness retains its value
as a ‘consolation prize,’” civil rights scholar Cheryl Harris explains. “It does not mean
that all whites will win, but simply that they will not lose.”7
     Who makes maps and who gets mapped? The redlining map is one that secures the
power of its makers: the white men on the Detroit Board of Commerce, their families,
and their communities. This particular redlining map is even called Residential Security
Map. But the title reflects more than a desire to secure property values. Rather, it reveals
a broader desire to protect and preserve home ownership as a method of accumulating
wealth, and therefore status and power, that was available to white people only. In far
too many cases, data-­driven “solutions” are still deployed in similar ways: in support
of the interests of the people and institutions in positions of power, whose worldviews
and value systems differ vastly from those of the communities whose data the systems
rely upon.8
     The DGEI map, by contrast, challenges this unequal distribution of data and power.
It does so in three key ways. First, in the face of missing data, DGEI compiled its own
counterdata. Warren describes how she developed relationships with “political people
in order to use them as a means of getting information from the police department in
order to find out exactly what time, where, how and who killed [each] child.”9 Second,
the DGEI map plotted the data they collected with the deliberate aim of quantify-
ing structural oppression. They intentionally and explicitly focused on the problems
of “death, hunger, pain, sorrow and frustration in children,” as they explain in the
report.10 Finally, the DGEI map was made by young Black people who lived in the
community, under the leadership of a Black woman who was an organizer in the com-
munity, with support provided by the academic geographers.11 The identities of these
Collect, Analyze, Imagine, Teach                                                        53



makers matter, their proximity to the subject matter matters, the terms of their collabo-
ration matter, and the leadership of the project matters.12
   For these reasons, the DGEI provides a model of the second principle of data femi-
nism: challenge power. Challenging power requires mobilizing data science to push back
against existing and unequal power structures and to work toward more just and equi-
table futures. As we will discuss in this chapter, the goal of challenging power is closely
linked to the act of examining power, the first principle of data feminism. In fact, the
first step of challenging power is to examine that power. But the next step—­and the
reason we have chosen to dedicate two principles to the topic of power—­is to take
action against an unjust status quo.
   Taking action can itself take many forms, and in this chapter we offer four starting
points: (1) Collect: Compiling counterdata—­in the face of missing data or institutional
neglect—­offers a powerful starting point as we see in the example of the DGEI, or in
María Salguero’s femicide maps discussed in chapter 1. (2) Analyze: Challenging power
often requires demonstrating inequitable outcomes across groups, and new computa-
tional methods are being developed to audit opaque algorithms and hold institutions
accountable. (3) Imagine: We cannot only focus on inequitable outcomes, because then
we will never get to the root cause of injustice. In order to truly dismantle power, we
have to imagine our end point not as “fairness,” but as co-­liberation. (4) Teach: The
identities of data scientists matter, so how might we engage and empower newcom-
ers to the field in order to shift the demographics and cultivate the next generation of
data feminists?


Analyze and Expose Oppression


One can make a direct comparison between yesterday’s redlining maps and today’s risk
assessment algorithms. The latter are used in many cities in the United States today to
inform judgments about the length of a particular prison sentence, the amount of bail
that should be set, and even whether bail should be set in the first place. The “risk” in
their name has to do with the likelihood of a person detained by the police commit-
ting a future crime. Risk assessment algorithms produce scores that influence whether
a person is sent to jail or set free, effectively altering the course of their life.
   But risk assessment algorithms, like redlining maps, are neither neutral nor objec-
tive. In 2016, Julia Angwin led a team at ProPublica to investigate one of the most
widely used risk assessment algorithms in the United States, created by the company
Northpointe (now Equivant).13 Her team found that white defendants are more often
mislabeled as low risk than Black defendants and, conversely, that Black defendants
are mislabeled as high risk more often than white defendants.14 Digging further into
54                                                                                    Chapter 2



the process, the journalists uncovered a 137-­question worksheet that each detainee is
required to fill out (figure 2.3). The detainee’s answers feed into the software, in which
they are compared with other data to determine that person’s risk score. Although
the questionnaire does not ask directly about race, it asks questions that, given the
structural inequalities embedded in US culture, serve as proxies for race. These include
questions like whether you were raised by a single mother, whether you have ever
been suspended from school, or whether you have friends or family that have been
arrested. In the United States, each of those questions is linked to a set of larger social,
cultural, and political—­and, more often than not, racial—­realities. For instance, it has
been demonstrated that 67 percent of Black kids grow up in single-­parent households,




Figure 2.3
Equivant’s risk assessment algorithm is called Correctional Offender Management Profiling for
Alternative Sanctions (COMPAS) and is derived from a defendant’s answers to a 137-­question
survey about their upbringing, personality, family, and friends, including many questions that
can be considered proxies for race, such as whether they were raised by a single mother. Note
that evidence of family criminality would not be admissible evidence in a court case for a crime
committed by an individual, but here it is used as a factor in making important decisions about
a person’s freedom. Courtesy of Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner for
ProPublica, 2016.
Collect, Analyze, Imagine, Teach                                                      55



whereas only 25 percent of white kids do.15 Similarly, studies have shown that Black
kids are punished more harshly than are white kids for the same minor infractions,
starting as early as preschool.16 So, though the algorithm’s creators claim that they do
not consider race, race is embedded into the data they are choosing to employ. What’s
more, they are using that information to further disadvantage Black people, whether
because of an erroneous belief in the objectivity of their data, or because they remain
unmoved by the evidence of how racism is operating through their technology.
   Sociologist Ruha Benjamin has a term for these situations: the New Jim Code—­where
software code and a false sense of objectivity come together to contain and control
the lives of Black people, and of other people of color.17 In this regard, the redlin-
ing map and the Equivant risk assessment algorithm share some additional similari-
ties. Both use aggregated data about social groups to make decisions about individuals:
Should we grant a loan to this person? What’s the risk that this person will reoff-
end? Furthermore, both use past data to predict future behavior—­and to constrain
it. In both cases, the past data in question (like segregated housing patterns or single
parentage) are products of structurally unequal conditions. These unequal conditions
are true across large social groups, and yet the technology uses those data as predic-
tive elements that will influence one person’s future. Surya Mattu, a former ProPublica
reporter who worked on the story, makes this point directly: “Equivant didn’t account
for the fact that African Americans are more likely to be arrested by the police regard-
less of whether they committed a crime or not. The system makes an assumption
that if you have been arrested you are probably at higher risk.”18 This is one of the
challenges of using data about people as an input into a system: the data are never
“raw.” Data are always the product of unequal social relations—­relations affected by
centuries of history. As computer scientist Ben Green states, “Although most people
talk about machine learning’s ability to predict the future, what it really does is pre-
dict the past.”19 Effectively such “predictive” software reinforces existing demographic
divisions, amplifying the social inequities that have limited certain groups for genera-
tions. The danger of the New Jim Code is that these findings are actively promoted
as objective, and they track individuals and groups through their lives and limit their
future potential.
   But machine learning algorithms don’t just predict the past; they also reflect current
social inequities. A less well-­known finding from the ProPublica investigation of Equiv-
ant, for example, is that it also surfaced significantly different treatment of women by
the algorithm. Due to a range of factors, women tend to recidivate—­to commit new
crimes—­less than men do. That means the risk scale for women “is such that somebody
with a high risk score that’s a woman is generally about the level of a medium risk score
56                                                                              Chapter 2



for a man. So, it’s actually really shocking that judges are looking at these and thinking
that high risk means the same thing for a man and a woman when it doesn’t,” explains
lead reporter Julia Angwin.20
     Angwin decided to focus the story on race in part because of the prior work of crimi-
nologists such as Kristy Holtfreter, which had already highlighted some of these gender
differentials.21 But there was another factor at play in her editorial decision: workplace
sexism faced by women reporters like Angwin herself. Angwin explains how she had
always been wary of working on stories about women and gender because she wanted
to avoid becoming pigeonholed as a reporter who only worked on stories about women
and gender. But, she explains, “one of the things I woke up to during the #MeToo
movement was how many decisions like that I had made over the years”—­an internal-
ized form of oppression that had discouraged her from covering those important issues.
In early 2018, when we conducted this interview, Angwin was hiring for her own data
journalism startup, the Markup, founded with a goal of using data-­driven methods
to investigate the differential harms and benefits of new technologies on society. She
was encouraged to see how many job candidates of all genders were pitching stories
on issues relating to gender inequality. “In the era of data and AI, the challenge is that
accountability is hard to prove and hard to trace,” she explains. “The challenge for
journalism is to try to make as concrete as possible those linkages when we can so we
can show the world what the harms are.”
     Angwin is pointing out a tricky issue that is unlikely to go away. The field of jour-
nalism has long prided itself on “speaking truth to power.” But today, the location of
that power has shifted from people and corporations to the datasets and models that
they create and employ. These datasets and models require new methods of interroga-
tion, particularly when they—­like Equivant’s—­are proprietary. How does one report
on a black box, as these harmful algorithms are sometimes described?22 Much like the
situation encountered by Gwendolyn Warren when she looked into the data on the
Detroit children’s deaths, or like María Salguero when she started logging femicides in
Mexico, ProPublica found no existing studies that examined whether the risk scores
were racially biased, or existing datasets they could use to point them to answers. To
write the risk assessment story, ProPublica had to assemble a dataset of their own. The
researchers looked at ten thousand criminal defendants from a single county in Florida
and compared their recidivism risk scores with people who actually reoffended in a
two-­year period. After doing some initial exploratory analysis, they created their own
regression model that considered race, age, criminal history, future recidivism, charge
degree, and gender. They found that age, race, and gender were the strongest predictors
of who received a high risk score—­with Black defendants 77 percent more likely than
Collect, Analyze, Imagine, Teach                                                         57



white ones to receive a higher violent recidivism score. Their analysis also included
creating models to test the overall accuracy of the COMPAS model over time and an
investigation of errors to see if there were racial differences in the distribution of false
positives and false negatives. As it turns out, there were: the system was more likely to
predict that white people would not commit additional crimes if released, when they
actually did recidivate.23
   Angwin and her coauthors used data science to challenge data science. By collecting
missing data and reverse-­engineering the algorithm that was judging each defendant’s
risk, they were able to prove systemic racial bias. This analysis method is called audit-
ing algorithms and it is being increasingly used in journalism and in academic research
in order show how the harms and benefits of automated systems are differentially
distributed. Computational journalism researcher Nicholas Diakopoulos has proposed
that work like this become formalized into an algorithm accountability beat, which
would help to make the practice more widespread.24 He and computer scientist Sorelle
Friedler have asserted that algorithms need to be held “publicly accountable” for their
consequences, and the press is one place where this accounting can take place.25 By
providing proof of how racism and sexism, among other oppressions, create unequal
outcomes across social groups, analyzing data is a powerful strategy for challenging
power and working toward justice.


The Pitfalls of Proof


Let’s pause here for a feminist who question, as we introduced in chapter 1. Who is it,
exactly, that needs to be shown the harms of such differentials of power? And what
kind of proof do they require to believe that oppression is real? Women who experi-
ence instances of sexism, as Angwin did in her workplace, already know the harms of
that oppressive behavior. The young adults whom Gwendolyn Warren worked with
in Detroit already knew intimately that the white commuters were killing their Black
neighbors and friends. They had no need to prove to their own communities that struc-
tural racism was a factor in these deaths. Rather, their goal in partnering with the DGEI
was to prove the structural nature of the problem to those in positions of power. Those
dominant groups and institutions were the ones that, by privileging their own social,
political, and economic interests, bore much of the responsibility for the problem; and
they also, because of the phenomenon we have described as a privilege hazard, were
unlikely to see that such problem existed in the first place. The theory of change that
motivates these efforts to use data as evidence, or “proof,” is that by being made aware
of the extent of the problem, those in power will be prompted to take action.
58                                                                              Chapter 2



     These kinds of data-­driven revelations can certainly be compelling. When the analy-
sis appears in a high-­profile newspaper or blog or TV show (in other words: a place
white enough and male enough to be considered mainstream), it can indeed prompt
people in power to act. The ProPublica story on risk assessment algorithms, for exam-
ple, prompted a New York City council member to propose an algorithmic accountabil-
ity bill. Enacted in 2018, the bill became the first legal measure to tackle algorithmic
discrimination in the United States and led to the creation of a task force focused on
“equity and fairness” in city algorithms.26 Should the city implement some of the task
force’s recommendations, it would influence the work of software vendors, as well as
legislation in other cities. This path of influence—­from community problem to gather-
ing proof to informed reporting to policy change—­represents the best aspirations of
speaking truth to power.27
     While analyzing and exposing oppression in order to hold institutions account-
able can be extremely useful, its efficacy comes with two caveats. Proof can just as
easily become part of an endless loop if not accompanied by other tools of commu-
nity engagement, political organizing, and protest. Any data-­based evidence can be
minimized because it is not “big” enough, not “clean” enough, or not “newsworthy”
enough to justify a meaningful response from institutions that have a vested interest
in maintaining the status quo.28 As we saw in chapter 1, María Salguero’s data on femi-
cides was augmented by government commissions, reports from international agen-
cies, and rulings of international courts. But none of those data-­gathering efforts have
been enough to prompt comprehensive action.
     Another feminist who question: On whom is the burden of proof is placed? In 2015,
communications researcher Candice Lanius wrote a widely shared blog post, “Fact
Check: Your Demand for Statistical Proof is Racist,” in which she summarizes the ample
research on how those in positions of power accept anecdotal evidence from those like
themselves, but demand endless statistics from minoritized groups.29 In those cases,
she argues convincingly, more data will never be enough.
     Proof can also unwittingly compound the harmful narratives—­whether sexist or rac-
ist or ableist or otherwise oppressive—­that are already circulating in the culture, inad-
vertently contributing to what are known as deficit narratives. These narratives reduce
a group or culture to its “problems,” rather than portraying it with the strengths,
creativity, and agency that people from those cultures possess. For example, in their
book Indigenous Statistics, Maggie Walter and Chris Anderson describe how statistics
used by settler colonial groups to describe Indigenous populations have mainly func-
tioned as “documentation of difference, deficit, and dysfunction.”30 This can occur
even when the creators have good intentions—­for example, as Kimberly Seals Allers
Collect, Analyze, Imagine, Teach                                                      59



notes (see chapter 1), a great deal of the media reporting on Black maternal mortality
data falls into the deficit narrative category. It portrays Black women as victims and
fails to amplify the efforts of the Black women who have been working on the issue
for decades.
   This goes for gender data as well. “What little data we collect about women tends
to be either about their experience of violence or reproductive health,” explains Nina
Rabinovitch Blecker, who directs communications for Data2X, a nonprofit aimed at
improving the quality of data related to gender in a global context.31 The current data
encourage additional deficit narratives—­in which women are relentlessly and reduc-
tively portrayed as victims of violent crimes like murder, rape, or intimate partner vio-
lence. These narratives imply that the subjects of the data have no agency and need
“saving” from governments, international institutions, or concerned citizens. As one
step to counteract that, Blecker chose to publish an example from Uruguay that didn’t
focus on violence, but rather on quantifying women’s unseen contributions to the
economy.32
   So, though collecting counterdata and analyzing data to provide proof of oppres-
sion remain worthy goals, it is equally important to remain aware of how the subjects
of oppression are portrayed. Working with communities directly, which we talk more
about in chapter 5, is the surest remedy to these harms. Indigenous researcher Maggie
Walter explains that ownership of the process is key in order to stop the propagation
of deficit narratives: “We [Indigenous people] must have real power in how statistics
about us are done—­where, when and how.”33 Key too is a sustained attention to the
ways in which communities themselves are already addressing the issues. These actions
are often more creative, more effective, and more culturally grounded than the actions
that any outside organization would take.


Envision Equity, Imagine Co-­liberation


As the examples discussed thus far in this book clearly demonstrate, one of the most
dangerous outcomes of the tools of data and data science being consolidated in the
hands of dominant groups is that these groups are able to obscure their politics and
their goals behind their technologies. Benjamin, whose book Race after Technology: Abo-
litionist Tools for the New Jim Code (mentioned earlier), describes this phenomenon as
the “imagined objectivity of data and technology” because data-­driven systems like
redlining and risk assessment algorithms are not really objective at all.34 Her concept
of imagined objectivity emphasizes the role that cultural assumptions and personal pre-
conceptions play in upholding this false belief: one imagines (wrongly) that datasets
and algorithms are less partial and less discriminatory than people and thus more
60                                                                                  Chapter 2



“objective.”35 But as we discuss in chapter 1, these data products seem objective only
because the perspectives of those who produce them—­elite, white men and the institu-
tions they control—­pass for the default. Assumptions about objectivity are becoming a
major focus in data science and related fields as algorithm after algorithm is revealed to
be sexist, racist, or otherwise flawed. What can the people who design these computa-
tional systems do to avoid these pitfalls? And what can everyone else do to help them
and hold them accountable?
     The quest for answers to these questions has prompted the development of a new
area of research known as data ethics. It represents a growing interdisciplinary effort—­
both critical and computational—­to ensure that the ethical issues brought about by
our increasing reliance on data-­driven systems are identified and addressed. Thus far,
the major trend has been to emphasize the issue of “bias,” and the values of “fair-
ness, accountability, and transparency” in mitigating its effects.36 This is a promising
development, especially for technical fields that have not historically foregrounded
ethical issues, and as funding mechanisms for research on data and ethics proliferate.37
However, as Benjamin’s concept of imagined objectivity helps to show, addressing bias
in a dataset is a tiny technological Band-­Aid for a much larger problem. Even the val-
ues mentioned here, which seek to address instances of bias in data-­driven systems,
are themselves non-­neutral, as they locate the source of the bias in individual people
and specific design decisions. So how might we develop a practice that results in data-­
driven systems that challenge power at its source?
     The following chart (table 2.1) introduces an alternate set of orienting concepts for
the field: these are the six ideals that we believe should guide data ethics work. These


Table 2.1
From data ethics to data justice

Concepts That Secure Power                    Concepts That Challenge Power
Because they locate the source of the         Because they acknowledge structural power
problem in individuals or technical systems   differentials and work toward dismantling them

Ethics                                        Justice

Bias                                          Oppression

Fairness                                      Equity

Accountability                                Co-­liberation

Transparency                                  Reflexivity

Understanding algorithms                      Understanding history, culture, and context
Collect, Analyze, Imagine, Teach                                                         61



concepts all have legacies in intersectional feminist activism, collective organizing, and
critical thought, and they are unabashedly explicit in how they work toward justice.
   In the left-­hand column, we list some of the major concepts that are currently cir-
culating in conversations about the uses of data and algorithms in public (and private)
life. These are a step forward, but they do not go far enough. On the right-­hand side, we
list adjacent concepts that emerge from a grounding in intersectional feminist activism
and critical thought. The gap between these two columns represents a fundamental dif-
ference in view of why injustice arises and how it operates in the world. The concepts
on the left are based on the assumption that injustice arises as a result of flawed indi-
viduals or small groups (“bad apples,” “racist cops,” “brogrammers”) or flawed techni-
cal systems (“the algorithm/dataset did it”). Although flawed individuals and flawed
systems certainly exist, they are not the root cause of the problems that occur again and
again in data and algorithms.
   What is the root cause? If you’ve read chapter 1, you know the answer: the matrix
of domination, the matrix of domination, and the matrix of domination. The concepts
on the left may do good work, but they ultimately keep the roots of the problem in
place. In other words, they maintain the current structure of power, even if they don’t
intend to, because they let the matrix of domination off the hook. They direct data
scientists’ attention toward seeking technological fixes. Sometimes those fixes are nec-
essary and important. But as technology scholars Julia Powles and Helen Nissenbaum
assert, “Bias is real, but it’s also a captivating diversion.”38 There is a more fundamental
problem that must also be addressed: we do not all arrive in the present with equal
power or privilege. Hundreds of years of history and politics and culture have brought
us to the present moment. This is a reality of our lives as well as our data. A broader
focus on data justice, rather than data ethics alone, can help to ensure that past inequi-
ties are not distilled into black-­boxed algorithms that, like the redlining maps of the
twentieth century, determine the course of people’s lives in the twenty-­first.
   In proposing this chart, we are not suggesting that ethics have no place in data
science, that bias in datasets should not be addressed, or that issues of transparency
should go ignored.39 Rather, the main point is that the concepts on the left are inad-
equate on their own to account for the root causes of structural oppression. By not tak-
ing root causes into account, they limit the range of responses possible to challenge
power and work toward justice. In contrast, the concepts on the right start from the
basic feminist belief that oppression is real, historic, ongoing, and worth dismantling.
                                              Chock proposes a restorative
   Media theorist and designer Sasha Costanza-­
approach to data justice.40 Drawing from theories of restorative justice—­
                                                                         meaning
that decisions should be made in ways that recognize and rectify any harms of the
62                                                                                    Chapter 2



past—­Costanza-­Chock asserts that any notion of algorithmic fairness must also acknowl-
edge the systematic nature of the unfairness that has long been perpetrated by certain
groups on others. They give the example of college admissions—­a topic that always
seems to be in the news, not the least because it’s a major mechanism of protecting
privilege.41 A restorative approach to college admissions would entail making decisions
about who gets admitted in the present on the basis of who was historically not admit-
ted in the past—­like women, who were excluded from MIT, where Constanza-­Chock
teaches, for decades. According to this model, a “fair” present-­day entering class that
accounts for history might be composed of 90 percent women and people of color.42
     Does this approach make fairness political? Emphatically yes, because all systems are
political. In fact, the appeal to avoid politics is a very familiar way for those in power
to attempt to hold onto it.43 The ability to sidestep politics is a privilege in itself—­held
only by those whose existence does not challenge the status quo. If you are a Black
woman or a Muslim man or a transgender service member and you live in the United
States today, your being in the world is political, whether or not you want it to be.44 So
rather than design algorithms that purport to be “color-­blind” (since color-­blindness
is of course a myth), Costanza-­Chock explains that we should be designing algorithms
that are just.45 This means shifting from the ahistorical notion of fairness to a model
of equity.
     Equity is justice of a specific flavor, and it is different than equality. Equality is mea-
sured from a starting point in the present: t = 0, where t equals time and 0 indicates that
no time has elapsed since now. Based on this formula, the principle of equality would
hold that resources and/or punishments should be doled out according to what is hap-
pening in the present moment—­the time when t = 0. But this formula for equal treat-
ment means that those who are ahead in the present can go further, achieve more, and
stay on top, whereas those who start out behind can never catch up. Kiddada Green,
executive director of the Black Mothers Breastfeeding Association, makes the case that
in a country where Black babies are dying at twice the rate of white babies, equality
is actually systematically unfair: “There is a level of political correctness in America
that causes some people to believe that equality is the way to go. Even when equality
is unfair, some say that it’s the right thing to do.”46 Working toward a world in which
everyone is treated equitably, not equally, means taking into account these present
power differentials and distributing (or redistributing) resources accordingly. Equity is
much harder to model computationally than equality—­as it needs to take time, history,
and differential power into account—­but it is not impossible.47
     This difficulty also underscores the point that bias (in individuals, in datasets, in
statistical models, or in algorithms) is not a strong enough concept in which to anchor
Collect, Analyze, Imagine, Teach                                                       63



ideas about equity and justice. In writing about the creation of New York’s Welfare
Management System in the early 1970s, for example, Virginia Eubanks describes:
“These early big data systems were built on a specific understanding of what constitutes
discrimination: personal bias.”48 The solution at the time was to remove the humans
from the loop, and it remains so today: without potentially bad—­in this case, racist—­
apples, there would be less discrimination. But this line of thinking illustrates what
whiteness studies scholar Robin DiAngelo would call the new racism: the belief that
racism is due to individual bad actors, rather than structures or systems.49 In relation
to welfare management, Eubanks emphasizes that this often meant replacing social
workers, who were often women of color, and who had empathy and flexibility and
listening skills, with an automated system that applies a set of rigid criteria, no matter
what the circumstances.
   While bias remains a serious problem, it should not be viewed as something that
can be fixed after the fact. Instead, we must look to understand and design systems
that address the source of the bias: structural oppression. In truth, oppression is itself
an outcome, one that results from the matrix of domination. In this model, majoritized
bodies are granted undeserved advantages and minoritized bodies must survive unde-
served hardships. Starting from the assumption that oppression is the problem, not
bias, leads to fundamentally different decisions about what to work on, who to work
with, and when to stand up and say that a problem cannot and should not be solved
by data and technology.50 Why should we settle for retroactive audits of potentially
flawed systems if we can design with a goal of co-­liberation from the start?51 And here,
co-­liberation doesn’t mean “free the data,” but rather “free the people.” The people in
question are not only those with less privilege, but also those with more privilege: data
scientists, designers, researchers, and educators—­in other words, those like ourselves—­
who play a role in upholding oppressive systems.
   The key to co-­liberation is that it requires a commitment to and a belief in mutual
benefit, from members of both dominant groups and minoritized groups; that’s the co
in the term. Too often, acts of data service performed by tech companies are framed as
charity work (we discuss the limits of “data for good” in chapter 5). The frame of co-­
liberation equalizes this exchange as a form of relationship building and demographic
healing. There is a famous saying credited to aboriginal activists in Queensland, Aus-
tralia, from the 1970s: “If you have come here to help me, you are wasting your time.
But if you have come because your liberation is bound up with mine, then let us work
together.”52
   What does this mean? As poet and community organizer Tawana Petty explains in
relation to efforts around antiracism in the United States: “We need whites to firmly
64                                                                               Chapter 2



believe that their liberation, their humanity, is also dependent upon the destruction of
racism and the dismantling of white supremacy.”53 The same goes for gender: men are
often not prompted to think about how unequal gender relations seep into the institu-
tions they dominate, resulting in harm for everyone.
     This goal of co-­liberation motivates the Our Data Bodies (ODB) project. Led by a
group of five women, including Gangadharan and Petty, who sit at the intersection of
academia and organizing work, this project is a community-­centered initiative focused
on data collection efforts that disproportionately impact minoritized people. Working
with community organizations in three US cities, the ODB project has led participatory
research initiatives and educational workshops, culminating in the recently released
Digital Defense Playbook, a set of activities, tools, and tip sheets intended to be used by
and for marginalized communities to understand how data-­driven technologies impact
their lives.54
     Digital Defense Playbook was born out of many years of relationship-­building and
research, as well as a deliberate shift. The group explains in the playbook’s introduc-
tion, “We wanted to shift who gets to define problems around data collection, data
privacy, and data security—­from elites to impacted communities; shine a light on how
communities have been confronting data-­driven problems as well as how they wish to
confront these problems; and forge an analysis of data and data-­driven technologies
from and with allied struggles.”55 In so doing, the ODB project demonstrates how co-­
liberation requires not only transparency of methods but also reflexivity: the ability to
reflect on and take responsibility for one’s own position within the multiple, intersect-
ing dimensions of the matrix of domination. Along the way, the scholars and organiz-
ers involved in the project decided to shift their research agenda, which had begun as
a general project about data profiling and resistance, to surveillance, in response to the
problems voiced by the communities themselves.56
     Even within big tech itself, there is evidence of an increasing sense of reflexivity
among employees for their role in creating harmful data systems. Employees have
pushed back against Google’s work with the Department of Defense (DoD) on Project
Maven, which uses AI to improve drone strike accuracy; Microsoft’s decision to take
$480 million from the Department of Defense to develop military applications of its
augmented reality headset HoloLens; and Amazon’s contract with US Immigration and
Customs Enforcement (ICE) to develop its Rekognition platform for use in targeting
individuals for detention and deportation at US borders.57 This pushback has led to the
cancelling of the Google and Microsoft projects, as well as political consciousness rais-
ing across the sector, which we discuss further in the book’s conclusion.58
     Designing datasets and data systems that dismantle oppression and work toward
justice, equity, and co-­liberation requires new tools in our collective toolbox. We have
Collect, Analyze, Imagine, Teach                                                       65



some good starting points; building more understandable algorithms is a laudable,
worthy research goal. And yet what we need to explain and account for are not only
the inner workings of machine learning, but also the history, culture, and context
that lead to discriminatory outputs in the first place. For example, it is not an isolated
incident that facial analysis software couldn’t “see” Joy Buolamwini’s face, as we dis-
cussed in chapter 1. It is not an isolated incident that the “Lena” image used to test
most image-­processing algorithms was the centerfold from the November 1972 issue
of Playboy, cropped demurely at the shoulders.59 It is not an isolated incident that the
women who worked on the ENIAC computer were not invited to the fiftieth anniver-
sary celebration in 1995. It is not an isolated incident that Christine Darden was not
promoted as quickly as her male coworkers. None of these are isolated incidents: they
are connected data points and eminently measurable and predictable outcomes of the
matrix of domination. But you can only detect the pattern if you know the history,
culture, and context that surrounds it.
   Data people, generally speaking, have choices—­choices in who they work for, which
projects they work on, and what values they reject.60 Starting from the assumption
that oppression is the problem, equity is the path, and co-­liberation is the desired goal
leads to fundamentally different projects that challenge power at their source. It also
leads to different metrics of success. These extend beyond the efficiency of a database
under load, the precision of a classification algorithm, or the size of a user base one
year after launch. The success of a project designed with co-­liberation in mind would
also depend on how much trust was built between institutions and communities, how
effectively those with power and resources shared their power and resources, how
much learning happened in both directions, how much the people and organizations
were transformed in the process, and how much inspiration for future work, together,
was co-­conspired. These metrics are a little more squishy than the numbers and rank-
ings that we tend to believe are our only option, but utterly and entirely measurable
nonetheless.


Teach Data Like an Intersectional Feminist


When Gwendolyn Warren and the DGEI researchers collected their data about hit and
runs on Black children or scoured Detroit playgrounds to weigh and measure the bro-
ken glass they found, they were not only doing this work to make a data-­driven case
for change. The “institute” part of the Detroit Geographic Expedition and Institute
described the educational wing of the organization that ran classes in data collection,
mapping, and cartography. It came about at Warren’s insistence that the academic geog-
raphers give something back to the community whose knowledge they were drawing
66                                                                               Chapter 2



upon for their research. She recognized that while a single map or project could make
a focused intervention, education would enable her community to come away with a
longer-­term strategy for challenging power. As it turned out, the institutional affilia-
tions of the academic geographers enabled them to offer free, for-­credit college courses,
which they taught in the community for community members.
     In her emphasis on education, Warren recognized its enduring role as a mechanism
of both empowerment and transformation. This belief is not new; as American educa-
tional reformer Horace Mann stated famously in 1848, “Education, then, beyond all
other divides of human origin, is a great equalizer of conditions of men—­the balance
wheel of the social machinery.” But here is the thing—­it really matters how we do that
equalizing and who we imagine that equalizing to serve. For his part, Mann was literal
about the “men”: education was to be an equalizer of men, but only certain men (read:
white, Anglo, Christian) and explicitly not women.61 Warren, on the other hand, rec-
ognized that access to education—­and to data science education in particular—­would
have to be expanded in order for it to achieve its equalizing force.
     Unfortunately, Warren’s transformative vision has still yet to enter the data science
classroom. As was true in Mann’s era, men still lead. Women faculty comprise less than
a third of computer science and statistics faculty. More than 80 percent of artificial
intelligence professors are men.62 This gender imbalance, and the narrowness of vision
that results, is compounded by the fact that data science is often framed as an abstract
and technical pursuit. Steps like cleaning and wrangling data are presented as solely
technical conundrums; there is less discussion of the social context, ethics, values, or
politics of data.63 This perpetuates the myth that data science about astrophysics is the
same as data science about criminal justice is the same as data science about carbon
emissions. This limits the transformative work that can be done. Finally, because the
goal of learning data science is modeled as individual mastery of technical concepts and
skills, communities are not engaged and conversations are restricted. Instead, teach-
ers impart technical knowledge via lectures, and students complete assignments and
quizzes individually. We might call this model of teaching “the Horace Mann Factory
Model of Data Science,” because it represents the exclusionary view that Mann himself
advanced. But let’s just call it the Man Factory for short.
     The Man Factory is really good at producing men, mainly elite white men like the
ones who already lead the classes. It’s not as good at producing women data scientists,
or nonbinary data scientists, or data scientists of color. For years, researchers and advo-
cacy organizations have recognized that there are problems with this “pipeline” for
technical fields; yet this research is framed around questions like “Why are there so
few women computer scientists?” and “Why are women leaving computing?”64 Note
Collect, Analyze, Imagine, Teach                                                       67



that these questions imply that it is the women who have the problem, inadvertently
perpetuating a deficit narrative. Feminist scholars who are studying the issue are, not
surprisingly, asking very different questions, like “How can the men running the Man
Factory share their power?” and “How can we structurally transform STEM education
together?”65
   One person currently modeling an answer to these questions is Laurie Rubel, the
math educator behind the Local Lotto project. If you were on the city streets of Brook-
lyn or the Bronx in the past five years, you may have inadvertently crossed paths
with one of her data science classes. You probably didn’t realize it because the classes
looked nothing like a traditional classroom (figure 2.4). Teenagers from the neighbor-
hood wandered around in small groups. They were outfitted with tablets, pen and
paper, cameras, and maps. They periodically took pictures on the street, walked into
bodegas, chatted with passersby in Spanish or English, and entered information on
their tablets.
   Rubel is a leader in an area called mathematics for spatial justice, which aims to
show how mathematical concepts can be taught in ways that relate to justice con-
cerns arising from students’ everyday lives, and to do so in dialogue with people in
their neighborhoods and communities. The goal of Local Lotto was to develop a place-­
specific way of teaching concepts related to data and statistics grounded in consider-
ations of equity.66 Specifically, Rubel and the other organizers of Local Lotto wanted
young learners to come up with a data-­driven answer to the question: “Is the lottery
good or bad for your neighborhood?”
   In New York, as in other US states that operate lotteries, lottery ticket sales go back
into the state budget—­sometimes, but not always, to fund educational programs.67 But
lottery tickets are not purchased equally across all income brackets or all neighbor-
hoods. Low-­wage workers buy more tickets than their higher-­earning counterparts.
What’s more, the revenue from ticket purchases is not allocated back to those workers
or the places they live. Because of this, scholars have argued that the lottery system is
a form of regressive taxation—­essentially a “poverty tax”—­whereby low-­income neigh-
borhoods are “taxed” more because they play more, but do not receive a proportional
share of the profit.68
   The Local Lotto curriculum was designed to expose high school learners to this
instance of social inequality. They begin by talking about the lottery and the idea of
probability by playing chance-­based games. Then they consider jackpot games like
the Sweet Millions lottery, advertised by New York State as “your best chance from
the New York Lottery to win a million for just a buck.” The best chance to win one
million, however, turns out to be about one in four million; an entire class session is
68                                                                                    Chapter 2




Figure 2.4
A data science classroom. The Local Lotto project (2012–2015) taught local high school students
statistics and data analysis rooted in neighborhood and justice concerns. Courtesy of the City
Digits Project Team, including Brooklyn College, the Civic Data Design Lab at MIT and the Center
for Urban Pedagogy. This work was supported by the National Science Foundation under Grant
No. DRL-­1222430.


devoted to a discussion about other instances of “four million” that more closely relate
to the learners’ lives.69 The learners then leave the classroom with the goal of collect-
ing data about how other people experience the lottery, which takes them back into
their neighborhoods. They map stores that sell lottery tickets. They record interviews
with shopkeepers and ticket buyers on their tablets and then geolocate them on their
maps. They take pictures of lottery advertising. Afterward, the learners analyze their
results and present them to the class. They examine choropleth maps of income levels,
they make ratio tables, and they correlate state spending of lottery profits with median
family income. (No surprise: there is no correlation.) Finally, they create a data-­driven
argument: an opinion piece supported with evidence from their statistical and spatial
analyses, as well as their fieldwork (figures 2.5 and 2.6).
Collect, Analyze, Imagine, Teach                                                              69




Figure 2.5
“Now You Know.” A student’s infographic poster responding to the New York Lottery advertising
message, “Hey, you never know ... ,” that explores the probability of winning anything—­ranging
from a million dollars to another ticket. Courtesy of the City Digits Project Team. This work was
supported by the National Science Foundation under Grant No. DRL-­1222430.
70                                                                                       Chapter 2




Figure 2.6
Final collaborative opinion piece asserting that the lottery is not good for the students’ neighbor-
hood and presenting evidence they collected to back up their case. See the multimedia slideshow at
http://citydigits.mit.edu/locallotto#tours-tab. Courtesy of Emmanuela, Angel, Robert, and Janeva.
This work was supported by the National Science Foundation under Grant No. DRL-­1222430.


     By formal measures, the Local Lotto approach worked: before one school’s imple-
mentation of Local Lotto, only two of forty-­seven learners were able to determine
the correct number of possible combinations in a lottery example. Later, almost half
(twenty-­one of forty-­seven) were successfully able to calculate the number of com-
binations. But perhaps more importantly, the Local Lotto approach made math and
statistics relevant to the students’ lives. One student shared that what he learned was
“something new that could help me in my local environment, in my house actually,”
and that after the course, he tried to convince his mother to spend less money on the
lottery by “showing her my math book and all the work.” Spanish-­speaking women in
the class who didn’t often participate in classroom discussion became essential transla-
tors during the participatory mapping module. Several students went on to teach other
teachers about the curriculum, both locally and nationally.70
     What’s different about the Local Lotto approach to teaching data analysis and
statistical concepts compared to the Man Factory? How is Local Lotto challeng-
ing power both inside and outside the classroom? First, it was woman-­led: the proj-
ect was conceived by three women leaders representing three institutions.71 Just as
with the DGEI map and school, led by Gwendolyn Warren, the identities of the cre-
ators matter. Second, rather than modeling data science as abstract and technical,
Local Lotto modeled a data science that was grounded in solving ethical questions
around social inequality that had relevance for learners’ everyday lives: Is the lottery
good or bad for your neighborhood? The project valued lived experience: the learn-
ers came in as “domain experts” in their neighborhoods. And it valued both qualita-
tive data and quantitative data: the learners spoke with neighborhood residents and
connected their beliefs, attitudes, and concerns to probability calculations. Learners
used community members’ voices as evidence in their final projects. Third, rather than
Collect, Analyze, Imagine, Teach                                                       71



valorizing individual mastery of technical skills as the gold standard, learners worked
together during every phase of the project. They used methods from art and design
(like the creation of infographics and digital slideshows) to practice communicating
with data.
   Even as we celebrate these intentional pedagogical choices, the Local Lotto proj-
ect still had its shortcomings, as the organizers noted in a 2016 paper for Cognition
and Instruction.72 Many of these stemmed from a basic fact: the teachers and course
designers of the project were white and Asian, whereas the youth in the classes were
predominantly Latinx and Black. This led to several issues. For instance, the curriculum
designers had intended to focus primarily on income inequality, but they discovered
that “the students consistently surfaced race.” Because race and ethnicity were not
part of the teaching material, the teachers felt that they did not have the experience
or background to discuss them explicitly and deflected those conversations. As they
write in the paper, “Youth, and in this case youth of color, have different understand-
ings about racial boundaries; theirs are differently nuanced and scaled than affluent,
white, or adult perspectives.” The organizers are now taking steps to explicitly integrate
discussions about race into the curriculum, as well as to include race, ethnicity, and age
data in the course projects.73
   The course designers also encountered “limited but recurring instances of resistance
from students” to the project’s central focus on income inequality. They attribute this
resistance to the fact that the course was developed and taught by outsiders and could
be seen as passing judgment on the people in their neighborhoods: that because they
were not from the community, the teachers were perpetuating a deficit narrative about
low-­income people. This is both a sophisticated and very fair pushback from the young
learners. Most people, regardless of their wealth or level of education, know they are
not going to win the lottery, after all. There is an element of imaginative fantasy in
purchasing a ticket. The campaign slogan, “Hey, you never know ...” appeals as much
to this fantasy as it does to the reality of the odds, and this fantasy has value too.
In reflecting on the unintended sense of judgment experienced by the students, the
course designers determined that, in the next iteration of the course, they would work
to connect students with people in the communities themselves who are actively work-
ing to address issues of income inequality.
   In both its successes and its failures, as well as its commitment to iteration and try-
ing again, Local Lotto encapsulates what it means to challenge power and privilege and
work toward justice. Justice is a journey. The discomfort that comes along with this
journey is par for the course. There is no such thing as mastery of feminism because
those who hold positions of privilege—­like those in data science, like the Local Lotto
72                                                                             Chapter 2



course designers, and like us, the authors of this book—­are constantly learning how
to be better allies and accomplices across difference. In this process, what becomes
most important is to “stay with the trouble,” as feminist philosopher Donna Haraway
would say.74 Staying with the trouble means persisting in your work, especially when
it becomes uncomfortable, unclear, or outright upsetting. One of the biggest strengths
of the Local Lotto project is the courage of its creators to publicly, transparently, and
reflexively interrogate themselves and their process, to detail their stumbling blocks,
and to describe their commitments to doing better in the future.


Challenge Power


After examining power, the next step is to challenge it—­map by map, audit by audit,
community by community, and classroom by classroom. Collecting counterdata to
quantify and visualize structural oppression, as Gwendolyn Warren and the DGEI did
with their map, helps those who occupy positions of power understand the scope,
scale, and character of the problems from which they are otherwise far removed.
Analyzing biased algorithms, as Julia Angwin and ProPublica did, can show the real,
material harms of automated systems, as well as build a base of evidence for politi-
cal or institutional change. At the same time, it is important to remember that
minoritized individuals and groups should not have to repeatedly prove that their
experiences of oppression are real. And data alone do not always lead to change—­
especially when that change also requires dominant groups to share their resources and
their power.
     Those of us who use data in our work must alter some of our most basic assump-
tions and imagine new starting points. Shifting the frame from concepts that secure
power, like fairness and accountability, to those that challenge power, like equity and
co-­liberation, can help to ensure that data scientists, designers, and researchers take
oppression and inequality as their grounding assumption for creating computational
products and systems. We must learn from—­and design with—­the communities we
seek to support. A commitment to data justice begins with an acknowledgment of the
fact that oppression is real, historic, ongoing, and worth dismantling. This commit-
ment is one that we must teach the next generation of data scientists and data citizens,
in communities and in classrooms, if we want to broaden our path toward justice.
3    On Rational, Scientific, Objective Viewpoints from Mythical,
Imaginary, Impossible Standpoints


Principle: Elevate Emotion and Embodiment
Data feminism teaches us to value multiple forms of knowledge, including the knowledge that
comes from people as living, feeling bodies in the world.




In 2012, twenty kindergarten children and six adults were shot and killed at an elemen-
tary school in Sandy Hook, Connecticut. In the wake of this unconscionable tragedy,
and of the additional acts of gun violence that followed, the design firm Periscopic
began a new project: to visualize all the gun deaths that took place in the United States
over the course of a calendar year. Although there is no shortage of prior work on the
subject in the form of bar charts or line graphs, Periscopic, a company with the tagline
“Do good with data,” took a different approach.
    When you load the project’s webpage, you first see a single orange line that arcs up
from the x-­axis on the left-­hand side of the screen. Then, the color abruptly changes to
white. A small dot drops down, and you see the phrase, “Alexander Lipkins, killed at
29” (figure 3.1a). The line continues to arc up across the screen and then down, com-
ing back to rest on the x-­axis, where a second phrase appears: “Could have lived to be
93.” Then, a second line appears—­the arc of another life. The animation speeds up and
the arcs multiply. A counter at the top right displays how many years of life have been
“stolen” from these victims of gun violence. After several excruciating minutes, the
visualization completes its count for the year: 11,419 people killed, totaling 502,025
stolen years (figure 3.1b).
    The visualization uses demographic data and rigorous statistical methods to arrive
at these numbers, as is explained in the methods section on the site. But what makes
Periscopic’s visualization so very different from a more conventional bar chart of simi-
lar information, such as “The Era of ‘Active Shooters’” from the Washington Post (figure
3.2)? The projects share the proposition that gun deaths present a serious threat. But
unlike the Washington Post bar chart, Periscopic’s work is framed around an emotion:
74                                                                                    Chapter 3




Figure 3.1
An animated visualization of the “stolen years” of people killed by guns in the United States in
2013. The first image (a) shows the beginning state of the animation and the second image (b)
shows the end state. Images by Periscopic.
Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints 75




Figure 3.2
A bar chart of the number of “active shooter” incidents in the United States between 2000 and
2015. Images by Christopher Ingraham for the Washington Post.


loss. People are dying; their remaining time on earth has been stolen from them. These
people have names and ages. They have parents and partners and children who suffer
from that loss as well.
   This message was clearly received, as was the project overall. It was featured in Wired
magazine, and even won an Information is Beautiful award. But it also caused some
stewing on the part of the visualization community. Alberto Cairo, the author of the
visualization book The Truthful Art, expressed his concerns about the use of emotion
and persuasion in the project: “Is it clear to a general audience that what they see is the
work of professionals who actively shape data to support a cause, and not the product
of automated processes?”1 At root for Cairo was the question of how detached and
“neutral” a visualization should be. He wondered, Should a visualization be designed
to evoke emotion?
   The received wisdom in technical communication circles is, emphatically, “No.”
In the recent book A Unified Theory of Information Design, authors Nicole Amare and
Alan Manning state: “The plain style normally recommended for technical visuals is
directed toward a deliberately neutral emotional field, a blank page in effect, upon
which viewers are more free to choose their own response to the information.”2 Here,
plainness is equated with the absence of design and thus greater freedom on the part
76                                                                               Chapter 3



of the viewer to interpret the results for themselves. Things like colors and icons work
only to stir up emotions and cloud the viewer’s rational mind.
     They’re not the first ones to posit this belief. In the field of data communication,
any kind of ornament has long been viewed as suspect. Why? As historian of science
Theodore Porter puts it, “Quantification is a technology of distance.”3 And distance,
he explains, is closely related to objectivity because it puts literal space between peo-
ple and the knowledge they produce. This desire for separation is what underlies the
nineteenth-­century statistician Karl Pearson’s exhortation, echoed in Cairo’s comments
about the Periscopic visualization, for people to set aside their “own feelings and emo-
tions” when performing statistical work.4 The more plain, the more neutral; the more
neutral, the more objective; and the more objective, the more true—­or so this line of
reasoning goes. At a data visualization master class in 2013, workshop leaders from the
Guardian newspaper held up spreadsheet data—­spreadsheet data!—­as an ideal for the
communication of quantitative data, calling it: “Clarity without persuasion.”5
     But persuasion is everywhere, even in spreadsheets, and—­as feminist philosopher
Donna Haraway would likely argue—­especially in spreadsheets. In the 1980s, Haraway
was among the first to connect the seeming neutrality and objectivity of data and their
visual display to the ideas about distance that we’ve just discussed. She described data
visualization, in particular, as “the god trick of seeing everything from nowhere.” The
view from nowhere—­from a distance, from up above, like a god—­may be data visual-
ization’s most signature feature. It’s also the most ethically complicated to navigate for
the ways in which it masks the people, the methods, the questions, and the messiness
that lies behind clean lines and geometric shapes. Haraway calls it the god trick: it’s a
trick because it makes the viewer believe that they can see everything, all at once, from
an imaginary and impossible standpoint. But it’s also a trick because what appears
to be everything, and what appears to be neutral, is always what she terms a partial
perspective. And in most cases of seemingly “neutral” visualizations, this perspective
is the one of the dominant, default group. Think back to the presumption of white-
ness as default that we discussed in the introduction, or—­for an example of an actual
visualization—­to the redlining map discussed in chapter 2. This is a good example of
the god trick at work.6
     The god trick and its underlying assumptions about neutrality and truth are baked
into today’s best practices for data visualization. This is largely due to the influence of
one man: the renowned statistical graphics expert Edward Tufte. Back in the 1980s,
Tufte invented a metric for measuring the amount of superfluous information included
in a chart. He called it the data-­ink ratio.7 In his view, a visualization designer should
strive to use ink to display data alone. Any ink devoted to something other than
Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints 77



the data themselves—­such as background color, iconography, or embellishment—­is a
suspect and intruder to the graphic. Visual minimalism, according to this logic, appeals
to reason first. As police officer Joe Friday says to every woman character on the Ameri-
can TV series Dragnet, “Just the facts, ma’am.” Decorative elements, on the other hand,
are associated with messy feelings—­or, worse, represent stealthy (and, according to
Tufte, unscientific) attempts at emotional persuasion. Data visualization has even been
named as “the unempathetic art” by designer Mushon Zer-­Aviv because of its emphatic
rejection of emotion.8
   The logic that sets up this false binary between emotion and reason is gendered, of
course, because the belief that women are more emotional than men (and, by contrast,
that men are more reasoned than women) is one of the most persistent stereotypes
across many Western cultures. Indeed, psychologists have called it a master stereotype
and puzzled over how it endures even when certain emotions—­even extreme ones, like
anger and pride—­are simultaneously associated with men.9 A central focus of feminist
scholarship has been to challenge false binaries like this one between reason and emo-
tion and to point out how they establish hierarchies as well. (We discuss this more in
chapter 4.) For now, the important thing to note is how false binaries work to benefit a
single one of Haraway’s partial perspectives: that of the group already at the top—­elite
white men.
   How can we let go of this binary logic? Two additional questions help challenge
this reductive way of thinking and the oppressive hierarchies that it supports. First, is
visual minimalism really more neutral? And second, how might activating emotion—­
leveraging, rather than resisting, emotion in data visualization—­help us learn, remem-
ber, and communicate with data? Exploring these questions helps get us closer to the
third principle of data feminism: embrace emotion and embodiment.


Visualization as Rhetoric


Information visualization has diverse origins. Its history is often traced from the explo-
sion of European men mapping their colonial conquests in the late fifteenth and early
sixteenth centuries, through the development of new visual typologies like the timeline
and the bar chart in the seventeenth and eighteenth centuries, to the adoption of those
forms by powerful nations as they amassed increasing amounts of data on the popula-
tions they sought to control. But feminist scholars are increasingly challenging this
simple narrative of progress, as well as its cast of characters, which is predominantly
white and male. Whitney Battle-­Baptiste and Britt Rusert recently published a new
edition of the visualization work of W. E. B. Du Bois, the renowned Black sociologist
78                                                                                 Chapter 3



and civil rights activist, who created his “data portraits” of African American life for
the 1900 Paris Exposition. Laura Bliss, in a blog post that went viral, called attention to
the “narrative maps” of Shanawdithit, a member of the Beothuk (Newfoundland) tribe,
which she created around 1829 at the urging of a visiting anthropologist. And Lauren,
one of the authors of this book, created a website that reanimates the historical charts
of Elizabeth Palmer Peabody, the nineteenth-­century editor and educator, who used
visualization in her teaching (figure 3.3).10
     Each of these early visualization designers understood how their images could func-
tion rhetorically. But in more recent history, many of data visualization’s theorists and
practitioners have come from technical disciplines aligned with engineering and com-
puter science and have not been trained in that most fundamental of Western commu-
nication theories. In his ancient Greek treatise, Aristotle defines rhetoric as “the faculty
of observing in any given case the available means of persuasion.”11 But rhetoric isn’t
only found in political speeches made by men dressed in tunics with wreaths on their
heads.12 Any communicating object that reflects choices about the selection and rep-
resentation of reality is a rhetorical object. Whether or not it is rhetorical (it always is)
has nothing to do with whether or not it is true (it may or may not be).
     The question of rhetoric matters because “a rhetorical dimension is present in every
design,” says visualization researcher Jessica Hullman.13 This includes visualizations
that do not deliberately intend to persuade people of a certain message. It especially and
definitively includes those so-­called neutral visualizations that do not appear to have an
editorial hand. In fact, those might even be the most perniciously persuasive visualiza-
tions of all!
     Editorial choices become most apparent when compared with alternative choices.
For example, in his book The Curious Journalist’s Guide to Data, journalist Jonathan Stray
discusses a data story from the New York Times about the September 2012 jobs report.14
The New York Times created two graphics from the report: one framed from the perspec-
tive of Democrats (the party in power at the time; figure 3.4a) and one framed from the
perspective of Republicans (figure 3.4b).
     Either of these graphics, considered in isolation, appears to be neutral and factual.
The data are presented with standard methods (line chart and area chart respectively)
and conventional positionings (time on the x-­axis, rates expressed as percentages on
the y-­axis, title placed above the graphic). There is a high data-­ink ratio in both cases
and very little in the way of ornamentation. But the graphics have significant editorial
differences. The Democrats’ graphic emphasizes that unemployment is decreasing—­in
its title, the addition of the thick blue arrow pointing downward, and the annotation
“Friday’s drop was larger than expected.” Whereas the Republicans’ graphic highlights
Figure 3.3
(a) Elizabeth Palmer Peabody’s chart of “Significant Events of the 17th Century United States” (1865).
(b) Peabody’s chart recreated in digital form by Lauren’s Digital Humanities Lab (2017). (c) A rendering
of Peabody’s chart reimagined as an interactive quilt by Lauren’s Digital Humanities Lab (2019). Images
by (a) Elizabeth Palmer Peabody, A Chronological History of the United States (1856), (b) the Georgia Tech
Digital Humanities Lab, and (c) Courtney Allen for the Georgia Tech Digital Humanities Lab.
80                                                                                 Chapter 3




Figure 3.3 (continued)


the fact that unemployment has been steadily high for the past three years—­through
the use of the “8 percent unemployment” reference line, the choice to use an area chart
instead of a line, and, of course, the title of the graphic.15 So neither graphic is neutral,
but both graphics are factual. As Jonathan Stray says, “The constraints of truth leave a
very wide space for interpretation.”16 When visualizing data, the only certifiable fact is
that it’s impossible to avoid interpretation (unless you simply republish the September
jobs report as your visualization, but then it wouldn’t be a visualization).
     Fields very close to visualization, like cartography, have long seen their work as
ideological. But discussions of rhetoric, editorial choices, and power have been far less
Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints 81




Figure 3.4
A data visualization of the September 2012 jobs report from the perspective of Democrats (a) and
Republicans (b). The New York Times data team shows how simple editorial changes lead to large
differences in framing and interpretation. As data journalist Jonathan Stray remarks on these
graphics, “The constraints of truth leave a very wide space for interpretation.” Images by Mike
Bostock, Shan Carter, Amanda Cox, and Kevin Quealy, for the New York Times, as cited in The
Curious Journalist’s Guide to Data by Jonathan Stray.
82                                                                                 Chapter 3



frequent in the field of data visualization. In 2011, Hullman and coauthor Nicholas
Diakopoulos wrote an influential paper reasserting the importance of rhetoric for
the data visualization community.17 Their main argument was that visualizing data
involves editorial choices: some things are necessarily highlighted, while others are
necessarily obscured. When designers make these choices, they carry along with them
framing effects, which is to say they have an impact on how people interpret the graph-
ics and what they take away from them.
     For example, it is standard practice to cite the source of one’s data. This functions on
a practical level—­so that readers may go out and download the data themselves. But
this choice also functions as what Hullman and Diakopoulos call provenance rhetoric
designed to signal the transparency and trustworthiness of the presentation source to
end users. Establishing trust between the designers and their audience in turn increases
the likelihood that viewers will believe what they see.
     Other aspects of data visualization also work to displace viewers’ attention from
editorial choices to reinforce a graphic’s perceived neutrality and “truthiness.” After
doing a sociological analysis, Helen Kennedy and coauthors determined that four
conventions of data visualization reinforce people’s perceptions of its factual basis: (1)
two-­dimensional viewpoints, (2) clean layouts, (3) geometric shapes and lines, and
(4) the inclusion of data sources at the bottom.18 These conventions contribute to the
perception of data visualization as objective, scientific, and neutral. Both unemploy-
ment graphics from the New York Times employ these conventions: the image space
is two-­dimensional and abstract; the layout is “clean,” meaning minimal and lacking
embellishment beyond what is necessary to communicate the data; the lines represent-
ing employment rates vary smoothly and faithfully against a geometrically gridded
background; and the source of the data is noted at the bottom. Either the Democrat or
the Republican graphic would have been entirely plausible as a New York Times visu-
alization, and very few of us would have thought to question the graphic’s framing of
the data.
     So if plain, “unemotional” visualizations are not neutral, but are actually extremely
persuasive, then what does this mean for the concept of neutrality in general? Scien-
tists and journalists are just some of the people who get nervous and defensive when
questions about neutrality and objectivity come up. Auditors and accountants get ner-
vous, too. They often assume that the only alternative to objectivity is a retreat into
complete relativism and a world in which alternative facts reign and everyone gets a
gold medal for having an opinion. But there are other options.
     Rather than valorizing the neutrality ideal and trying to expunge all human traces
from a data product because of their bias, feminist philosophers have proposed a goal
Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints 83



of more complete knowledge. Donna Haraway’s idea of the god trick comes from a
larger argument about the importance of developing feminist objectivity. It’s not just
data visualization but all forms of knowledge that are situated, she explains, meaning
that they are produced by specific people in specific circumstances—­cultural, historical,
and geographic.19 Feminist objectivity is a tool that can account for the situated nature
of knowledge and can bring together multiple—­what she terms partial—­perspectives.
Sandra Harding, who developed her ideas alongside Haraway, proposes a concept of
strong objectivity. This form of objectivity works toward more inclusive knowledge pro-
duction by centering the perspectives—­or standpoints—­of groups that are otherwise
excluded from knowledge-­making processes.20 This has come to be known as standpoint
theory. To supplement these ideas, Linda Alcoff has introduced the idea of position-
ality, a concept that emphasizes how individuals come to knowledge-­making proc-
esses from multiple positions, each determined by culture and context.21 All of these
ideas offer alternatives to the quest for a universal objectivity—­which is, of course, an
unattainable goal.
   The belief that universal objectivity should be our goal is harmful because it’s always
only partially put into practice. This flawed belief is what provoked renowned cardiolo-
gist Dr. Nieca Goldberg to title her book Women Are Not Small Men because she found
that heart disease in women unfolds in a fundamentally different way than in men.22
The vast majority of scientific studies—­not just of heart disease, but of most medical
conditions—­are conducted on men, with women viewed as varying from this “norm”
only by their smaller size.23 The key to fixing this problem is to acknowledge that all
science, and indeed all work in the world, is undertaken by individuals. Each person
occupies a particular perspective, as Haraway might say; a particular standpoint, as
Harding might say; or a particular set of positionalities, as Alcott might say. And all
would agree that research by only men and about only men cannot be universalized to
make knowledge claims about all other people in the world.
   Disclosing your subject position(s) is an important feminist strategy for being trans-
parent about the limits of your—­or anyone’s—­knowledge claims. Thus, for example,
we (the authors) included statements about our own positionalities in the introduction
in order to disclose the gender, race/ethnicity, class, ability, education, and other sub-
ject positions that informed the writing of this book. Rather than viewing these posi-
tionalities as threats or as influences that might have biased our work, we embraced
them as offering a set of valuable perspectives that could frame our work. This is an
approach that we would like to see others embrace as well. Each person’s intersecting
subject positions are unique, and when applied to data science, they can generate cre-
ative and wholly new research questions.
84                                                                                 Chapter 3



Data Visceralization


This embrace of multiple perspectives and positionalities helps to rebalance the hierar-
chy of reason over emotion in data visualization.24 How? Since the early 2000s, there
has been an explosion of research about affect—­the term that academics use to refer to
emotions and other subjective feelings—­from fields as diverse as neuroscience, geog-
raphy, and philosophy. (We discuss affect further in chapter 7.) This work challenges
the thinking that casts emotion out as irrational and illegitimate, even as it undeniably
influences the social, political, and scientific processes of the world. Evelyn Fox Keller,
a physicist turned philosopher, famously employed the Nobel Prize–­winning research
of geneticist Barbara McClintock to show how even the most profound of scientific
discoveries are generated from a combination of experiment and insight, reason and
emotion.25
     Once we embrace the idea of leveraging emotion in data visualization, we can truly
appreciate what sets Periscopic’s “US Gun Deaths” graphic apart from the Washington
Post graphic or from any number of other gun death charts that have appeared in
newspapers and policy documents. The Washington Post graphic, for example, repre-
sents death counts as blue ticks on a generic bar chart. If we didn’t read the caption, we
wouldn’t know whether we were counting gun deaths in the United States or haystacks
in Kansas or exports from Malaysia or any other statistic. In contrast, the Periscopic
visualization leads with loss, grief, and mourning—­primarily through its rhetorical
emphasis on counting “stolen years.” This draws the attention of viewers to “what
could have been.” The counting is reinforced by the visual language for representing
the “stolen years” as grey lines, appropriate for numbers that are rigorously determined
but not technically facts because they come from a statistical model. The visualization
also uses animation and pacing to help us first appreciate the scale of one life, and
then compound that scale 11,419-­fold. The magnitude of the loss, especially when
viewed in aggregate and over time, makes a statement of profound truth revealed to us
through our own emotions. It is important to note that emotion and visual minimal-
ism are not incompatible here; the Periscopic visualization shows us how emotion can
be leveraged alongside visual minimalism for maximal effect.
     Skilled data artists and designers know these things already, or at least intuit them.
Like the Periscopic team, others are pushing the boundaries of what affective and
embodied data visualization could look like. In 2010, Kelly Dobson founded the Data
Visceralization research group at the Rhode Island School of Design (RISD) Digital +
Media graduate program. The goal for this group was not to visualize data but to vis-
ceralize it. Visual things are for the eyes, but visceralizations are representations of data
Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints 85



that the whole body can experience, emotionally as well as physically—­data that “we
see, hear, feel, breathe and even ingest,” writes media theorist Luke Stark.26
   The reasons for visceralizing data have to do with more than simply creative experi-
mentation. First, humans are not two eyeballs attached by stalks to a brain computer.
We are embodied, multisensory beings with cultures and memories and appetites.27
Second, people with visual disabilities need a way to access the data encoded in charts
and dashboards as well. According to the World Health Organization, 253 million peo-
ple globally live with some form of visual impairment, on the spectrum from limited
vision to complete blindness.28 For reasons of accessibility, Aimi Hamraie, the director
of the Mapping Access project at Vanderbilt University, advocates for a form of data vis-
ceralization, although not in those exact terms: “Rather than relying entirely on visual
representations of data,” they explain, “digital-­accessibility apps could expand access
by incorporating ‘deep mapping,’ or collecting and surfacing information in multiple
sensory formats.”29
   At the moment, however, examples of objects and events that make use of multiple
sensory formats are more likely to be found in the context of research labs and gal-
leries and museums. For example, in A Sort of Joy (Thousands of Exhausted Things), the
theater troupe Elevator Repair Service joined forces with the data visualization firm
the Office of Creative Research to script a live performance based on metadata about
the artworks held by New York’s Museum of Modern Art (MoMA).30 With 123,951
works in its collection, MoMA’s metadata consists of the names of artists, the titles
of artworks, their media formats, and their time periods. But how does an artwork
make it into the museum collection to begin with? Major art museums and their col-
lection policies have long been the focus of feminist critique because the question of
whose work gets collected translates into the question of whose work is counted in the
annals of history.31 As you might guess, this history has mostly consisted of a parade
of white male European “masters,” as the Guerrilla Girls project pictured in figure 3.5
reminds us.
   In 1989, the Guerrilla Girls, an anonymous collective of women artists, published
an infographic: Do Women Have to Be Naked to Get into the Met. Museum? The graphic
makes a data-­driven argument by comparing the gender statistics of artists collected by
another New York museum, the Metropolitan Museum of Art (the Met) to the gender
statistics of the subjects and models in the artworks. It was designed to be displayed on
a billboard, but it was rejected by the sign company because it “wasn’t clear enough.”32
If you ask us, it’s pretty clear: the Met readily collects paintings in which women
are the (naked) subjects but it collects very few artworks created by women artists
themselves.
86                                                                                    Chapter 3




Figure 3.5
Do Women Have to Be Naked to Get into the Met. Museum? An infographic (of a sort) created by the
Guerrilla Girls in 1989, intended to be displayed on a bus billboard. Courtesy of the Guerrilla
Girls.


     After being thwarted by the sign company, the Guerrilla Girls then paid for the info-
graphic to be printed on posters displayed throughout the New York City bus system,
until the Metropolitan Transportation Authority (MTA) cancelled the contract, stating
that the figure seemed to have more than a fan in her hand. It is definitely more than a
fan, but this deliberate understatement reveals the MTA’s discomfort with this provoca-
tive, activist image.33
     A Sort of Joy deploys wholly different tactics to similar ends. The performance starts
with a group of white men standing in a circle in the center of the room. They face
out toward the audience, which surrounds them. The men are dressed like stereotypi-
cal museum visitors: collared shirts, slacks, and so on. Each wears headphones and
holds an iPad on which the names of artists in the collection scroll by. “John,” the
men say together. We see the iPads scrolling through all the names of the artists in the
MoMA collection whose first name is John: John Baldessari, John Cage, John Lennon,
John Waters, and so on. Three female performers, also wearing headphones and car-
rying iPads with scrolling names, pace around the circle of men. “Robert,” the men
say together, and the names scroll through the Roberts alphabetically. The women are
silent and keep walking. “David,” the men say together. It soon becomes apparent that
the artists are sorted by first name, and then ordered by which first name has the most
works in the collection. Thus, the Johns and Roberts and Davids come first, because they
have the most works in the collection. But Marys have fewer works, and Mohameds
and Marías are barely in the register. Several minutes later, after the men say “Michael,”
Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints 87



“James,” “George,” “Hans,” “Thomas,” “Walter,” “Edward,” “Yan,” “Joseph,” “Martin,”
“Mark,” “José,” “Louis,” “Frank,” “Otto,” “Max,” “Steven,” “Jack,” “Henry,” “Henri,”
“Alfred,” “Alexander,” “Carl,” “Andre,” “Harry,” “Roger,” and “Pierre,” “Mary” finally
gets her due, spoken by the female performers, the first sound they’ve made.
   For audience members, the experience is slightly confusing at first. Why are the men
in a circle? Why do they randomly speak someone’s name? And why are those women
walking around so intently? But “Mary” becomes a kind of aha moment, highlighting
the highly gendered nature of the collection—­exactly the same kind of experience of
insight that data visualization is so good at producing, according to researcher Martin
Wattenberg.34 From that point on, audience members start to listen differently, eagerly
awaiting the next female name. It takes more than three minutes for “Mary” to be
spoken, and the next female name, “Joan,” doesn’t come for a full minute longer. “Bar-
bara” follows immediately after that, and then the men return to reading: “Werner,”
“Tony,” “Marcel,” “Jonathan.”
   From a data analysis perspective, A Sort of Joy consists of simple operations: counting
and grouping. A bar chart or a tree map of first names could easily have represented
the same results. But presenting the dataset as a time-­based experience makes the audi-
ence wait and listen and experience. It also runs counter to the mantra in information
visualization expressed by researcher Ben Shneiderman in the mid-­1990s: “Overview
first, zoom and filter, then details-­on-­demand.”35 In this data performance, we do not
see the overview first. We hear and see and experience each datapoint one at a time
and only slowly construct a sense of the whole. The different gender expressions, body
movements, and verbal tones of the performers draw our collective attention to the
issue of gender in the MoMA collection. We start to anticipate when the next woman’s
name will arise. We feel the gender differential, rather than see it.
   This feeling is affect. It comprises the emotions that arise when experiencing the
performance, as well as the physiological reactions to the sounds and movements made
by the performers, as well as the desires and drives that result—­even if that drive is to
walk into another room because the performance is disconcerting or just plain long.
   Data visceralizations that leverage affect aren’t limited to major art institutions.
Catherine and artist Andi Sutton led walking tours of the future coastline of Boston
based on sea level rise.36 Interactive artist Mikhail Mansion made a leaning, bobbing
chair that animatronically shifts based on real-­time shifts in river currents.37 Nonprofit
organizations in Tanzania staged a design competition for data-­driven clothing that
incorporated statistics about gender inequality and closed the project with a fashion
runway show.38 Artist Teri Rueb stages “sound encounters” between the geologic layers
of a landscape and the human body that is affected by them.39 Simon Elvins drew a
88                                                                              Chapter 3



giant paper map of pirate radio stations in London that you can actually listen to.40 A
robot designed by Annina Rust decorates real pies with pie charts about gender equal-
ity, and then visitors eat them.41
     These projects may seem to be speaking to another part of brain (or belly) than your
standard histograms or network maps, but there is something to be learned from the
opportunities opened up by visceralizing data. Deliberately embracing emotions like
wonder, confusion, humor, and solidarity enables a valuable form of data maximal-
ism, one that allows for multisensory entry points, greater accessibility, and a range of
learning types.


Visceralizing Uncertainty


Scientific researchers are now proving by experiment what designers and artists have
known through practice: activating emotion, leveraging embodiment, and creating
novel presentation forms help people grasp and learn more from data-­driven argu-
ments, as well as remember them more fully.42 As it turns out, visceralizing data may
help designers solve one particularly pernicious problem in the visualization commu-
nity: how to represent uncertainty in a medium that’s become rhetorically synony-
mous with the truth. To this end, designers have created a huge array of charts and
techniques for quantifying and representing uncertainty. These include box plots, vio-
lin plots (figure 3.6), gradient plots, and confidence intervals.43 Unfortunately, how-
ever, people are terrible at recognizing uncertainty in data visualizations, even when
they’re explicitly told that something is uncertain. This remains true even for some
researchers who use data themselves!44
     For example, let’s consider the Total Electoral Votes graphic displayed as part of
the New York Times live online coverage of the 2016 presidential election (figure 3.7).
The blue and red lines represent the New York Times’s best guess at the outcomes
over the course of election night and into the following day. The gradient areas show
the degree of uncertainty that surrounded those guesses, with the darker inner area
showing electoral vote outcomes that came up 25 percent to 75 percent of the time,
and the lighter outer areas showing outcomes that came up 75 percent to 95 percent
and 5 percent to 25 percent of the time, respectively. If you look closely at the far left
of the graphic, which represents election night (everything prior to the 12:00 a.m. axis
label), the outcome of Trump winning and Clinton losing easily falls within the 5 to 25
percent likelihood range.
     Although many election postmortems pronounced the 2016 election the Great
Failure of Data and Statistics, because most simulations and other statistical models
Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints 89




Figure 3.6
What is the best way to communicate uncertainty in a medium that looks so certain? Designers
have created diverse chart forms to try to solve this problem. Depicted here are five violin plots;
each shows the distribution of data along with their probability density (the purple part). You
could also think of this form as a beautiful purple vagina, as the comic xkcd has observed; see
https://www.xkcd.com/1967/. Images from the Data Visualisation Catalogue.




Figure 3.7
A 2016 chart from the New York Times that uses opacity—­darker and lighter shades of blue and
red—­to indicate uncertainty. Images by Gregor Aisch, Nate Cohn, Amanda Cox, Josh Katz, Adam
Pearce, and Kevin Quealy for the New York Times.
90                                                                                 Chapter 3



suggested that Clinton would win, most forecasts did include the possibility of a
Trump victory. The underlying problem was not the failure of data but the difficulties
of depicting uncertainty in visual form. People are just not sufficiently trained to recog-
nize uncertainty in graphics such as this. Rather than interpreting the gradient bands
as probabilities (e.g., Trump had a 20 percent chance of winning at 6 p.m.), people may
interpret them as votes (e.g., Trump had 20 percent of the vote at 6 p.m.). Or they may
ignore the gradient bands altogether and look only at the lines. Or they see Clinton on
top and assume she is winning. This is called heuristics in psychology literature—­using
mental shortcuts to make judgments—­and it happens all the time when people are
asked to assess probabilities.45 A large part of the problem is that visualization conven-
tions reinforce these misjudgments. The graphics look so certain, even when they are
trying their very hardest to visually illustrate uncertainty!
     Jessica Hullman, whose work on rhetoric we’ve already mentioned, offers one solu-
tion to this problem. Instead of creating fixed plots as in the New York Times example
that represents uncertainty in aggregate or static form, Hullman advocates for ren-
dering experiences of uncertainty.46 In other words, leverage emotion and affect so that
people experience uncertainty perceptually. Or, to invoke a common refrain from
rhetorical training and design schools, “show, don’t tell.” Rather than telling people
that they are looking at uncertainty while employing a certain-­looking graphic style—­
which creates conditions ripe for those pesky heuristics to intervene—­make them feel
the uncertainty.
     We can see a good example of showing uncertainty in action on the same New York
Times live election coverage webpage. At the top of the page was a gauge (figure 3.8)
that showed the New York Times’s real-­time prediction of who was likely to win the race,
with a gradient of categories that ranged from medium blue (“Very Likely” that Clinton
would win) to medium red (“Very Likely” that Trump would win). But the needle did
not stay in one place. It jittered between the twenty-­fifth and seventy-­fifth percentiles,
showing the range of outcomes that the New York Times was then predicting, based
on simulations using the most recent data. At the beginning of the day, the range of
motion was fairly wide but still only showed the needle on the Hillary Clinton side.
As the night went on, its range narrowed, and the center moved closer and closer to
the red side of the gauge. By 9 p.m., the needle jittered just a little, and on the Trump
side only.
     A number of New York Times readers were aggressive in their dislike of the jitter, call-
ing it “irresponsible” and “unethical” and “the most stressful thing I’ve ever looked
at online and I’ve seen a lot of stressful shit.”47 In response, Gregor Aisch, one of the
designers of the gauge, defended it, explaining that “we thought (and still think!) this
Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints 91




Figure 3.8
The controversial “jittering” election gauge featured in the New York Times coverage of the 2016
presidential election. Images by Gregor Aisch, Nate Cohn, Amanda Cox, Josh Katz, Adam Pearce,
and Kevin Quealy for the New York Times.


movement actually helped demonstrate the uncertainty around our forecast, convey-
ing the relative precision of our estimates.”48 So was this “unethical” design, or the
sophisticated communication of uncertainty?
   Building off of Hullman’s work, we’d say that the answer is the latter. The jitter-
ing election gauge was actually exhibiting current best practices for communicating
uncertainty. It gave people the perceptual, intuitive, visceral, and emotional experience
of uncertainty to reinforce the quantitative depiction of uncertainty. The fact that it
unsettled so much of the New York Times readership probably had less to do with the
ethics of the visualization and more to do with the outcome of the election. So score
one for emotion in the task of representing uncertainty.


Don’t Never Do a God Trick


So does this mean that all election graphics should jitter? Or that data visceralizations
are categorically superior to visual graphics? Or that a data visualization framed around
emotion is always the “better” choice for the design task at hand? Or that designers
should never use the god trick to present a view from above?
   The answer to these questions may surprise you: definitively no! If there is any
single rule in design, it’s that context is queen. A design choice made in one context
or for one audience does not translate to other contexts or audiences. Simply stated: it
is never a good idea to say “never” in design. We delve deeper into the importance of
context when working with data in chapter 6.
92                                                                                Chapter 3



     Let’s take the god trick as an example. Even though the god trick can do harm—­for
example, in the form of those racist, objective-­looking redlining maps from chapter
2—­there are also good reasons to use the god trick as a form of recuperation, contesta-
tion, or empowerment. As renowned data visualization designer Fernanda Viégas says,
“The kind of overview that data visualization provides is one of the superpowers I
treasure the most.”49
     It is this “superpower”—­the aerial view from no body—­that we see put into practice
in the map Coming Home to Indigenous Place Names in Canada (figure 3.9a). Margaret
Pearce, a cartographer and member of the Citizen Potawatomi Nation, spent fifteen
months collecting Indigenous place names from First Nations, Métis, and Inuit peo-
ples. The map depicts the land that is known in a contemporary Anglo-­Western con-
text as Canada, but without any of the common colonial orientation points, like the
boundaries of the provinces or the locations of major cities like Ottawa, Montréal, and
Nova Scotia. For example, you can see in figure 3.9b that places like Kinoomaagewaabi-
kaang (“Teaching rocks”) and Odawa (“Traders”) and Kazabazua gajiibajiwan (“River
runs under”) fill the area usually described as Toronto. As the publisher’s description
states, “The names are ancient and recent, both in and outside of time, and they
express and assert the Indigenous presence across the Canadian landscape in Indig-
enous languages.”50
     Coming Home to Indigenous Place Names in Canada leverages the authority of the
god’s-­eye view to challenge the colonizer’s view, to advocate for a “reseeing” of the
land under terms of engagement that recognize Indigenous sovereignty and respect
Indigenous homelands. The extent of geographic territory included and the sheer
number of names asserts the Indigenous presence as major, originary, and ongoing.
This is by design. Pearce intentionally created the map with the same paper size, fold,
scale, and projection as the map published by Natural Resources Canada, the country’s
geographic authority.51 By replicating these design features, Coming Home proposes an
alternative yet equally authoritative conception of national identity.52
     There is actually another twist on top of its intentionally authoritative view: the map
does not reveal everything.53 As the cartographer, Pearce proceeded with the Indigenous
methodologies of respect, responsibility, and reciprocity.54 What this meant in practice
was caring for each name as Indigenous cultural property—­securing the permissions
for each name and respecting when communities did not want to share English trans-
lations of the name. She is definitive about the fact that the names are not data: “The
place names aren’t datasets. The place names are cultural property being shared with
the map that come from people.” Protecting that cultural property also meant protect-
ing the exact geographic location of each site from being shared with outsiders. This
is where the scale of the god trick has protective effects: because it is generalized, at
Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints 93




Figure 3.9
Overview and detail view of Coming Home to Indigenous Place Names in Canada (2017). (a) The map
depicts First Nations, Métis, and Inuit place names collected from many tribes and nations across
what is today more commonly called Canada. (b) The detail view depicts Indigenous names from
the area around Toronto. Map by Margaret W. Pearce; map design copyright 2017 Canadian-­
American Center, University of Maine.
94                                                                                    Chapter 3




Figure 3.9 (continued)
Place names in this detail image shared by permission of the following:
Alan Corbiere.
Hiio Delaronde and Jordan Engel, “Haudenosaunee Country in Mohawk,” The Decolonial Atlas,
decolonialatlas.wordpress.com/2015/02/04/haudenosaunee-country-in-mohawk-2/, by permis-
sion of the authors.
Charles Lippert and Jordan Engel, “The Great Lakes: An Ojibwe Perspective,” The Decolonial Atlas,
decolonialatlas.wordpress.com/2015/04/14/the-great-lakes-in-ojibwe-v2/, by permission of the
authors.
Kitigan Zibi Anishinabeg.
Brian McInnes, Sounding Thunder: The Stories of Francis Pegahmagabow (East Lansing: Michigan
State University Press, 2016), by permission of Brian McInnes, with gratitude to James Dumont
and Wasauksing First Nation.
Woodland Cultural Centre, place names from Frances Froman, Alfred Keye, Lottie Keye, and
Carrie Dyck, English-­Cayuga/Cayuga-­English Dictionary (Toronto, Ontario: University of Toronto
Press); and Marianne Mithun and Reginald Henry, Wadewayęstanih. A Cayuga Teaching Grammar
(Brantford, Ontario: Woodland Publishing, The Woodland Cultural Centre, 1984), by permission
of Amos Key Jr. and Carrie Dyck.
Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints 95



1:5,000,000, it serves to communicate general location without pinpointing exact loca-
tion. In this case, the god trick communicates Indigenous authority while preserving
Indigenous autonomy.
   Although the map has been published and the project is ostensibly finished, Pearce’s
commitment to and care for the Indigenous names continues. Each time the map is
reproduced, such as in the detail image in figure 3.9b, Pearce writes to the communities
to whom the names belong, explains the proposed context of the names, and requests
permission for the names to be reproduced in that context. You can see these permis-
sions as they were granted in the caption for the figure. Pearce proceeds with sensitivity
to her own positionalities, as well as the depth of meaning of each particular place, its
name, and the community. Place names are “relations,” she says. “And they’re not my
relations, it’s not my territory. ... They co-­exist as relations that are incorporated into
the community.” Cartography, then, becomes not a straightforward representation of
“what is” in some absolute sense. Rather, Coming Home is a map of relations, conversa-
tions, and shared investments across difference in the landscape.


Elevate Emotion and Embodiment


The third principle of data feminism, and the theme of this chapter, is to elevate emo-
tion and embodiment. As we have shown, these are crucial if often undervalued tools in
the data communication toolbox. They help avoid inadvertently conveying the view
from no body: the view from an imaginary and impossible standpoint that does not
and cannot exist.
   How has the whole picture, the overview, or the god trick come to be seen as rational
and objective at all? How did the field of data visualization arrive at a set of conven-
tions that prioritize rationality, devalue emotion, and completely ignore the nonseeing
organs in the human body? Who is excluded when only vision is included?
   Any knowledge community inevitably places certain things at the center and casts
others out, in the same way that male bodies are almost always taken as the norm in
scientific studies while female bodies are viewed as deviations, or that abled bodies
are almost always taken as primary design cases while disabled bodies require a design
retrofit. Feminist human-­computer interaction (HCI) scholar Shaowen Bardzell asserts
designers should look first to those at the margins: the people pushed to the margins
in any particular design context demonstrate who and what the system is trying to
exclude.55 Subsequent work in HCI insists that designers then work to “demarginalize
the ‘margins’ by recognizing intersections that exist, and engaging solidarity to navi-
gate towards equity and inclusion.”56
96                                                                            Chapter 3



     In the case of data visualization, what is excluded is emotion and affect, embodi-
ment and expression, embellishment and decoration. These are the aspects of human
experience associated with women, and thus devalued by the logic of our master ste-
reotype. But Periscopic’s gun violence visualization shows how visual minimalism can
coexist with emotion for maximum impact. Works like A Sort of Joy demonstrate that
data communication can be visceral—­an experience for the whole body. And Coming
Home to Indigenous Place Names in Canada establishes that the god trick itself can be
used to simultaneously engender emotion and challenge injustice.
     Rather than making universal rules and ratios (think: data-­ink) that exclude some
aspects of human experience in favor of others, our time is better spent working toward
a more holistic and more inclusive ideal. All design fields, including visualization and
data communication, are fields of possibility. Black feminist sociologist Patricia Hill
Collins describes an ideal knowledge situation as one in which “neither ethics nor
emotions are subordinated to reason.”57 Rebalancing emotion and reason opens up the
data communication toolbox and allows us to focus on what truly matters in a design
process: honoring context, architecting attention, and taking action to defy stereotypes
and reimagine the world.58
4    “What Gets Counted Counts”


Principle: Rethink Binaries and Hierarchies

Data feminism requires us to challenge the gender binary, along with other systems of counting
and classification that perpetuate oppression.




“Sign in or create an account to continue.” At a time in which every website seems to
require its own user account, these words often elicit a groan—­and the inevitability of
yet another password that will soon be forgotten. But for people like Maria Munir, the
British college student who famously came out as nonbinary to then president Barack
Obama on live TV, the prospect of creating a new user account is more than mere
annoyance.1 Websites that require information about gender as part of their account
registration process almost always only provide a binary choice: “male or female.”2
For Munir, those options are insufficient. They also take an emotional toll: “I wince
as I’m forced to choose ‘female’ over ‘male’ every single time, because that’s what
my passport says, and ... being non-­binary is still not legally recognised in the UK,”
Munir explains.3
    For the millions of nonbinary people in the world—­that is, people who are not
either male or female, men or women—­the seemingly simple request to “select gender”
can be difficult to answer, if it can be answered at all.4 Yet when creating an online
user account, not to mention applying for a national passport, the choice between
“male” or “female,” and only “male” or “female,” is almost always the only one.5
These options (or the lack thereof) have consequences, as Munir clearly states: “If you
refuse to register non-­binary people like me with birth certificates, and exclude us in
everything from creating bank accounts to signing up for mailing lists, you do not
have the right to turn around and say that there are not enough of us to warrant
change.”6
    “What gets counted counts,” feminist geographer Joni Seager has asserted, and
Munir is one person who understands that.7 What is counted—­like being a man or a
woman—­often becomes the basis for policymaking and resource allocation. By con-
trast, what is not counted—­like being nonbinary—­becomes invisible (although there
are also good reasons for being invisible in some contexts, and we’ll come back to
98                                                                              Chapter 4



those shortly). Seager’s research focus is gender, the environment, and policy (see fig-
ure 4.1), and she points out that there is more global data on gender being collected
than ever before. And yet, these data collection efforts often still leave many people
out, including nonbinary people, lesbians, and older women. Even among those who
are counted, they tend to be asked very narrow questions about their lives. “Women in
poor countries seem to be asked about 6 times a day what kind of contraception they
use,” Seager quipped in a lecture at the Boston Public Library. “But they are not asked
about whether they have access to abortion. They are not asked about what sports they
like to play.”8
     The process of converting qualitative experience into data can be empowering,
and even has the potential to be healing, as we address toward the end of this chap-
ter. When thoughtfully collected, quantitative data can be empowering too. So many
issues of structural inequality are problems of scale, and they can seem anecdotal until
they are viewed as a whole. For instance, in 2014, when film professors Shelley Cobb
and Linda Ruth Williams set out to count the women involved in the film industry in
the United Kingdom, they encountered a woman screenwriter who had never before
considered the fact that in the United Kingdom, women screenwriters are outnum-
bered by screenwriters of other genders at a rate of four to one.9 She expressed surprise:
“I didn’t even know that because screenwriters never get to meet each other.”10
     A similar situation occurred in the example of ProPublica’s reporting on maternal
mortality in the United States, as discussed in chapter 1. The investigative team set out
to count all the mothers who had died in childbirth or from complications shortly
thereafter. They interviewed many families of women who had died while giving
birth, but, like the screenwriter, few of the families were aware that the phenomenon
extended beyond their own daughters and sisters, partners and friends. This lack of
data, like the issue of maternal mortality itself, is another structural problem, and it
serves as an example of why feminist sociologists like Ann Oakley have long advocated
for the use of quantitative methods alongside qualitative ones. Without quantitative
research, Oakley explains, “it is difficult to distinguish between personal experience
and collective oppression.”11
     But before collective oppression can be identified through analyses like the one
that ProPublica conducted, the data must exist in the first place. Which brings us back
to Maria Munir and the importance of collecting data that reflects the population it
purports to represent. On this issue, Facebook was ahead of the curve when, in 2014, it
expanded the gender categories available to registered users from the standard two to
over fifty choices, ranging from “Genderqueer” to “Neither”—­a move that was widely
praised by a range of LGBTQ+ advocacy groups (figure 4.2a).12 One year later, when the
“What Gets Counted Counts”                                                                      99




Figure 4.1
Maternity and paternity leave around the globe from The Women’s Atlas, 5th edition (2018). Joni
Seager and Annie Olson started working on the first women’s atlas in 1980, when there was very
little global data on women. The book is now in its fifth edition, but Seager highlights that there
are still huge gender data gaps. Image courtesy of Joni Seager and Penguin Books.
100                                                                                Chapter 4



company abandoned its select-­from-­options model altogether, replacing the “Gender”
dropdown menu with a blank text field, the decision was touted as even more progres-
sive (figure 4.2b).13 Because Facebook users could input any word or phrase to indi-
cate their gender, they were at last unconstrained by the assumptions imposed by any
preset choice.14
   But additional research by information studies scholar Rena Bivens has shown that
below the surface, Facebook continues to resolve users’ genders into a binary: either
“male” or “female.”15 Evidently, this decision was made so that Facebook could allow
its primary clients—­advertisers—­to more easily market to one gender or the other.
Put another way, even if you can choose the gender that you show to your Facebook
friends, you can’t change the gender that Facebook provides to its paying customers
(figure 4.3). And this discrepancy leads right back to the issues of power we’ve been
discussing since the start of this book: it’s corporations like Facebook, and not individu-
als like Maria Munir, who have the power to control the terms of data collection. This
remains true even as it is people like Munir who have personally (and often painfully)
run up against the limits of those classification systems—­and who best know how they
could be improved, remade, or in some cases, abolished altogether.
   Feminists have spent a lot of time thinking about classification systems because the
criteria by which people are divided into the categories of man and woman is exactly
that: a classification system.16 And while the gender binary is one of the most widespread
classification systems in the world today, it is no less constructed than the Facebook
advertising platform or, say, the Golden Gate Bridge. The Golden Gate Bridge is a physi-
cal structure; Facebook ads are a virtual structure; and the gender binary is a conceptual
one. But all these structures were created by people: people living in a particular place, at
a particular time, and who were influenced—­as we all are—­by the world around them.17
   Many twentieth-century feminist scholars attempted to address the social construc-
tion of gender by treating gender as something separate from sex. But that distinction
is increasingly breaking down. Both gender and sex are social constructs, as it turns out.
Even sex, which today is sometimes still considered in biologically essential terms, has
a distinct cultural history. It can be traced to a place (Europe) and a time (the Enlight-
enment) when new theories about democracy and what philosophers called “natural
rights” began to emerge. Before then, there was a hierarchy of the sexes, with men on
the top and women on the bottom. (Thanks, Aristotle!18) But there wasn’t exactly a
binary distinction between those two (or any other) sexes. In fact, according to histo-
rian of sex and gender Thomas Laqueur, most people believed that women were just
inferior men, with penises located inside instead of outside of their bodies and that—­
for reals!—­could descend at any time in life.19
“What Gets Counted Counts”                                                                    101




Figure 4.2
(a) Facebook’s initial attempt to allow users to indicate additional genders, circa 2014. Image
courtesy of Slate. (b) Facebook’s updated gender field, circa 2018. Screenshot by Lauren F. Klein.
102                                                                                 Chapter 4




Figure 4.3
Detailed view of Facebook’s new account creation page, circa 2018. Note that you still have to
choose “Female” or “Male”—a binary choice—when you sign up. Screenshot by Lauren F. Klein.


   For the idea of a sex binary to gain force, it would take figures like Thomas Jefferson
declaring that all men were created equal, and entire countries like the United States
to be founded on that principle. Once that happened, political leaders began to worry
about what, exactly, they had declared: to whom did the principle of equality apply?
All sorts of systems for classifying people have their roots in that era—­not only sex but
also, crucially, race.20 Before the eighteenth century, Western societies understood race
as a concept tied to religious affiliation, geographic origin, or some combination of
“What Gets Counted Counts”                                                              103



both. Race had very little to do with skin color until the rise of the transatlantic slave
trade, in the seventeenth century.21 And even then, race was still a hazy concept. It
would take the scientific racism of the mid-­eighteenth century for race to begin to be
defined by Western societies in terms of black and white.
   Take Carl Linnaeus, for example, and the revolutionary classification system that
he is credited with creating.22 Linnaeus’s system of binomial classification is the one
that scientists still use to today to classify humans and all other living things. But Lin-
naeus’s system didn’t just include the category of homo sapiens, as it turns out. It also
incorrectly—­but as historians would tell you, unsurprisingly—­included five subcatego-
ries of humans separated by race. (One of these five was set aside for mythological
humans who didn’t exist in real life, in case you’re still ready to get behind his science.)
But Linnaeus’s classification system wasn’t even the worst of the lot. Over the course
of the eighteenth century, increasingly racist systems of classification began to emerge,
along with pseudosciences like comparative anatomy and physiognomy. These allowed
elite white men to provide a purportedly scientific basis for the differential treatment
of people of color, women, disabled people, and gay people, among other groups.
Although those fields have long since been discredited, their legacy is still visible in
instances as far-­ranging as the maternal health outcomes that we’ve already discussed,
to the divergent rates of car insurance that are offered to Black vs. white drivers, as
described in an investigation conducted by ProPublica and Consumer Reports.23 What’s
more, as machine learning techniques are increasingly extended into new domains of
human life, scientific racism is itself returning. Pointing to and debunking one machine
learning technique that employs images of faces in an attempt to classify criminals,
                                                    Blaise Agüera y Arcas, Margaret
three prominent artificial intelligence researchers—­
Mitchell, and Alexander Todorov—­have asserted that scientific racism has “entered
a new era.”24
   A simple solution might be to say, “Fine, then. Let’s just not classify anything or
anyone!” But the flaw in that plan is that data must be classified in some way to be
put to use. In fact, by the time that information becomes data, it’s already been classi-
fied in some way. Data, after all, is information made tractable, to borrow a term from
computer science. “What distinguishes data from other forms of information is that
it can be processed by a computer, or by computer-­like operations,” as Lauren has
written in an essay coauthored with information studies scholar Miriam Posner.25 And
to enable those operations, which range from counting to sorting and from model-
ing to visualizing, the data must be placed into some kind of category—­if not always
into a conceptual category like gender, then at the least into a computational category
like Boolean (a type of data with only two values, like true or false), integer (a type of
104                                                                             Chapter 4



number with no decimal points, like 237 or −1), or string (a sequence of letters or words,
like “this”).
   Classification systems are essential to any working infrastructure, as information
theorists Geoffrey Bowker and Susan Leigh Star have argued in their influential book
Sorting Things Out.26 This is true not only for computational infrastructures and con-
ceptual ones, but also for physical infrastructures like the checkout line at the grocery
store. Think about how angry a shopper can get when they’re stuck in the express
line behind someone with more than the designated fifteen items or less. Or, closer to
home, think of the system you use (or should use) to sort your clothes for the wash.
It’s not that we should reject these classification systems out of hand, or even that we
could if we wanted to. (We’re pretty sure that no one wants all their socks to turn pink.)
It’s just that once a system is in place, it becomes naturalized as “the way things are.”
This means we don’t question how our classification systems are constructed, what
values or judgments might be encoded into them, or why they were thought up in the
first place. In fact—­and this is another point made by Bowker and Star—­we often forget
to ask these questions until our systems become objects of contention, or completely
break down.
   Bowker and Star give the example of the public debates that took place in the 1990s
around the categories of race employed on the US Federal Census. At issue was whether
people should be able to choose multiple races on the census form. Multiracial people
and their families were some of the main proponents of the option, who saw it as a way
to recognize their multiple identities rather than forcing them to squeeze themselves
into a single, inadequate box. Those opposed included the Congressional Black Caucus
as well as some Black and Latinx civil rights groups that saw the option as potentially
reducing their representative voice.27 Ultimately, the 2000 census did allow people to
choose multiple races, and millions of people took advantage of it. But the debates
around that single category illustrate how classification gets complicated quickly, and
with a range of personal and political stakes.28
   Classification systems also carry significant material consequences, and the US Cen-
sus provides an additional example of that. Census counts are used to draw voting
districts, make policy decisions, and allocate billions of dollars in federal resources.
The recent Republican-­led proposal to introduce a question about citizenship status
on the 2020 census represents an attempt to wield this power to very pointed politi-
cal ends. Because undocumented immigrants know the risks, like deportation, that
come with being counted, they are less likely to complete the census questionnaire.
But because both political representation and federal funding are allocated according
to the number and geographic areas of people counted in the census, undercounting
“What Gets Counted Counts”                                                              105



undocumented immigrants (and the documented immigrants they often live with)
means less voting power—­and fewer resources—­accorded to those groups. This is a
clear example of what we term the paradox of exposure: the double bind that places
those who stand to significantly gain from being counted in the most danger from that
same counting (or classifying) act.
   In each of these cases, as is true of any case of not fitting (or not wanting to fit)
neatly into a box, it’s important to ask whether it’s the categories that are inadequate, or
whether—­and this is a key feminist move—­it’s the system of classification itself. Lurk-
ing under the surface of so many classification systems are false binaries and implied
hierarchies, such as the artificial distinctions between men and women, reason and
emotion, nature and culture, and body and world. Decades of feminist thinking have
taught us to question why these distinctions have come about; what social, cultural, or
political values they reflect; what hidden (or not so hidden) hierarchies they encode;
and, crucially, whether they should exist in the first place.


Questioning Classification Systems


Let’s spend some time with an actual person who has started to question the classifica-
tion systems that surround him: one Michael Hicks, an eight-­year-­old Cub Scout from
New Jersey. Why is Mikey, as he’s more commonly known, so concerned about classifi-
cation? Well, Mikey shares his name with someone who has been placed on a terrorist
watch list by the US federal government. As a result, Mikey has also been classified as
a potential terrorist and is subjected to the highest level of airport security screening
every time that he travels. “A terrorist can blow his underwear up and they don’t catch
him. But my 8-­year-­old can’t walk through security without being frisked,” his mother
lamented to Lizette Alvarez, a reporter for the New York Times who covered the issue
in 2010.29
   Of course, in some ways, Mikey is lucky. He is white, so he does not run the risk
of racial profiling—­unlike, for example, the many Black women who receive TSA pat-­
downs due to their natural hair.30 Moreover, Mikey’s name sounds Anglo-­European,
so he does not need to worry about religious or ethnic profiling either—­unlike, for
another example, people named Muhammad who are disproportionately pulled over
by the police due to their Muslim name.31 But Mikey the Cub Scout still helps to expose
the brokenness of some of the categories that structure the TSA’s terrorist classification
system; the combination of first and last name is simply insufficient to classify some-
one as a terrorist or not.
106                                                                               Chapter 4



   Or, consider another person with a history of bad experiences at the (literal) hands
of the TSA. Sasha Costanza-­Chock is nonbinary, like Maria Munir. They are also a
design professor at MIT, so they have a lot of experience both living with and think-
ing through oppressive classification systems. In a 2018 essay, “Design Justice, A.I.,
and Escape from the Matrix of Domination,” they give a concrete example of why
design justice is needed in relation to data.32 The essay describes how the seemingly
simple system employed by the operators of those hands-­in-­the-­air millimeter-­wave
airport security scanning machines is in fact quite complex—­and also fundamentally
flawed.
   Few cisgender people are aware of the fact that before you step into a scanning
machine, the TSA agent operating the machine looks you up and down, decides whether
you are a man or a woman, and then pushes a button to select the corresponding gen-
der on the scanner’s touchscreen interface. That human decision loads the algorithmic
profile for either male bodies or female ones, against which your body’s measurements
are compared. If your measurements diverge from the statistical norm of that gender’s
body—­whether the discrepancy is because you’re concealing a deadly weapon, because
your body doesn’t fit neatly into either of the two categories that the system has pro-
vided, or because the TSA agent simply made the wrong choice—­you trigger a “risk
alert.” Then, in an act of what legal theorist Dean Spade terms administrative violence,
you are subjected to the same full-­body pat-­down as a potential terrorist.33 Here it’s not
that the scanning machines rely upon an insufficient number of categories, as in the
case of Mikey the Cub Scout, or that they employ the wrong ones, as Mikey’s mom
would likely say. It’s that the TSA scanners shouldn’t rely on gender to classify air trav-
elers to begin with. (And while we’re going down that path, how about we imagine a
future without a state agency that systematically pathologizes Black women and trans
people and Cub Scouts in the first place?)
   So when we say that what gets counted counts, it’s folks like Sasha Costanza-­Chock
or Mikey Hicks or Maria Munir that we’re thinking about. Because flawed classifica-
tion systems—­like the one that underlies the airport scanner’s risk-­detection algorithm
or the one that determines which names end up on terrorist watch lists or simply
(simply!) the gender binary—­are not only significant problems in themselves, but also
symptoms of a more global condition of inequality. The matrix of domination, which
we introduced in chapter 1, describes how race, gender, and class (among other things)
intersect to enhance opportunities for some people and constrain opportunities for
others.34 Under the matrix of domination, normative bodies pass through scanners,
borders, and bathrooms with ease; these systems have been designed by people like
them, for people like them, with an aim—­sometimes explicit—­of keeping people not
like them out.35
“What Gets Counted Counts”                                                            107



  As these examples help to show, the forces that operate through the matrix of domi-
nation are sneaky and diffuse. And they show up everywhere—­even in pockets on
pants. A recent journalistic investigation of the size of pockets in eighty pairs of men’s
and women’s jeans confirmed what women (and men and nonbinary people who wear
women’s jeans) have been saying anecdotally for years: that their pants pockets just
aren’t big enough (figure 4.4).36 More specifically, the pockets of jeans designed for
women are 48 percent shorter and 6.5 percent narrower than the pockets of jeans
designed for men. This size does matter! According to the same study, only 40 percent




Figure 4.4
From “Someone Clever Once Said Women Were Not Allowed Pockets,” a comparative study of
pockets in women’s and men’s jeans by The Pudding (2018). Visualization by Jan Diehm and
Amber Thomas for The Pudding.
108                                                                              Chapter 4



of the front pockets of women’s jeans can fit a smartphone, and less than half “can
fit a wallet specifically designed to fit in front pockets.” Hence the thriving market for
women’s handbags (to hold the aforementioned front-­pocket wallet) and for replace-
ment smartphone screens (for when your phone invariably falls out of your too-­small
pocket and cracks).
   Now, the designers of any particular pair of women’s jeans are almost certainly not
thinking: “Let’s oppress women by making their pockets too small.” They are probably
only thinking about what looks nice. But what looks nice has a history too. Before
the seventeenth century, “pockets” were external sacks on strings that could be tied
above or below other garments. But starting in the 1600s, men’s clothing began to
feature internal pockets. Meanwhile, women’s clothing became increasingly close-­cut.
By the late eighteenth century, the women’s pocket reached its breaking point, result-
ing in emergence of a new fashion item called a reticule, otherwise known as a purse.
These tiny handbags were made out of cloth and, according to the Victoria and Albert
Museum’s helpful online history of pockets, could not hold very much.37 And yet, as
the museum curators point out, in an era in which most people shared all of their
shelves and dressers, these reticules were one of the few places for women to store any
items they wanted to keep to themselves. Fast forward to the present, and women (and
people who wear women’s fashion) must still carry their belongings outside of their
clothes and on public display. They’re also limited in their ability to use both of their
hands at the same time. It’s (mostly) a minor annoyance, but it’s one way among many
that the patriarchy—­a term that describes the combination of legal frameworks, social
structures, and cultural values that contribute to the continued male domination of
society—­inadvertently and invisibly reproduces itself. In this case, it’s pants—­perhaps
even the ones you’re wearing right now—­that compound and consolidate the patriar-
chy’s oppressive force.
   In addition to pants pockets, one of the other things that upholds the patriarchy
is, as it turns out, our ideas about gender itself. We’ve already asserted that gender is a
social construct, but what does this phrase really mean? Queer theorist Judith Butler
has long maintained that gender is best understood as a repeated performance, a set of
categories that cohere by, for instance, wearing jeans with small pockets (or no pock-
ets at all) or by participating in an activity that is similarly gender-­coded, like child-­
rearing, or—­importantly for Butler—­having heterosexual sex.38 These performative acts,
as she terms them, repeated so many times that they become taken as fact, are what
define the gender categories that we have today. Butler’s idea of gender as performative
moves away from an essentialist conception of the term: the idea that there is some
innate or “essential” criteria that makes one, for instance, a woman or man. But these
“What Gets Counted Counts”                                                             109



performances still reinforce the categories of gender, she reminds us, even if the actions
and activities that determine them are not innate.
   Gender is certainly complicated. This is one thing about which most contemporary
scholars of gender largely agree. Conceptions of gender in health and clinical fields are
also evolving as well. For example, the American Medical Association now calls gen-
der a “spectrum” rather than a binary, and as of 2018 it issued a firm statement that
“sex and gender are more complex than previously assumed.”39 But it’s important to
remember that there have always been more variations in gender identity and expres-
sion than most Anglo-­Western societies have cared to acknowledge or to collectively
remember. This is evidenced in the range of regional and vernacular terms, such as
kothi, hijra, and dhurani, that are currently used to describe the genders of people across
South Asia that fall outside the binary; we see it in the additional umbrella terms, such
as two-­spirit, that describe people in some North American Indigenous communities;
and many more.40 Not to mention that some people are gender-­fluid, meaning their
gender identity may shift from day to day, year to year, or situation to situation. And
yet—­at least in a US context—­gender data is still almost always collected in the binary
categories of “male” and “female” and visually represented by some form of binary
division as well.41 This remains true even as a 2018 Stanford study found that, when
given the choice among seven points on a gender spectrum, more than two-­thirds of
the subjects polled placed themselves somewhere in the middle.42
   As survey designers, and data scientists more generally, there would seem to be an
obvious response to the Stanford report: collect gender data in more than binary cat-
egories, making sure to disaggregate the data—­that is, compare the data by genders
during the analysis phase. One recent alternative to the binary, developed by Public
Health England in collaboration with LGBTQ+ organizations in the United Kingdom,
is in evidence in figure 4.5. This two-­item questionnaire was designed for use in rou-
tine national surveillance of HIV in England and Wales to determine self-­identified
gender and cis or trans status in a public health context. The designers offer three
named genders, a catch-­all fourth category, and an option for not disclosing gender
identity. In a separate question, they ask about gender at birth, again giving an option
for not disclosing. The survey design uses sensitive wording and inclusive terminology
to allow trans and genderqueer populations to be counted. These questions are being
considered for expanded use across other national health records and data collection
systems in the United Kingdom.
   Should all future gender data collection use this model? Not necessarily, and here’s
why: In a world in which quantification always leads to accurate representation, and
accurate representation always leads to positive change, then always counting gender
110                                                                                    Chapter 4




Figure 4.5
From the Positive Voices survey of people living with HIV in England and Wales developed by
Public Health England in collaboration with several partner organizations. This represents current
best practices for collecting nonbinary gender data in an Anglo-­Western public health context,
but it’s still important to recognize that different decisions might be warranted depending on the
context. Courtesy of Peter Kirwin, Public Health England, 2018.


identities outside the binary makes perfect sense. But being represented also means
being made visible, and being made visible to the matrix of domination—­which con-
tinuously develops laws, practices, and cultural norms to police the gender binary—­
poses significant risks to the health and safety of minoritized groups. Under the current
administration in the United States, for example, transgender people are banned from
serving in the military and, once identified as such, denied access to certain forms
of healthcare.43 This demonstrates some of the risks of having one’s gender counted
as something other than man or woman—­risks that can occur in many contexts,
depending on what data are being collected, by whom, and whether they are person-
ally identifiable (or easily deanonymized). It’s also important to recognize how trans
and nonbinary people may possibly be identified even within otherwise large datasets
simply because there are fewer of them relative to the larger population. This possibil-
ity poses additional risks, in the form of unwanted attention in the case of people who
would prefer not to disclose their gender identity, or in the form of discrimination,
violence, or even imprisonment, depending on the place they live.
   As data scientists, what should we do amid these potential harms? Depending on
the circumstances and the institution that is doing the collecting, the most ethical
decision can vary. It might be to avoid collecting data on whether someone is cis or
transgender, to make all gender data optional, to not collect gender data at all, or even
“What Gets Counted Counts”                                                            111



to stick with binary gender categories. Social computation researcher Oliver Haimson
has asserted that “in most non-­health research, it’s often not necessary to know par-
ticipants’ assigned gender at birth.”44 Heath Fogg Davis agrees: his book Beyond Trans
argues that we don’t need to classify people by sex on passports and licenses, for bath-
rooms or sports, among other things.45 By contrast, J. Nathan Matias, Sarah Szalavitz,
and Ethan Zuckerman chose to keep gender data in binary form for their application
FollowBias, which detects gender from names, in order to avoid making a person’s gen-
der identity public against their wishes.46
   The ethical complexity of whether to count gender, when to count gender, and
how to count gender illuminates the complexity of acts of classification against the
backdrop of structural oppression. Because when it comes to data collection, and the
categories that structure it, there are power imbalances up and down, side to side, and
everywhere in between. Because of these asymmetries, data scientists must proceed
with awareness of context (discussed further in chapter 6) and an analysis of power in
the collection environment (discussed further in chapter 1) to determine whose inter-
ests are being served by being counted, and who runs the risk of being harmed.


Rethinking Binaries in Data Visualization


A feminist critique of counting, and of the binary classification systems that often
structure those acts, is not limited to a focus on gender alone. A binary logic also
pervades our thinking about race, for example, as feminist scholars Brittney Cooper
and Margaret Rhee explain. Drawing from ideas about intersectionality, they call for
“hacking” the Black/white binary that, on the one hand, helps to expose the racism
experienced by Black people in the United States and, on the other, erases the other
forms of racism experienced by Indigenous as well as Latinx, Asian American, and
other minoritized groups. “Binary racial discourses elide our struggles for justice,” they
state plainly.47 By challenging the binary thinking that erases the experiences of certain
groups while elevating others, we can work toward more just and equitable data prac-
tices and consequently toward a more just and equitable future.
   Sometimes, however, the goal of challenging binary thinking can be constrained
by the realities of the field. Visualization designers, for example, do not typically have
control over the collection practices of the data they are asked to visualize. They often
inherit binary data that they then need to “hack” from within. What might this look
like? We might point to the reporters on the Lifestyle Desk of the Telegraph, a British
newspaper, who, in March 2018, were considering how to honor International Wom-
en’s Day and were struck by the significant gender gap in the United Kingdom in terms
112                                                                                   Chapter 4



of education, politics, business, and culture.48 As journalists, they were working with
multiple sources of data collected by other agencies, which all came in binary form. But
they wanted to ensure that they didn’t further reinforce any gender stereotypes. They
paid particular attention to color. One line of designer logic would favor cultural con-
vention for interpretability, like using pink for women and blue for men, but a feminist
line would use color choices to hack those same conventions (figure 4.6).
   Pink and blue is, after all, another hierarchy, and the goal of the Telegraph team
members was to mitigate inequality, not reinforce it. So they took a different source
for inspiration: the Votes for Women campaign of early twentieth-­century England, in
which purple was employed to represent freedom and dignity and green to represent




Figure 4.6
“Born Equal. Treated Unequally” was an interactive feature in the Telegraph in 2018 that exam-
ined the gender gap in the United Kingdom along a number of dimensions. Although the authors
treated gender as a binary category, they used color to challenge stereotypically man/woman color
coding. Feature by Claire Cohen, Patrick Scott, Ellie Kempster, Richard Moynihan, Oliver Edging-
ton, Dario Verrengia, Fraser Lyness, George Ioakeimidis, and Jamie Johnson, for the Telegraph.
“What Gets Counted Counts”                                                               113



hope. When thinking about which of these colors to assign to each gender, they took
a perceptual design principle as their guide: “Against white, purple registers with far
greater contrast and so should attract more attention when putting alongside the green
[sic], not by much but just enough to tip the scales. In a lot of the visualisations men
largely outnumber women, so it was a fairly simple method of bringing them back into
focus,” Fraser Lyness, the Telegraph’s director of graphic journalism told visualization
designer Lisa Charlotte Rost.49 Here, one hierarchy—­the hierarchy in which colors are
perceived by the eye—­was employed to challenge another one: the hierarchy of gender.
When put into practice, this simple method had the result of communicating clearly
without reinforcing stereotypes.
   But the Telegraph journalists could have gone one step further to rethink binaries.
They had an opportunity to communicate to the public that gender is not a binary
by spelling that out—­in the text of the story or in a caption under the graphics or
by showing visually that there was no data for nonbinary people. Their colleagues at
the Guardian recently adopted this latter strategy in their interactive piece “Does the
New Congress Reflect You?” about the 2018 US midterm elections.50 The piece presents
three categories: cis male, cis female, and trans + nonbinary. When you click on “trans
+ nonbinary,” as in figure 4.7, the interactive map displays all of the districts in grey,
because “0 people in Congress are like you.” The absence of data becomes an important
takeaway, as meaningful as the data themselves.51
   These examples have shown gender as a dimension of analysis, but how might we
visually represent gender itself? This is a challenge of visualizing complexity of the high-
est degree, and Amanda Montañez, a designer for Scientific American, took this challenge
head on (figure 4.8). She was tasked with creating an infographic to accompany an
article on the evolving science of gender and sex—­categories that she, like most people,
viewed as distinct but related.52 As she explains in a blog post on the Scientific American
website, she first envisioned a simple spectrum, or perhaps two spectrums: one for sex
and one for gender.53 But she soon found confirmation of what we’ve been saying so far
in this chapter: that few things in life can be truly reduced to binaries, and that insisting
on binary categories of data collection—­with respect to gender, to sex, to their relation,
or to anything else—­fails to acknowledge the value of what (or who) rests in between
and outside.
   We have already established that gender is more than binary; but it’s less commonly
acknowledged that sex is more than a binary too. As feminist biologist Anne Fausto-­
Sterling confirms, “There is no single biological measure that unassailably places each
and every human into one of two categories—­male or female.”54 Intersex people, who
constitute an estimated 1.7 percent of the population, may have ovaries and a penis,
114                                                                                  Chapter 4




Figure 4.7
“Does the New Congress Reflect You?” is a 2018 interactive that appeared in the Guardian. Users
select their own demographic characteristics to see how many people like them are in the 2018
Congress. Clicking on “trans + nonbinary” leads to a blank map showing zero people in Congress
like you. Image by Sam Morris, Juweek Adolphe, and Erum Salam for the Guardian.


or “mosaic genetics” in which some of one’s cells have XX chromosomes and some
have XY.55 It’s also increasingly acknowledged that that sex, like gender, and some-
times together with gender, is multilayered and continuously unfolding throughout a
person’s life.
   To begin to represent this complexity, Montañez had to begin by rejecting much of
the data and research that she and her research assistant turned up, either on account
of flawed categories or on account of flawed collection practices. She decided to focus
on sex, and after an extensive design process, which included consulting with domain
experts, Montañez and the design firm Pitch Interactive, which helped finalize the
diagram, arrived at the result. Beyond XX and XY is a complex diagram, which employs
a color spectrum to represent the sex spectrum, a vertical axis to represent change over
“What Gets Counted Counts”                                                                 115



time, and branching arrows to connect to text blocks that provide additional con-
textual information. The design offers a beautifully executed visual challenge to the
scientifically incorrect idea that there are only two sexes, and even that the concepts of
sex and gender are wholly distinct. Visualization is often thought of as a way to reduce
complexity, but here it operates in the reverse—­to push simple, oppressive ideas to be
more complex, nuanced, and just.


Refusing Data, Recovering Data


Montañez’s graphic made what was already counted count. In other words, she took
what scientists and theorists knew to be true about the nature of sexual differentiation
and made that knowledge more accessible and public. But counting in itself is not
necessarily an unmitigated good, nor is putting it on public display. We have already
introduced the idea of the paradox of exposure where people are harmed by being
made visible to a system. But because system designers from dominant groups do not
experience the harms of being counted or of being made visible without consent—­this
is the privilege hazard, once again—­they rarely anticipate these needs or account for
them in the design process. This is the reason that questions about counting must be
accompanied by questions about consent, as well as of personal safety, cultural dignity,
and historical context.
   It’s Facebook, once again, that helps to prove this point. Information studies schol-
ars Oliver Haimson and Anna Lauren Hoffman have studied the effects of the com-
pany’s “real name” policy, under which the platform determines each user’s registered
name to be either “real” and authentic or simply “fake.”56 (In our teacher voices, we
now say: Does anyone note the problem with this binary thinking here?) Haimson and
Hoffman point out that trans and queer people may choose to have multiple online
identities, which may be fluid and contextual and possibly necessary to protect them-
selves. As another example, abuse survivors may need to take steps to make themselves
unfindable through search, even as they still want to be connected to their loved ones.



Figure 4.8 (following two pages)
Beyond XX and XY (2017) visualizes the known factors that contribute to sexual differentia-
tion at different stages of human life, from conception to birth to puberty and beyond. Con-
trary to received wisdom, sex is not a binary that is fixed at birth, but rather a layered and
time-­based process of differentiation, with more than two possible outcomes. Reproduced with
permission. Copyright © 2017 Scientific American, a division of Nature America, Inc. All rights
reserved.
118                                                                            Chapter 4



   Compounding the contextual nature of these factors, Facebook enforces its real
name policy algorithmically—­flagging names with “too many” words or with unusual
capital letters. Haimson and Hoffman note that Facebook’s algorithms disproportion-
ately flag Native American names for violation because those names often differ in
structure and form from Anglo-­Western names (the subject position of the systems’
designers, and therefore presumed to be the default; the privilege hazard once again).
What’s more, users can also report other users for not having real names, resulting in—­
for example—­a single person systematically targeting several hundred drag queens’
profiles for removal. Facebook claims that the real name policy exists for safety, but
Haimson and Hoffman clearly show that the policy actively imperils the safety of some
of the platform’s most marginalized users. As we’ve already begun to suggest, some-
times the most ethical thing to do is to help people be obscure, hidden, and invisible.57
The example of Facebook demonstrates the fundamental importance of obtaining con-
sent when counting and of enabling individuals to refuse acts of counting and classifi-
cation in light of potential harms.
   Acts of counting and classification, especially as they relate to minoritized groups,
must always balance harms and benefits. When data are collected about real people
and their lives, risks ranging from exposure to violence are always present. But when
deliberately considered, and when consent is obtained, counting can contribute to
efforts to increase valuable and desired visibility. The Colored Conventions Project
(CCP), led by a team of students and faculty at the University of Delaware, demon-
strates how to thoughtfully navigate this balance in the present by looking at the
past.58 Among the goals of the project is to create a machine-­readable corpus of meet-
ing minutes from the nineteenth-­century Colored Conventions: events in which Black
Americans, fugitive and free, gathered to strategize about how to achieve legal, social,
economic, and educational justice. These meeting minutes are valuable because they
have yet to be counted, so to speak, in the stories commonly told about the movement
to end slavery in the nineteenth-­century United States. Those stories tend to privilege
the actions of white abolitionists because theirs were the stories that were recorded
in print. But the Colored Conventions help to document the vital role of the Black
activists who were working within their own communities to end slavery and achieve
liberation.
   The creation of the corpus enables these important activists to be counted, as well
as have their words (as recorded in the meeting minutes) analyzed and incorporated
into the historical record. But the process of converting the meeting minutes into data
strongly recalls the original violence that accompanied the slave trade, when human
      in fact, the very ancestors of these activists—­
lives—­                                              were reduced to numbers and
“What Gets Counted Counts”                                                              119



names. In recognition of this irreconcilable tension, the CCP requires that all those
who download the corpus commit to a set of principles, including “a use of data that
humanizes and acknowledges the Black people whose collective organizational histo-
ries are assembled” in the corpus, and a request to “contextualize and narrate the con-
ditions of the people who appear as ‘data’ and to name them when possible.”59
   There is a second tension that the CCP navigates in an exemplary fashion, which
has to do with the content of the corpus itself. Because it is derived from the conven-
tions’ official meeting minutes, it records only the “official” participants in the con-
ventions and the discussions they initiated. These participants were almost exclusively
men. To address this disparity, the CCP team asks its teaching partners to sign a Memo
of Understanding (MoU) before introducing students to the project. The MoU requests
that all instructors introduce a woman involved in the conventions, such as a wife,
daughter, sister, or fellow church member, alongside every male delegate who is named
(figure 4.9).60 From this work of recovery, the CCP is creating a second dataset of the
women’s names—­those who would otherwise go uncounted and therefore unrecog-
nized for their work. They are using data collection to make these contributions count.




Figure 4.9
An engraving of an 1869 Colored Convention, published in Harper’s Weekly, showing men at the
podium and women seated and standing in the rear. Image courtesy of Jim Casey.
120                                                                                    Chapter 4



Counting as Healing, Counting as Accountability


In the nineteenth century, as today, so many of the disparities introduced into data­
sets had to do with much larger and much more profound asymmetries of power. The
asymmetries are often directly reflected in the power dynamics between who is doing
the counting and who is being counted. But when a community is counting for itself,
about itself, there is the potential that data collection can be not only be empowering
but also healing. One example of this that draws from the personal experience of one
of the authors of this book. It was 2014, and Catherine was a student and nursing her
baby daughter at the time, as well as struggling to pump breastmilk for her in unsavory
places like server rooms and bathroom floors. Frustrated, she and six student colleagues
came together to publish a call for ideas and stories that could help to improve breast
pump technology.61 These stories led to a research paper about breast pump design, as
well as the creation of the Make the Breast Pump Not Suck Hackathon (figure 4.10)—­an




Figure 4.10
The 2018 Make the Breast Pump Not Suck Hackathon was the second gathering of the community
at MIT and focused on racial equity in breastfeeding, as well as shifting paid leave policy in the
United States. Photo by Rebecca Rodriguez and Ken Richardson, MIT Media Lab.
“What Gets Counted Counts”                                                              121



ongoing forum for sharing stories, hacking pumps, and reengineering the postpartum
ecosystem that surrounds them.62
   Although innovation spaces had long been holding hackathons for health technol-
ogy, the 2014 event was one of the first about birth and breastfeeding. As such, it led
to participants sharing stories in a space that was (temporarily) free of the stigma sur-
rounding breastfeeding. These stories pointed to common experiences and patterns in
the spirit of “the personal is political” consciousness-­raising events. Participants recog-
nized these stories as data that could be used—­and in fact were used—­to demand more
from breast pump makers, from workplaces, and from society, to help transform the
self-­blame that women often experience as a result of difficulties with birth and breast-
feeding into collective political action.63
   But action by whom, and action for whom? Following the 2014 event, we (meaning
the organizers) reflected on its successes and its limitations—­in particular, its lack of
an intersectional approach.64 In the United States, maternal health carries significant
race and class inequities, as discussed in chapter 1. The first hackathon did not con-
sider those inequities; it centered the needs of some the most privileged mothers and
produced designs that favored their experiences. We decided to try again. In 2017 and
2018, we multiplied the single event into a participatory research project, a policy sum-
mit, and a community innovation program, as well as a hackathon. In all of these, we
deliberately centered the needs and the participation of parents of color, low-­income
parents, and LGBTQ+ parents. When we arrived at the hackathon the second time
around, it was the result of over a year of relationship building and identity work on
the part of the organizers with our community partners.
   Ensuring that the 2018 hackathon would fully welcome the participation of these
families required multiple forms of accountability. Guided by Jenn Roberts, our lead
organizer for equity and inclusion, we wrote a values statement and convened an advi-
sory board with leaders in breastfeeding, equity, and maternal health. We also devel-
oped a set of metrics to shape the demographics of the event.65 These metrics were
designed to prioritize racial diversity, gender diversity, diversity of sexual orientation,
geographic diversity, and domain diversity, with additional priority given for young
people and newcomers. On the application form, potential participants were encour-
aged to self-­identify their gender and race, specify their location, and choose mul-
tiple options from a list of predefined domain expertise categories (like “parent” or
“designer/artist”). We also invited them to write about why they wanted to attend, and
if they chose to disclose information about their sexual orientation or their financial
position, then we considered that information in the process.
122                                                                              Chapter 4



   Were these categories reductive? Of course they were. No person can fit their whole
self into a form, regardless of how many blank text fields are provided. Did the form
reflect the true nature of each person’s intersecting identities and how those identities
impact that person’s being in the world? The answer to this question is also unsurpris-
ing: of course it did not. But the process of collecting this demographic data—­which
was, crucially, undertaken voluntarily and from within the community itself—­resulted
in an event that was indeed guided by the knowledge and experience of the groups that
our coalition had hoped to center.66
   Catherine shared this experience with Lauren as we were beginning to draft this
book, and we decided to use a similar process to help hold ourselves accountable to
the values that we wanted to inform Data Feminism and the criteria by which certain
projects and texts would be selected for inclusion. We determined specific numbers and
percentages that, in our view, would help keep us accountable to those values, as well as
the categories of data collection that would be required to determine whether the met-
rics had been met. (These are viewable in the appendix, Our Values and Our Metrics for
Holding Ourselves Accountable.) At two phases in the process—­first when we posted
the draft of the manuscript online, and second after we submitted the manuscript for
copyediting—­one of our research assistants, Isabel Carter, audited the projects and cita-
tions of the book. (They describe their research methods in more detail in “Auditing
Data Feminism,” included as another appendix.) As with the hackathon, these metrics
were not the only method we employed for holding ourselves accountable. We also
interviewed the creators of many of the projects we reference, cleared our quotes and
portrayals of their work with them, and published a draft of the book online for open
peer review, among other approaches.
   Was our method of counting perfect? Of course not. We are certain we have made
mistakes. This is among the reasons that we decided to keep our disaggregated data
private, even as we published the aggregated results. What about the idea to count
people and projects in the first place? Shouldn’t that be viewed as contributing to the
same reduction in complexity that we have argued against thus far in this book? As this
chapter has demonstrated, counting is always complicated. But undertaken deliber-
ately, tailored to specific goals, and with issues of privacy and potential harms always in
mind, counting can be used to support accountability—­as one method, among many,
of working toward a larger goal.


Rethink Binaries and Hierarchies


Counting and classification can be powerful parts of the process of creating knowledge.
But they’re also tools of power in themselves. Historically, counting and classification
“What Gets Counted Counts”                                                           123



have been used to dominate, discipline, and exclude. This is where the fourth principle
of data feminism, rethink binaries and hierarchies, enters in. The gender binary offers a
key example of how classification systems are constructed by cultures and societies and
reflect both their values and their biases. The cases of the TSA airport scanners, Face-
book user profiles, and plain old pants show us how gender and sex binaries—­along
with scientifically incorrect understandings of both gender and sex—­get encoded into
technical systems (and also jeans)! Those systems, in turn, recirculate erroneous and
harmful ideas.
  An intersectional feminist approach to counting insists that we examine and, if
necessary, rethink the assumptions and beliefs behind our classification infrastructure,
as well as consistently probe who is doing the counting and whose interests are served.
Counting and measuring do not always have to be tools of oppression. We can also use
them to hold power accountable, to reclaim overlooked histories, and to build collec-
tivity and solidarity. When we count within our own communities, with consideration
and care, we can work to rebalance unequal distributions of power.
5    Unicorns, Janitors, Ninjas, Wizards, and Rock Stars


Principle: Embrace Pluralism

Data feminism insists that the most complete knowledge comes from synthesizing multiple
perspectives, with priority given to local, Indigenous, and experiential ways of knowing.




In Spring 2017, Bloomberg News ran an article with the provocative title “America’s
Rich Get Richer and the Poor Get Replaced by Robots.”1 Using census data, the authors
reported that income inequality is widening across the nation. San Francisco is leading
the pack, with an income gap of almost half a million dollars between the richest and
the poorest twenty percent of residents. As in other places, the wealth gap has race and
gender dimensions. In the Bay Area, people of color earn sixty-­eight cents for every
dollar earned by white people, and 59 percent of single mothers live in poverty.2 San
Francisco also has the lowest proportion of children to adults in any major US city,
and—­since 2003—­an escalating rate of evictions.
    Although the San Francisco Rent Board collects data on these evictions, it does not
track where people go after they are evicted, how many of those people end up home-
less, or which landlords are responsible for systematically evicting major blocks of the
city. In 2013, the Anti-­Eviction Mapping Project (AEMP) stepped in. The initiative is
a self-­described collective of “housing justice activists, researchers, data nerds, artists,
and oral historians.” It is a multiracial group with significant, though not exclusive,
project leadership by women. The AEMP is mapping eviction, and doing so through a
collaborative, multimodal, and—­yes—­quite messy process.
    If you visit antievictionmap.com, you won’t actually find a single map. There
are seventy-­eight distinct maps linked from the homepage: maps of displaced resi-
dents, of evictions, of tech buses, of property owners, of the Filipino diaspora, of the
declining numbers of Black residents in the city, and more. The AEMP has a distinct
way of working that is grounded in its stated commitment to antiracist, feminist,
and decolonial methodologies.3 Most of the projects happen in collaboration with
nonprofits and community-­based organizations. Additional projects originate from
126                                                                              Chapter 5



within the collective. For example, the group is working on producing an atlas
of the Bay Area called Counterpoints: Bay Area Data and Stories for Resisting Displace-
ment, which will cover topics such as migration and relocation, gentrification and the
prison pipeline, Indigenous and colonial histories of the region, and speculation about
the future.4
   One of the AEMP’s longest-­standing collaborations is with the Eviction Defense Col-
laborative (EDC), a nonprofit that provides court representation for people who have
been evicted. Although the city does not collect data on the race or income of evictees,
the EDC does collect those demographics, and it works with 90 percent of the tenants
whose eviction cases end up in San Francisco courts.5 In 2014, the EDC approached
the AEMP to help produce its annual report and in return offered to share its demo-
graphic data with the organization.6 Since then, the two groups have continued to
work together on EDC annual reports, as well as additional analyses of evictions with
a focus on race. The AEMP has also gone on to produce reports with tenants’ rights
organizations, timelines of gentrification with Indigenous students, oral histories with
grants from anthropology departments, and murals with arts organizations, as well as
maps and more maps.
   Some of the AEMP’s maps are designed to leverage the ability of data visualization
to make patterns visible at a glance. For example, the Tech Bus Stop Eviction Map pro-
duced in 2014 plots the locations of three years of Ellis Act evictions.7 This is a form of
“no-­fault” eviction in which landlords can claim that they are going out of the rental
business. In many cases, this is so that they can convert the building to a condominium
and sell the units at significant profit. San Francisco has seen almost five thousand
invocations of the Ellis Act since 1994. On the map (figure 5.1), the AEMP plotted Ellis
Act evictions in relationship to the location of technology company bus stops. Starting
in the 2000s, tech companies with campuses in Silicon Valley began offering private
luxury buses as a perk to attract employees who wanted to live in downtown San Fran-
cisco but didn’t want the hassle of commuting. Colloquially known as “the Google
buses” because Google was the most visible company to implement the practice, these
vehicles use public bus stops—­illegally at first—­to shuttle employees to their offices in
comfort (and also away from the public transportation system, which would otherwise
reap the benefits of their fares).8 Because a new, wealthy clientele began to seek condos
near the bus stops, property values soared—­and so did the rate of evictions of long-­time
neighborhood residents. The AEMP analysis showed that, between 2011 and 2013, 69
percent of no-­fault evictions occurred within four blocks of a tech bus stop. The map
makes that finding plain.
Unicorns, Janitors, Ninjas, Wizards, and Rock Stars                                             127




Figure 5.1
Tech Bus Stop Eviction Map by the Anti-­Eviction Mapping Project (2014) plots evictions with
“Google bus stops” in San Francisco. The group’s analysis showed that 69 percent of no-­fault
evictions in the city occurred within four blocks of a tech bus stop. Courtesy of the Anti-­Eviction
Mapping Project.
128                                                                             Chapter 5



   But other AEMP maps are intentionally designed not to depict a clear correlation
between evictions and place. In Narratives of Displacement and Resistance (figure 5.2a),
five thousand evictions are each represented as a differently sized red bubble, so the
base map of San Francisco is barely visible underneath.9 On top of this red sea, sky-­blue
bubbles dot the locations where the AEMP has conducted interviews with displaced
residents, as well as activists, mediamakers, and local historians. Clicking on a blue
bubble sets one of the dozens of oral histories in motion—­for instance, the story of
Phyllis Bowie (figure 5.2b), a resident facing eviction from her one-­bedroom apart-
ment. “I was born and raised in San Francisco proudly,” she begins. Bowie goes on to
recall how she returned from the Air Force and worked like crazy for two years at her
own small business, building up an income record that would make her eligible for a
lease-­to-­own apartment in Midtown, the historically Black neighborhood where she
had grown up. In 2015, however, the city broke the master lease and took away the
rent control on her building. Now tenants like Bowie, who moved there with the prom-
ise of homeownership, are facing skyrocketing rents that none of them can afford.
Bowie is leading rent strikes and organizing the building’s tenants, but their future
is uncertain.
   This uncertainty is carried over into the design of the map, which uses a phenome-
non that is often discouraged in data visualization—­occlusion—­to drive home its point.
Occlusion typically refers to the “problem” that occurs when some marks (like the evic-
tion dots) obscure other important features (like the whole geography of the city). But
here it underscores the point that there are very few patterns to detect when the entire
city is covered in big red eviction bubbles and abundant blue story dots. Put another
way, the whole city is a pattern, and that pattern is the problem—­much more than a
problem of information design.
   In this way, the Narratives map enacts dissent from San Francisco city policies in the
same way that it enacts dissent from the conventions of information design. It refuses
both the clarity and cleanliness associated with the best practices of data visualization
and the homogenizing and “cleanliness” associated with the forces of gentrification
that lead to evictions in the first place.10 The visual point of the map is simple and
exhortative: there are too many evictions. And there are too many eviction stories. The
map does not efficiently reveal how evictions data may be correlated with Bay Area
Rapid Transit (BART) stops, income, Google bus stops, or any other potential dimen-
sions of the data. Even finding the Narratives map is difficult, given the sheer number
of maps and visualizations on the AEMP website. There is no “master dashboard” that
integrates all the information that the AEMP has collected into a single interface.11
Figure 5.2
Narratives of Displacement and Resistance (2018) by the Anti-­Eviction Mapping Project (a), includ-
ing a detail view from Phyllis Bowie’s story (b). Courtesy of the Anti-­Eviction Mapping Project.
Made in collaboration with the San Francisco Ruth Assawa School of the Arts. The interview was
shot by Marianne Maeckelbergh and Brandon Jourdan and edited by students Shilo Arkinson and
Avidan Novogrodsky-­Godt, and facilitated by Alexandra Lacey and Jin Zhu.
130                                                                                Chapter 5



But all these design choices reinforce the main purpose of the AEMP: to document the
effects of displacement and to resist it through critical and creative means.
   The AEMP thus offers a rich set of examples of feminist counterdata collection and
countervisualization strategies. Individually and together, the maps illustrate how
Katie Lloyd Thomas, founding member of the feminist art architecture collective taking
place, envisions “partiality and contestation” taking place in graphical design. “Rather
than tell one ‘true’ story of consensus, [the drawing/graphic] might remember and
acknowledge multiple, even contradictory versions of reality,” she explains.12 The Nar-
ratives map populates its landscape with these contradictory realities. It demonstrates
that behind each eviction is a person—­a person like Bowie, with a unique voice and a
unique story. The voices we hear are diverse, multiple, specific, divergent—­and delib-
erately so.
   In so doing, the Narratives map, and the AEMP more generally, exemplify the fourth
principle of data feminism and the theme of this chapter: embrace pluralism. Embracing
pluralism in data science means valuing many perspectives and voices and doing so at
all stages of the process—­from collection to cleaning to analysis to communication.
It also means attending to the ways in which data science methods can inadvertently
work to suppress those voices in the service of clarity, cleanliness, and control. Many
of our received ideas about data science work against pluralistic meaning-­making proc-
esses, and one goal of data feminism is to change that.


Strangers in the Dataset


Data cleaning holds a special kind of mythos in data science. “It is often said that 80%
of data analysis is spent on the process of cleaning and preparing the data,” writes
Hadley Wickham in the first sentence of the abstract of “Tidy Data,” his widely cited
paper from 2014.13 Wickham is also the author of the tidyr package for R, a popular
programming language and statistical computing platform. (Packages are prewritten
collections of functions, and other forms of code, that can be installed and used in any
project.) Wickham’s package shares the sentiment expressed in his paper: that data are
inherently messy and need to be tidied and tamed.
   Wickham is not alone in this belief. Since the release of his tidyr package, Wickham
has gone on to create the “tidyverse,” an expanded set of packages that formalize a
clear workflow for processing data, which have been developed by a team of equally
enthusiastic contributors. Articles in the popular and business press corroborate this
insistence on tidiness, as well as its pressing need. In Harvard Business Review, the work
of the data scientist is glorified in terms of this tidying function: “At ease in the digital
Unicorns, Janitors, Ninjas, Wizards, and Rock Stars                                     131



realm, they are able to bring structure to large quantities of formless data and make
analysis possible.”14 Here, the intrepid analyst wrangles orderly tables from unstruc-
tured chaos. According to the article, it’s “the sexiest job of the 21st century.”15 But for
the New York Times, the data analyst’s work is far less attractive. A 2014 article equated
the task of cleaning data to the low-­wage maintenance work of “janitors.”16
   Whether or not you think data scientists are sexy (they are), and whether or not you
think janitors should be offended by this classist reference (we all should be), certain
assumptions and anxieties remain consistent across these different articulations about
the need for tidiness, cleanliness, and order, and the qualities of the people who should
be doing this work. They must be able to tame the chaos of information overload. They
must “scrub” and “cleanse” dirty data. And they must undertake deliberate action—­
either extraordinary or mundane—­to put data back in their proper place.
   But what might be lost in the process of dominating and disciplining data? Whose
perspectives might be lost in that process? And, conversely, whose perspectives might
be additionally imposed? The ideas expressed by Wickham, and by the press, carry the
assumption that all data scientists, in all contexts, value cleanliness and control over
messiness and complexity. But as the example of the AEMP demonstrates, these are not
the requirements, nor the goals, of all data projects.
   Before moving forward, we find it important to acknowledge that these ideas about
cleanliness and control contain troubling traces of a movement from a prior era: eugen-
ics, the upsetting, nineteenth-­century source of much of modern statistics. As we have
referenced in previous chapters via the work of Dean Spade and Rori Rohlfs, many of
the men often cited as the earliest statistical luminaries, such as Karl Pearson, Adolphe
Quetelet, Francis Galton, and Ronald Fisher, were also leaders in the eugenics move-
ment.17 In her book Ghost Stories for Darwin: The Science of Variation and the Politics of
Diversity, postcolonial science studies scholar Banu Subramaniam details how over the
course of the late nineteenth and early twentieth centuries, as statistics became the
preferred language of communication between biologists and social scientists, certain
ideas from the eugenics movement also carried over into this broader scientific con-
versation.18 While the most odious aspects of these ideas have been largely (and thank-
fully) stripped away, certain core principles—­like a generalized belief in the benefit of
control and cleanliness—­remain.19 To be clear: the point here is not that anyone who
cleans their data is perpetuating eugenics.20 The point, rather, is that the ideas underly-
ing the belief that data should always be clean and controlled have tainted historical
roots. As data scientists, we cannot forget these roots, even as the ideas themselves have
been tidied up over time.
132                                                                              Chapter 5



   This is the long history that library and information studies scholars Katie Raw-
son and Trevor Muñoz likely allude to in their assertion that “the cleaning paradigm
assumes an underlying, ‘correct’ order.” Like Subramaniam, but in the context of clean-
ing data more specifically, Rawson and Muñoz caution that cleaning can function as
a “diversity-­hiding trick.”21 In the perceived messiness of data, there is actually rich
information about the circumstances under which it was collected. Data studies scholar
Yanni Loukissas concurs. Rather than talking about datasets, he advocates that we talk
about data settings—­his term to describe both the technical and the human processes
that affect what information is captured in the data collection process and how the
data are then structured.22
   As an example of how the data setting matters, Loukissas tells the story of being at a
hackathon in Cambridge, Massachusetts, where he began to explore a dataset from the
Clemson University Library, located in South Carolina. He stumbled across a puzzling
record in which the librarian had noted the location of the item as “upstate.” Such a
designation is, of course, relational to the place of collection. For South Carolinians,
upstate is a very legible term that refers to the westernmost region of the state, where
Clemson is located. But it does not hold the same meaning for a person from New York,
where upstate refers to its own northern region, nor does it hold the same meaning for
a person sitting at a hackathon in Massachusetts, which does not have an upstate part
of the state. Had someone at the hackathon written the entry from where they sat,
they might have chosen to list the ten or so counties that South Carolinians recognize
as upstate, so as to be more clearly legible to a wider geographic audience. But there is
meaning conveyed by the term that would not be conveyed by other, more general
ways of indicating the same region. Only somebody already located in South Carolina
would have referred to that region in that way. From that usage of the term, we can
reason that the data were originally collected in South Carolina. This information is
not included elsewhere in the library record.
   It is because of records like this one that the process of cleaning and tidying data can
be so complicated and, at times, can be a destructive rather than constructive act. One
way to think of it is like chopping off the roots of a tree that connects it to the ground
from which it grew. It’s an act that irreversibly separates the data from their context.
   We might relate the growth of tools like tidyr that help to trim and tidy data to
another human intervention into the environment: the proliferation of street names
and signs that transformed the landscape of the nineteenth-­century United States.
Geographer Reuben Rose-­Redwood describes how, for example, prior to the Revolu-
tionary War, very few streets in Manhattan had signs posted at intersections.23 Street
names, such as they existed, were vernacular and related to the particularity of a
Unicorns, Janitors, Ninjas, Wizards, and Rock Stars                                      133



spot—­for example, “Take a right at the red house.” But with the increased mobility of
people and things—­think of the postal system, the railroads, or the telegraph—­street
names needed to become systematized. Rose-­Redwood calls this the production of “leg-
ible urban spaces.” Then, as now, there is high economic value to legible urban spaces,
particularly for large corporations (we’re looking at you, Amazon) to deliver boxes of
anything and everything directly to people’s front doors.24
   The point here is that one does not need street names for navigation until one has strang-
ers in the landscape. Likewise, data do not need cleaning until there are strangers in the
dataset. The Clemson University Library dataset was perfectly clear to Clemson’s own
librarians. But once hackers in Cambridge get their hands on it, upstate started to make
a lot less sense, and it was not at all helpful in producing, for instance, a map of all
the library records in the United States. Put more generally, once the data scientists
involved in a project are not from within the community, once the place of analysis
changes, once the scale of the project shifts, or once a single dataset needs to be com-
bined with others—­then we have strangers in the dataset.
   Who are these strangers? As we’ve already started to suggest, people who work with
data are alternately called unicorns (because they are rare and have special skills), wiz-
ards (because they can do magic), ninjas (because they execute complicated, expert
moves), rock stars (because they outperform others), and janitors (because they clean
messy data) (figure 5.3). Amazon dropped the “janitor” part in a recent job ad, but it
managed to work in a few of these metaphors: “Amazon needs a rockstar engineer ...
You are passionate ... You succeed fearlessly ... You are a coding ninja.”25
   These rock stars and ninjas are strangers in the dataset because, like the hackers in
Cambridge, they often sit at one, two, or many levels removed from the collection and
maintenance process of the data that they work with. This is a negative externality—­an
inadvertent third-­party consequence—­that arises when working with open data, appli-
cation programming interfaces (APIs), and the vast stores of training data available
online. These data appear available and ready to mobilize, but what they represent is
not always well-­documented or easily understood by outsiders. Being a stranger in the
dataset is not an inherently bad thing, but it carries significant risk of what renowned
postcolonial scholar Gayatri Spivak calls epistemic violence—­the harm that dominant
groups like colonial powers wreak by privileging their ways of knowing over local and
Indigenous ways.26
   This problem is compounded by the belief that data work is a solitary undertaking.
This is reflected in the fact that unicorns are unique by definition, and wizards, ninjas,
and rock stars are also all people who seem to work alone. This is a fallacy, of course;
every rock star requires a backing band, and if we’ve learned anything from Harry
134                                                                                        Chapter 5




Figure 5.3
Searching Media Cloud between 2012 and 2018 shows that unicorn is the most commonly refer-
enced metaphor in relation to data scientists, with wizard a close second. There are fewer than fifty
articles about data ninjas, rock stars, and janitors, but they appear in high-­profile venues like the
Washington Post and Forbes. The Media Cloud platform at www.mediacloud.org was developed at
the MIT Center for Civic Media and archives just under a million articles and blog posts every day.
Graphic by Catherine D’Ignazio. Data from www.mediacloud.org.


Potter, it’s that any particular tap of the wand is the culmination of years of education,
training, and support. Wizards, ninjas, rock stars, and janitors each have something
else in common: they are assumed to be men.27 If you doubt this assertion, try doing a
Google image search and count how many male-presenting wizards and janitors you
see before you get to a single female-presenting one. Or consider why news articles
about “data janitors” don’t describe them as doing “cleaning lady” work? Like janito-
rial work, which is disproportionately undertaken by working-­class people of color, the
idea of the data ninja also carries racist connotations.28 And there’s even more: shared
among the first four terms—­unicorns, wizards, ninjas, rock stars—­is a focus on the
individual’s extraordinary technical expertise and their ability to prevail when others
cannot. We might have more accurately said “his ability to prevail” because these ideas
about individual mastery and prevailing against steep odds are, of course, also associ-
ated with men.
   There is a “genius” in the world of eviction data—­it is Matthew Desmond, des-
ignated as such by the MacArthur Foundation for his work on poverty and eviction
Unicorns, Janitors, Ninjas, Wizards, and Rock Stars                                  135



in the United States. He is a professor and director of the Eviction Lab at Princeton
University, which has been working for several years to compile a national database
of evictions and make it available to the general public. Although the federal gov-
ernment collects national data on foreclosures, there is no national database of evic-
tions, something that is desperately needed for the many communities where housing
is in crisis.
   Initially, the Eviction Lab had approached community organizations like the AEMP
to request their data. The AEMP wanted to know more—­about privacy protections and
how the Eviction Lab would keep the data from falling into landlord hands. Instead of
continuing the conversation, the Eviction Lab turned to a real estate data broker and
purchased data of lower quality. But in an article written by people affiliated with the
AEMP and other housing justice organizations, the authors state, “AEMP and Tenants
Together have found three-­times the amount of evictions in California as Desmond’s
Eviction Lab show.”29
   As Desmond tells it, this decision was due to a change in the lab’s data collection
strategy. “We’re a research lab, so one thing that’s important to us is the data cleaning
process. If you want to know does Chicago evict more people than Boston, you’ve got
to compare apples to apples.”30 In Desmond’s view, it is more methodologically sound
to compare datasets that have been already aggregated and standardized. And yet Des-
mond acknowledges that Eviction Lab’s data for the state of California is undercount-
ing evictions; there is even a message on the site that makes that explicit. So here’s an
ethical quandary: Does one choose cleaner data at a larger scale that is relatively easy
and quick to purchase? Or more accurate data at a local scale for which one has to
engage and build trust with community groups?
   In this case, the priority was placed on speed at the expense of establishing trusted
relationships with actors on the ground, and on broad national coverage at the expense
of local accuracy. Though the Eviction Lab is doing important work, continued deci-
sions that prioritize speed and comprehensiveness can’t help but maintain the cultural
status of the solitary “genius,” effectively downplaying the work of coalitions, com-
munities, and movements that are—­not coincidentally—­often led primarily by women
and people of color.
   What might be gained if we not only recognized but also valued the fact that
data work involves multiple voices and multiple types of expertise? What if produc-
ing new social relationships—­increasing community solidarity and enhancing social
cohesion—­was valued (and funded) as much as acquiring data? We think this would
lead to a multiplication of projects like the AEMP: projects that do demonstrable good
with data and do so together with the communities they seek to support.
136                                                                            Chapter 5



On Power, Pluralism, and Process


Although the Anti-­Eviction Mapping Project could have handed off its valuable data
                                unicorn-­
to a single mapmaking rock star-­             wizard-­
                                        ninja-­      janitor, the group made an
intentional decision to include many designers in the process, including many nonex-
perts who experienced the power of making maps for the first time. In addition to the
diverse array of data products that resulted, which reflected the diverse voices of the
AEMP’s various collaborators, this decision also had the (wholly intentional) result of
building technical capacity. Slowly and surely, map by map, collaboration by collabo-
ration, local residents strengthened their mapping skills and their relationships with
each other. This latter result too was intentional; one of the stated goals of the AEMP
is to “build solidarity and collectivity among the project’s participants who could help
one another in fighting evictions and collectively combat the alienation that eviction
produces.”31
   This goal reflects a key tenet of feminist thinking, which is the recognition that a
multiplicity of voices, rather than one single loud or technical or magical one, results
in a more complete picture of the issue at hand. Feminist philosophers like Donna
Haraway, who we introduced in chapter 3, prompted a wave of thinkers who have
continued to develop the idea that all knowledge is partial, meaning no single person
or group can claim an objective view of the capital-T Truth.32 But embracing pluralism,
as this concept is often described today, does not mean that everything is relative,
nor does it mean that all truth claims have equal weight. And it most certainly does
not mean that feminists do not believe in science. It simply means that when people
make knowledge, they do so from a particular standpoint: from a situated, embodied
location in the world. More than that, by pooling our standpoints—­or positionalities—­
together, we can arrive at a richer and more robust understanding of the world.33
   So, how do we begin down the path to this deeper understanding in data science?
The first step in activating the value of multiple perspectives is to acknowledge the
partiality of your own. This means disclosing your project’s methods, your decisions,
and—­importantly for work that strives to address injustice—­your own positionalities.
This is called reflexivity, and we modeled this in the introduction to this book. You may
have heard the phrase coined by David Weinberger, “transparency is the new objec-
tivity.”34 We take this to mean that there is a way to build space for transparency plus
reflexivity in data science, rather than undertaking projects that purport to be objec-
tive (but, as we’ve discussed, never really are). Transparency and reflexivity allow the
people involved in any particular project to be explicit about the methods behind their
project, as well as their own identities.
Unicorns, Janitors, Ninjas, Wizards, and Rock Stars                                      137



   People in journalism have been doing this for some time—­at least as it relates to
their data and methods. For example, Bloomberg’s interactive visualization “What’s
Really Warming the World?” (figure 5.4) walks the reader through a range of common
arguments that try to explain away global warming with reasons that don’t have to do
with human industry or behavior.35 It’s a compelling piece in terms of content alone,
but another interesting thing about it is that it devotes nearly a third of its screen real
estate to describing its data and methods.
   Providing access to data and describing the methods employed are becoming con-
ventions in data journalism, just as they are in scientific fields. Although these accounts
are presently focused on technical details—­where the data were from, what statistical
models were developed, and what analysis was performed—­there is a seed of possibil-
ity for using this same space to reveal additional details: Who was on the team? What
were points of tension and disagreement? When did the team need to talk to data
stewards or domain experts or local communities? Which hypotheses were pursued
but ultimately proved false? There is a story about how every evidence-­based argument
comes into being, and it is often a story that involves money and institutions, as well
as humans and tools. Revealing this story through transparency and reflexivity can be
a feminist act.
   Reflexivity can sometimes be as simple as being explicit about or even visualizing
who is doing the counting and mapping behind the scenes.36 Take the example of the
aerial-­mapping image in figure 5.5.37 The Public Laboratory for Open Technology and
Science (Public Lab) is a citizen science group that got its start during the BP oil spill in
2010.38 It makes high-­resolution aerial maps by flying balloons and kites, which dangle
cheap digital cameras over the environmental sites they seek to study. The technique
is low cost, but the imagery produced is often higher resolution than existing satellite
imagery because of the proximity to the ground. As in the image, the mappers them-
selves are often visible in the final product in the form of little bodies, gathered in
boats or standing in clumps on a shoreline, looking up at the camera above them. The
balloon string leads the eye back to their forms. Here the creators are not distanced or
absent but represented in the final product. Literally.


Data for “Good” versus Data for Co-­liberation


Embracing the value of multiple perspectives shouldn’t stop with transparency and
reflexivity. It also means actively and deliberately inviting other perspectives into the
data analysis and storytelling process—more specifically, those of the people most mar-
ginalized in any given context. Intersectional feminist scholars have long insisted that
Figure 5.4
“What’s Really Warming the World?,” published in 2015, devotes a third of its real estate to
describing the methods for how the authors worked with the data. Graphic by Catherine D’Ignazio,
based on reporting by Eric Roston and Blacki Migliozzi for Bloomberg Businessweek.
Unicorns, Janitors, Ninjas, Wizards, and Rock Stars                                        139




Figure 5.5
From a Public Lab research note by Eymund Diegel about mapping sewage flows in the Gowanus
Canal in 2012 after Hurricane Sandy. Note the people on boats doing the mapping and the bal-
loon tether that links the camera and image back to their bodies. Courtesy of Eymund Diegel for
Public Lab.


we should be creating new knowledge and new designs from the margins. “Marginal-
ized subjects have an epistemic advantage, a particular perspective that scholars should
consider, if not adopt, when crafting a normative vision of a just society,” as Black
feminist scholar Jennifer C. Nash explains.39
   What does this mean? From a gender perspective, it means beginning with the per-
spectives of women and nonbinary people. On a project that involves international
development data, it means beginning not with institutional goals but with Indig-
enous standpoints. For the AEMP, it means centering the voices and experiences of
those who have been evicted. Follow-­up work about designing from the margins argues
that designers and engineers shouldn’t only be engaging people at the margins but also
actively working to dismantle the center/margins distinction in the first place.40 More
140                                                                                  Chapter 5



recently, the Design Justice Network has transformed this key tenet of intersectional
thinking into one of its design principles, stating: “We center the voices of those who
are directly impacted by the outcomes of the design process.”41
   How might this work? To begin, it requires a design process in which many actors
can participate—­people with technical expertise, as well as those with lived exper-
tise, domain expertise, organizing expertise, and community history expertise. It also
means shifting the overarching goal of such projects from “doing good with data” to
designing for co-­liberation; remember from chapter 2 that this is an end state in which
people from dominant groups and minoritized groups work together to free themselves
from oppressive systems. The key differences between data for good and data for co-­
liberation are highlighted in table 5.1.
   Data for good is a frame that is increasingly employed to describe data science proj-
ects that are socially engaged and/or undertaken in the public interest. The Bloomberg
corporation has been sponsoring Data for Good conferences since 2014. Nonprofit
consulting groups like Delta Analytics have sprouted up to match volunteers with
technical expertise with mission-­driven organizations. In 2019, one such organiza-
tion, DataKind, received a $20 million gift from a funding collaborative called Data
Science for Social Impact, with monies contributed by the Rockefeller Foundation


Table 5.1
Features of “data for good” versus data for co-­liberation

                                                       “Data for good”   Data for co-­liberation

Leadership by members of minoritized groups                              √
working in community
Money and resources managed by members of                                √
minoritized groups
Data owned and governed by the community                                 √
Quantitative data analysis “ground truthed”                              √
through a participatory, community-­centered
data analysis process
Data scientists are not rock stars and wizards,                          √
but rather facilitators and guides
Data education and knowledge transfer are part                           √
of the project design
Building social infrastructure—­community                                √
solidarity and shared understanding—­is part of
the project design
Unicorns, Janitors, Ninjas, Wizards, and Rock Stars                                       141



and Mastercard.42 There are also educational experiments underway, like the Utrecht
Data School and the University of Chicago’s Data Science for Social Good summer
fellowship program. In the latter, aspiring data scientists work with governments and
nonprofit organizations to address problems in diverse domains such as education,
health, public safety, and economic development.43 Related efforts in artificial intel-
ligence have sprung up, like the AI4Good Foundation, Project Impact sponsored by
                             centered AI Summer Lab at McGill University—­
Intel Corporation, the women-­                                           the
list goes on.
   These efforts have had demonstrable social impact. And yet, there remains a nag-
ging fuzziness with respect to what it means to “do good.” Whose good are we talking
about? What are the terms? Who maintains the databases when the unicorn-­wizards
leave the community? And who pays for the cloud storage when the development por-
tion of the project is complete?
   These issues are beginning to be discussed within the data for good community. Sara
Hooker, a deep-­learning researcher at Google Brain and the founder of Delta Analytics,
has observed that the idea of “data for good” lacks precision.44 To contribute clarity
to the phrase, Hooker proposes a rough taxonomy of this type of work, identifying
four distinct flavors: (1) volunteering skilled labor, (2) donating tech, (3) working with
nonprofits or governments as partners, and/or (4) running data-­education programs in
underserved communities.45 In each of these areas, there remain some thorny issues:
the fickleness of volunteer labor, the fact that even committed volunteers often lack
local knowledge, the community’s capacity (or lack thereof) to perform and/or pay for
its own technical maintenance, the fact that work initiated by nonprofits and govern-
ments cannot always be assumed to be “good,” and so on. Hooker’s point is that when
the goal is a vague notion of “good,” there is no way to address such concerns.
   By contrast, a model that positions co-­liberation as the end goal leads to a very specific
set of processes and practices, as well as criteria for success. Co-­liberation is grounded in
the belief that enduring and asymmetrical power relations among social groups serve as
the root cause of many societal problems. Rather than framing acts of technical service
as benevolence or charity, the goal of co-­liberation requires that those technical work-
ers acknowledge that they are engaged in a struggle for their own liberation as well,
even and especially when they are members of dominant groups.
   For these reasons, data projects designed with co-­liberation in mind must be very
specific about the power dynamics involved. They must take preemptive steps to coun-
teract the privilege hazard that comes with work undertaken by members of dominant
groups. For example, one such step is to deliberately concentrate strategic leadership,
142                                                                             Chapter 5



financial resources, and data ownership in the hands of collaborators from minoritized
groups.46 It also recognizes that differential power has a silencing effect and that for a
variety of reasons—­as discussed in chapter 4—­quantitative data can leave people out.
To address these almost certain gaps, a model of data for co-­liberation would delib-
erately pair quantitative analyses with inclusive civic processes, resulting in locally
informed, ground-­truthed insights that derive from many perspectives.
   The scope of data for co-­liberation is also broader. It explicitly includes two addi-
tional outcomes that data for good typically does not: (1) knowledge transfer and (2)
building social infrastructure. The first involves a two-­way exchange: in one direction,
technical capacity building within the community so that any data products can be
maintained and/or enhanced without requiring external expertise, and in the other,
enhanced understanding of and respect for local knowledge for external collabora-
tors, as well as chipping away at their own individual and institutional privilege haz-
ards. Building social infrastructure involves an explicit focus on cultivating community
solidarity through the project. This entails allocating financial and human resources
to the community aspects of the project and not only to technical aspects like proc-
essing data or building apps. In the co-­liberation model, data science projects become
community science projects. They take place simultaneously in the database and in
public space.


Data for Co-­liberation in Action


What does data for co-­liberation look like in action? Are there examples of feminist
data science that value quantitative methods and pluralistic processes, data education
and community solidarity?
   Since 2012, Rahul and Emily Bhargava have partnered with community organi-
zations from Belo Horizonte to Boston to create data murals in public spaces (figure
5.6). These are large-­scale infographics that are both designed by and tell stories about
the people who live and work in those spaces. In all cases, the people themselves
sought out the collaboration, having recognized a need within their own community
space. For example, in 2013, Groundwork Somerville, an urban agriculture nonprofit,
approached the Bhargavas because it was in the process of establishing its first urban
farm. As Emily recalls, “The site was disorderly—­it was behind a used car parts build-
ing and hidden between other semi-­industrial lots. They had built raised beds and
planted for one growing season but passersby were stealing the vegetables.”47 The
organization was also running a high school employment program called the Green
Unicorns, Janitors, Ninjas, Wizards, and Rock Stars                                         143




Figure 5.6
The process of making a data mural involves conversation, building prototypes with craft materi-
als, workshops in data analysis, and actual painting. Courtesy of Data Therapy, Emily and Rahul
Bhargava, 2018.


Team but struggling to fully involve the young people in its mission to create healthier
communities.
   Linking those two challenges together, Groundwork Somerville and the Bhargavas
decided to enlist the young people in creating a mural that illustrated the purpose of
the farm to the community (figure 5.7). First, they brought together data from several
sources: demographic data from the city, geographic information system (GIS) data on
unused lots, and internal data from Groundwork Somerville that included growing
records, food donations, and attendance logs at community events. Then they worked
with the learners over several after-­school sessions to review and discuss the data and
engage in storyfinding (aka data analysis). By the end of these sessions, the youth had
sketched the overall outline and iconography of the resulting mural. Read left to right,
the mural frames the problem: A man grasps for a basket of veggies, but it says “healthy
food is hard to get.” The claim is backed up by data that show the cost of healthy food
and the number of people with prediabetes. Additional data document the number of
unused lots in the city and the amount of land that has been reclaimed for urban farm-
ing, pointing to future opportunity. In the next section, the mural depicts a Ground-
work Somerville truck bringing affordable produce to the neighborhood and includes
the fact that it employs over four hundred youth residents. The data mural ends with a
vision for a unified and healthy community, “Together Livin’ Better.”
   On July 30, 2013, the mayor of Somerville and other community leaders attended
the ribbon-­cutting to officially launch the renovated garden. Emily describes the visit:
144                                                                              Chapter 5




Figure 5.7
The Groundwork Somerville data mural, painted by youth, staff and volunteers at Groundwork
Somerville, and the Bhargavas in 2013. Courtesy of Data Therapy, Emily and Rahul Bhargava.


“The youth, having just spent weeks looking at the data, painting the mural together,
and building relationships with staff and volunteers, were able to talk about the story
in great detail to their elected officials.”48
   The partnership represents some of the best aspects of data projects designed for
co-­liberation. But murals are just one possible output from this type of pluralistic,
community-­centered process.49 For example, Digital Democracy works with Indige-
nous groups around the world to defend their rights through collecting data and mak-
ing maps.50 In the process, they have developed SMS services with domestic violence
groups in Haiti and supported the Wapichana people in Guyana to make a data-­driven
case for land rights to the government. In another example, the Westside Atlanta Land
Trust (WALT) has worked toward affordable housing in Atlanta, Georgia, through par-
ticipatory data collection.51 Disappointed in what it termed “triple-­incorrect” county-­
level data, the organization set out to collect its own spatial dataset about property
abandonment and disinvestment and has used it to push municipal policymakers
for change.52
   Each of these projects—­the data mural, the land rights claim, and the abandoned
property map—­could be said to exemplify the data for co-­liberation model. Each origi-
nates from a need articulated by the community, rather than projected onto it by more
powerful institutions. Each relies upon methods of data collection and processing. And
each also incorporates an explicit process of knowledge transfer from external collabo-
rators (consultants, academics, nonprofit specialists) to the community itself. More
importantly, none of those external collaborators envision themselves as unicorns, jan-
itors, ninjas, wizards, or rock stars. “At Digital Democracy, we try to fight the superhero
narrative,” says director Emily Jacobi. “We are sidekicks rather than superheroes.”53
Unicorns, Janitors, Ninjas, Wizards, and Rock Stars                                  145



   Along these lines, each of these projects is explicitly designed to build community
solidarity and shared understanding around a civic issue. As Amanda Meng and Carl
DiSalvo explain with respect to the WALT project, “Data collection became a way of
generating not simply awareness of the conditions, but a sensitivity to the conditions
and to [community members’] own experience of and situation of being blinded to
these conditions.”54 Data collection and participatory analysis become a gathering
technology—­a kind of campfire—­where information is exchanged, social cohesion is
enhanced, and future actions are co-­conspired.55 The campfire model also has the effect
of challenging the deficit narratives that so often surround underserved communities.56
                                                  Cortés has asserted, cultivating
Instead, as social movement scholar Maribel Casas-­
community solidarity contributes to a sense of collective agency and transformative
possibility, what she describes as “an innovative sense of political participation and
re-­invigorating political imaginaries.”57


Does the Campfire Scale?


Data for co-­liberation is simultaneously more specific than data for good and larger in
scope. By advocating for community-­based leadership and resources, qualitative inves-
tigations to complement quantitative analyses, and intentional project design in areas
that exceed the purview of data for good projects, such as knowledge transfer and
building community solidarity, data for co-­liberation enables the participation of many
actors and agents. But can this pluralistic model scale up? What would “big data for
co-­liberation” look like? Would it be possible at all?
   Questions about big data versus little data or quantitative data versus qualitative
data are far too often framed as false binaries. And as with all false binaries, there
are paths through those distinctions that combine and multiply options, rather than
reduce or divide the world into fake choices. The key question to keep in mind is how
we can scale up data for co-­liberation in ways that remain careful, community-­based,
and complex.58
   One real-­world example of this intentional practice is the Global Atlas of Environ-
mental Justice (EJ Atlas), a large-­scale research project and open archive (figure 5.8)
directed by scholars Leah Temper and Joan Martinez-­Alier.59 Initiated in 2012 by a
team of researchers at the Universitat Autónoma de Barcelona, in Spain, the EJ Atlas
represents a systematic collection of global ecological conflicts. It was created in part
as a response to assertions that environmental justice scholars were focusing too much
attention on local case studies at the expense of making global connections. For the
146                                                                                      Chapter 5




Figure 5.8
The Global Atlas of Environmental Justice (EJ Atlas; https://ejatlas.org/) works in partnership with
activists, civil society organizations, and social movements to systematically document ecological
conflicts around the globe. The scope and scale of the EJ Atlas enables activists to connect with
others and facilitates researchers studying conflicts in a quantitative and comparative context,
without sacrificing a commitment to a pluralistic process and the dignity of local and community
knowledges. Courtesy of the Global Atlas of Environmental Justice, 2019.


most part, the global ecological conflicts that the EJ Atlas collects are cases straight out
of the matrix of domination, of wealthy people overutilizing natural resources and
displacing environmental risk and degradation to poorer people, who are often also
minoritized because of their gender, indigeneity, race, and/or geography. For example,
one of the cases in the EJ Atlas discusses the efforts of Berta Cáceres and the Lenca
Indigenous people to oppose the construction of a dam and hydropower plant in Hon-
duras.60 Cáceres won international awards for her community organizing, only to be
tragically assassinated by employees of the company building the dam. (The assassins
were later convicted in a Honduran court of law.61) In the EJ Atlas, the entry on Cáce-
res’s organizing efforts includes copious links, photos, and geographic data about the
Unicorns, Janitors, Ninjas, Wizards, and Rock Stars                                      147



dam, as well as information about where the incident falls in the atlas’s typology of
conflict.
   The EJ Atlas demonstrates that scale is not incompatible with valuing local knowl-
edges and relationships with local actors. Temper and her colleagues were able to draw
on past relationships with partner organizations to assemble their archive, demonstrat-
ing how time invested in building relationships at the outset of a project, or of a career,
continues to accrue benefits for all parties involved.62 At the time of writing, the EJ Atlas
includes close to three thousand cases of ecological conflicts from around the world.
Collaborators have created submaps from the atlas for countries including Colombia,
Italy, and Turkey. Activists have used the atlas to draw media and policymaker atten-
tion to overlooked environmental conflicts, such as the construction of a ski resort in
the middle of a nature park in Kazakhstan, which has disturbed hydrological systems.63
The atlas has also enabled comparative empirical research, like that undertaken by
economist Begüm Özkaynak and several colleagues.64 Their work employs social net-
work analysis to study the relationships among mining corporations, their financiers,
and environmental groups challenging that practice to understand the geography of
the actors and their connections to each other.65
   Scale is emphatically not incompatible with the feminist imperative to value mul-
tiple and local knowledges. Research like Özkaynak’s and that of her colleagues would
not be possible without a large-­scale archive like the EJ Atlas. A pluralistic, participa-
tory, iterative process like the EJ Atlas will take longer to scale than the extractive,
quantity-­at-­all-­costs approach of conventional big data. But ultimately the data—­and
the relationships, and the community capacity—­will be of higher quality.


Embrace Pluralism


The fifth principle of data feminism is to embrace pluralism in the whole process of
working with data, from collection to analysis to communication to decision-­making.
As we describe in this chapter, data work carries a high risk of enacting what Gayatri
Spivak has termed epistemic violence, particularly when the people doing the work are
strangers in the dataset, when they are one or more steps removed from the local con-
text of the data, and when they view themselves (or are viewed by society) as unicorns,
rock stars, and wizards.
   Embracing pluralism is a feminist strategy for mitigating this risk. It allows both
time and space for a range of participants to contribute their knowledge to a data
project and to do so at all stages of that project. In contrast to an underspecified data
for good model, embracing pluralism offers a way to work toward a model of data for
148                                                                             Chapter 5



co-­liberation. This means transferring knowledge from experts to communities and
explicitly cultivating community solidarity in data work, as we see in the case of the
Anti-Eviction Mapping Project. Moreover, embracing pluralism is not incompatible
with “bigness” or scale; the EJ Atlas shows how pluralistic processes can be used to
assemble a global archive and support empirical work in the service of justice. A single
data scientist wizard will never defeat the matrix of domination alone, no matter how
powerful their spells might be. But a well-­designed, data-­driven, participatory process,
one that centers the standpoints of those most marginalized, empowers project partici-
pants, and builds new relationships across lines of social difference—­well, that might
just have a chance.
6    The Numbers Don’t Speak for Themselves


Principle: Consider Context

Data feminism asserts that data are not neutral or objective. They are the products of unequal
social relations, and this context is essential for conducting accurate, ethical analysis.




In April 2014, 276 young women were kidnapped from their high school in the town
of Chibok in northern Nigeria. Boko Haram, a militant terrorist group, claimed respon-
sibility for the attacks. The press coverage, both in Nigeria and around the world, was
fast and furious. SaharaReporters.com challenged the government’s ability to keep its
students safe. CNN covered parents’ anguish. The Japan Times connected the kidnap-
pings to the increasing unrest in Nigeria’s northern states. And the BBC told the story
of a girl who had managed to evade the kidnappers. Several weeks after this initial
reporting, the popular blog FiveThirtyEight published its own data-­driven story about
the event, titled “Kidnapping of Girls in Nigeria Is Part of a Worsening Problem.”1
The story reported skyrocketing rates of kidnappings. It asserted that in 2013 alone
there had been more than 3,608 kidnappings of young women. Charts and maps
accompanied the story to visually make the case that abduction was at an all-­time high
(figure 6.1).
    Shortly thereafter, the news website had to issue an apologetic retraction because
its numbers were just plain wrong. The outlet had used the Global Database of Events,
Language and Tone (GDELT) as its data source. GDELT is a big data project led by com-
putational social scientist Kalev Leetaru. It collects news reports about events around
the world and parses the news reports for actors, events, and geography with the aim of
providing a comprehensive set of data for researchers, governments, and civil society.
GDELT tries to focus on conflict—­for example, whether conflict is likely between two
countries or whether unrest is sparking a civil war—­by analyzing media reports. How-
ever, as political scientist Erin Simpson pointed out to FiveThirtyEight in a widely cited
Twitter thread, GDELT’s primary data source is media reports (figure 6.2).2 The project is
not at a stage at which its data can be used to make reliable claims about independent
150                                                                                      Chapter 6




Figure 6.1
In 2014, FiveThirtyEight erroneously charted counts of “daily kidnappings” in Nigeria. The news
site failed to recognize that the data source it was using was not counting events, but rather media
reports about events. Or some events and some media reports. Or it was counting something, but
we are still not sure what. Image by FiveThirtyEight.


cases of kidnapping. The kidnapping of schoolgirls in Nigeria was a single event. There
were thousands of global media stories about it. Although GDELT de-­duplicated some
of those stories to a single event, it still logged, erroneously, that hundreds of kidnap-
ping events had happened that day. The FiveThirtyEight report had counted each of
those GDELT pseudoevents as a separate kidnapping incident.
   The error was embarrassing for FiveThirtyEight, not to mention for the reporter, but it
also helps to illustrate some of the larger problems related to data found “in the wild.”
First, the hype around “big data” leads to projects like GDELT wildly overstating the
completeness and accuracy of its data and algorithms. On the website and in publica-
tions, the project leads have stated that GDELT is “an initiative to construct a catalog
The Numbers Don’t Speak for Themselves                                                       151




Figure 6.2
Two tweets by Erin Simpson in response to FiveThirtyEight’s erroneous interpretation of the GDELT
dataset. Tweets by Erin Simpson on May 13, 2014.


of human societal-­scale behavior and beliefs across all countries of the world, connect-
ing every person, organization, location, count, theme, news source, and event across
the planet into a single massive network that captures what’s happening around the
world, what its context is and who’s involved, and how the world is feeling about it,
every single day.”3 That giant mouthful describes no small or impotent big data tool. It
is clearly Big Dick Data.
   Big Dick Data is a formal, academic term that we, the authors, have coined to denote
big data projects that are characterized by masculinist, totalizing fantasies of world
domination as enacted through data capture and analysis. Big Dick Data projects
ignore context, fetishize size, and inflate their technical and scientific capabilities.4
In GDELT’s case, the question is whether we should take its claims of big data at face
value or whether the Big Dick Data is trying to trick funding organizations into giving
the project massive amounts of research funding. (We have seen this trick work many
times before.)
152                                                                              Chapter 6



   The GDELT technical documentation does not provide any more clarity as to
whether it is counting media reports (as Simpson asserts) or single events. The data-
base FiveThirtyEight used is called the GDELT Event Database, which certainly makes
it sound like it’s counting events. The GDELT documentation states that “if an event
has been seen before it will not be included again,” which also makes it sound like it’s
counting events. And a 2013 research paper related to the project confirms that GDELT
is indeed counting events, but only events that are unique to specific publications. So
it’s counting events, but with an asterisk. Compounding the matter, the documenta-
tion offers no guidance as to what kinds of research questions are appropriate to ask the
database or what the limitations might be. People like Simpson who are familiar with
the area of research known as event detection, or members of the GDELT community,
may know to not believe (1) the title of the database, (2) the documentation, and (3)
the marketing hype. But how would outsiders, let alone newcomers to the platform,
ever know that?
   We’ve singled out GDELT, but the truth is that it’s not very different from any num-
ber of other data repositories out there on the web. There are a proliferating number
of portals, observatories, and websites that make it possible to download all manner of
government, corporate, and scientific data. There are APIs that make it possible to write
little programs to query massive datasets (like, for instance, all of Twitter) and down-
load them in a structured way.5 There are test datasets for network analysis, machine
learning, social media, and image recognition. There are fun datasets, curious datasets,
and newsletters that inform readers of datasets to explore for journalism or analysis.6
In our current moment, we tend to think of this unfettered access to information as an
inherent good. And in many ways, it is kind of amazing that one can just google and
download data on, for instance, pigeon racing, the length of guinea pig teeth, or every
single person accused of witchcraft in Scotland between 1562 and 1736—­not to men-
tion truckloads and truckloads of tweets.7
   And though the schooling on data verification received by FiveThirtyEight was rightly
deserved, there is a much larger issue that remains unaddressed: the issue of context.
As we’ve discussed throughout this book, one of the central tenets of feminist thinking
is that all knowledge is situated. A less academic way to put this is that context matters.
When approaching any new source of knowledge, whether it be a dataset or dinner
menu (or a dataset of dinner menus), it’s essential to ask questions about the social,
cultural, historical, institutional, and material conditions under which that knowledge
was produced, as well as about the identities of the people who created it.8 Rather than
seeing knowledge artifacts, like datasets, as raw input that can be simply fed into a
statistical analysis or data visualization, a feminist approach insists on connecting data
The Numbers Don’t Speak for Themselves                                                153



back to the context in which they were produced. This context allows us, as data sci-
entists, to better understand any functional limitations of the data and any associated
ethical obligations, as well as how the power and privilege that contributed to their
making may be obscuring the truth.


Situating Data on the Wild Wild Web


The major issue with much of the data that can be downloaded from web portals or
through APIs is that they come without context or metadata. If you are lucky you
might get a paragraph about where the data are from or a data dictionary that describes
what each column in a particular spreadsheet means. But more often than not, you get
something that looks like figure 6.3.
   The data shown in the figure—­open budget data about government procurement in
São Paulo, Brazil—­do not look very technically complicated. The complicated part is
figuring out how the business process behind them works. How does the government
run the bidding process? How does it decide who gets awarded a contract? Are all the
bids published here, or just the ones that were awarded contracts? What do terms like
competition, cooperation agreement, and terms of collaboration mean to the data publisher?
Why is there such variation in the publication numbering scheme? These are only a
few of the questions one might ask when first encountering this dataset. But with-
out answers to even some of these questions—­to say nothing of the local knowledge
required to understand how power is operating in this particular ecosystem—­it would
be difficult to even begin a data exploration or analysis project.
   This scenario is not uncommon. Most data arrive on our computational door-
step context-­free. And this lack of context becomes even more of a liability when
accompanied by the kind of marketing hype we see in GDELT and other Big Dick
Data projects. In fact, the 1980s version of these claims is what led Donna Haraway
to propose the concept of situated knowledge in the first place.9 Subsequent feminist
work has drawn on the concept of situated knowledge to elaborate ideas about ethics
and responsibility in relation to knowledge-­making.10 Along this line of thinking, it
becomes the responsibility of the person evaluating that knowledge, or building upon
it, to ensure that its “situatedness” is taken into account. For example, information
studies scholar Christine Borgman advocates for understanding data in relation to
the “knowledge infrastructure” from which they originate. As Borgman defines it, a
knowledge infrastructure is “an ecology of people, practices, technologies, institutions,
material objects, and relationships.”11 In short, it is the context that makes the data
possible.
Figure 6.3
Open budget data about procurement and expenses from the São Paulo prefecture in Brazil.
Although Brazil has some of the most progressive transparency laws on the books, the data that
are published aren’t necessarily always accessible or usable by citizens and residents. In 2013,
researcher Gisele Craveiro worked with civil society organizations to give this open budget data
more context. Images from SIGRC for the Prefecture of São Paulo, Brazil.
The Numbers Don’t Speak for Themselves                                               155



  Ironically, some of the most admirable aims and actions of the open data move-
ment have worked against the ethical urgency of providing context, however inad-
vertently. Open data describes the idea that anyone can freely access, use, modify, and
share data for any purpose. The open data movement is a loose network of organiza-
tions, governments, and individuals. It has been active in some form since the mid-­
2000s, when groups like the Open Knowledge Institute were founded and campaigns
like Free Our Data from the Guardian originated to petition governments for free access
to public records.12 The goals are good ones in theory: economic development by
building apps and services on open data; faster scientific progress when researchers
share knowledge; and greater transparency for journalists, citizens, and residents to be
able to use public information to hold governments accountable. This final goal was
a major part of the framing of former US president Obama’s well-­known memoran-
dum on transparency and open government.13 On his very first day in office, Obama
signed a memorandum that directed government agencies to make all data open by
default.14 Many more countries, states, and cities have followed suit by developing
open data portals and writing open data into policy. As of 2019, seventeen coun-
tries and over fifty cities and states have adopted the International Open Data Char-
ter, which outlines a set of six principles guiding the publication and accessibility of
government data.15
  In practice, however, limited public funding for technological infrastructure
has meant that governments have prioritized the “opening up” part of open data—­
publishing spreadsheets of things like license applications, arrest records, and flood
zones—­but lack the capacity to provide any context about the data’s provenance, let
alone documentation that would allow the data to be made accessible and usable by the
general public. As scholar Tim Davies notes, raw data dumps might be good for starting
a conversation, but they cannot ensure engagement or accountability.16 The reality is
that many published datasets sit idle on their portals, awaiting users to undertake the
intensive work of deciphering the bureaucratic arcana that obscures their significance.
This phenomenon has been called zombie data: datasets that have been published with-
out any purpose or clear use case in mind.17
  Zombies might be bad for brains, but is zombie data really a problem? Wired maga-
zine editor Chris Anderson would say, emphatically, “No.” In a 2008 Wired article, “The
End of Theory,” Anderson made the now-­infamous claim that “the numbers speak for
themselves.”18 His main assertion was that the advent of big data would soon allow
data scientists to conduct analyses at the scale of the entire human population, without
needing to restrict their analysis to a smaller sample. To understand his claim, you need
to understand one of the basic premises of statistics.
156                                                                               Chapter 6



   Statistical inference is based on the idea of sampling: that you can infer things
about a population (or other large-­scale phenomenon) by studying a random and/
or representative sample and then mapping those findings back on the population
(or phenomenon) as a whole. Say that you want to know who all of the 323 million
people in the US will vote for in the coming presidential election. You couldn’t con-
tact all of them, of course, but you could call three thousand of them on the phone
and then use those results to predict how the rest of the people would likely vote.
There would also need to be some statistical modeling and theory involved, because
how do you know that those three thousand people are an accurate representation of
the whole population? This is where Anderson made his intervention: at the point at
which we have data collected on the entire population, we no longer need modeling,
or any other “theory” to first test and then prove. We can look directly at the data
themselves.
   Now, you can’t write an article claiming that the basic structure of scientific inquiry
is obsolete and not expect some pushback. Anderson wrote the piece to be provoca-
tive, and sure enough, it prompted numerous responses and debates, including those
that challenge the idea that this argument is a “new” way of thinking in the first
place (e.g., in the early seventeenth century, Francis Bacon argued for a form of induc-
tive reasoning, in which the scientist gathers data, analyzes them, and only thereafter
forms a hypothesis).19 One of Anderson’s major examples is Google Search. Google’s
search algorithms don’t need to have a hypothesis about why some websites have more
incoming links—­other pages that link to the site—­than others; they just need a way to
determine the number of links so they can use that number to determine the popular-
ity and relevance of the site in search results. We no longer need causation, Anderson
insists: “Correlation is enough.”20 But what happens when the number of links is also
highly correlated with sexist, racist, and pornographic results?
   The influence of racism, sexism, and colonialism is precisely what we see described
in Algorithms of Oppression, information studies scholar Safiya Umoja Noble’s study of
the harmful stereotypes about Black and Latinx women perpetuated by search algo-
rithms such as Google’s. As discussed in chapter 1, Noble demonstrates that Google
Search results do not simply correlate with our racist, sexist, and colonialist society;
that society causes the racist and sexist results. More than that, Google Search re­inforces
these oppressive views by ranking results according to how many other sites link to
them. The rank order, in turn, encourages users to continue to click on those same
sites. Here, correlation without context is clearly not enough because it recirculates rac-
ism and sexism and perpetuates inequality.21
The Numbers Don’t Speak for Themselves                                                 157



   There’s another reason that context is necessary for making sense of correlation,
and it has to do with how racism, sexism, and other forces of oppression enter into
the environments in which data are collected. The next example has to do with sexual
assault and violence. If you do not want to read about these topics, you may want to
skip ahead to the next section.


In April 1986, Jeanne Clery, a student at Lehigh University, was sexually assaulted and
murdered in her dorm room. Her parents later found out that there had been thirty-­
eight violent crimes at Lehigh in the prior three years, but nobody had viewed that as
important data that should be made available to parents or to the public. The Clerys
mounted a campaign to improve data collection and communication efforts related
to crimes on college campuses, and it was successful: the Jeanne Clery Act was passed
in 1990, requiring all US colleges and universities to make on-­campus crime statistics
available to the public.22
   So we have an ostensibly comprehensive national dataset about an important public
topic. In 2016, three students in Catherine’s data journalism class at Emerson College—­
Patrick Torphy, Michaela Halnon, and Jillian Meehan—­downloaded the Clery Act data
and began to explore it, hoping to better understand the rape culture that has become
pervasive on college campuses across the United States.23 They soon became puzzled,
however. Williams College, a small, wealthy liberal arts college in rural Massachusetts,
seemed to have an epidemic of sexual assault, whereas Boston University (BU), a large
research institution in the center of the city, seemed to have strikingly few cases rela-
tive to its size and population (not to mention that several high-­profile sexual assault
cases at BU had made the news in recent years).24 The students were suspicious of these
numbers, and investigated further. After comparing the Clery Act data with anony-
mous campus climate surveys (figure 6.4), consulting with experts, and interviewing
survivors, they discovered, paradoxically, that the truth was closer to the reverse of the
picture that the Clery Act data suggest. Many of the colleges with higher reported rates
of sexual assault were actually places where more institutional resources were being
devoted to support for survivors.25
   As for the colleges with lower numbers, this is also explained by context. The Clery
Act requires colleges and universities to provide annual reports of sexual assault and
other campus crimes, and there are stiff financial penalties for not reporting. But the
numbers are self-­reported, and there are also strong financial incentives for colleges not
to report.26 No college wants to tell the government—­let alone parents of prospective
students—­that it has a high rate of sexual assault on campus. This is compounded by
158                                                                                   Chapter 6




Figure 6.4
Data journalism students at Emerson College were skeptical of the self-­reported Clery Act data
and decided to compare the Clery Act results with anonymous campus climate survey results
about nonconsensual sexual contact. Although there are data-­     quality issues with both data­
sets, the students assert that if institutions are providing adequate support for survivors, then
there will be less of a gap between the Clery-reported data and the proportion of students that
report nonconsensual sexual conduct. Courtesy of Patrick Torphy, Michaela Halnon, and Jillian
Meehan, 2016.
The Numbers Don’t Speak for Themselves                                                   159



the fact that survivors of sexual assault often do not want to come forward—­because of
social stigma, the trauma of reliving their experience, or the resulting lack of social and
psychological support. Mainstream culture has taught survivors that their experiences
will not be treated with care and that they may in fact face more harm, blame, and
trauma if they do come forward.27
   There are further power differentials reflected in the data when race and sexuality
are taken into account. For example, in 2014, twenty-­three students filed a complaint
against Columbia University, alleging that Columbia was systematically mishandling
cases of rape and sexual violence reported by LGBTQ students. Zoe Ridolfi-­Starr, the
lead student named in the complaint, told the Daily Beast, “We see complete lack of
knowledge about the specific dynamics of sexual violence in the queer community,
even from people who really should be trained in those issues.”28
   Simply stated, there are imbalances of power in the data setting—­to use the phrase
coined by Yanni Loukissas that we discussed in chapter 5—­so we cannot take the num-
bers in the dataset at face value. Lacking this understanding of power in the collection
environment and letting the numbers “speak for themselves” would tell a story that
is not only patently false but could also be used to reward colleges that are system-
atically underreporting and creating hostile environments for survivors. Deliberately
undercounting cases of sexual assault leads to being rewarded for underreporting. And
the silence around sexual assault continues: the administration is silent, the campus
culture is silent, the dataset is silent.29


Raw Data, Cooked Data, Cooking


As demonstrated by the Emerson College students, one of the key analytical missteps
of work that lets “the numbers speak for themselves” is the premise that data are a raw
input. But as Lisa Gitelman and Virginia Jackson have memorably explained, data enter
into research projects already fully cooked—­the result of a complex set of social, politi-
cal, and historical circumstances. “‘Raw data’ is an oxymoron,” they assert, just like
“jumbo shrimp.”30 But there is an emerging class of “data creatives” whose very exis-
tence is premised on their ability to context-­hop—­that is, their ability to creatively mine
and combine data to produce new insights, as well as work across diverse domains. This
group includes data scientists, data journalists, data artists and designers, researchers,
and entrepreneurs—­in short, pretty much everyone who works with data right now.
They are the strangers in the dataset that we spoke of in chapter 5.
   Data’s new creative class is highly rewarded for producing work that creates new
value and insight from mining and combining conceptually unrelated datasets.
160                                                                              Chapter 6



Examples include Google’s now defunct Flu Trends project, which tried to geographi-
cally link people’s web searches for flu symptoms to actual incidences of flu.31 Or a proj-
ect of the Sun Sentinel newspaper, in Fort Lauderdale, Florida, which combined police
license plate data with electronic toll records to prove that cops were systematically and
dangerously speeding on Florida highways.32 Sometimes these acts of creative synthe-
sis work out well; the Sun Sentinel won a Pulitzer for its reporting and a number of the
speeding cops were fired. But sometimes the results are not quite as straightforward.
Google Flu Trends worked well until it didn’t, and subsequent research has shown that
Google searches cannot be used as 1:1 signals for actual flu phenomena because they
are susceptible to external factors, such as what the media is reporting about the flu.33
   Instead of taking data at face value and looking toward future insights, data scien-
tists can first interrogate the context, limitations, and validity of the data under use.
In other words, one feminist strategy for considering context is to consider the cook-
ing process that produces “raw” data. As one example, computational social scientists
Derek Ruths and Jürgen Pfeffer write about the limitations of using social media data
for behavioral insights: Instagram data skews young because Instagram does; Reddit
data contains far more comments by men than by women because Reddit’s overall
membership is majority men. They further show how research data acquired from
those sources are shaped by sampling because companies like Reddit and Instagram
employ proprietary methods to deliver their data to researchers, and those methods
are never disclosed.34 Related research by Devin Gaffney and J. Nathan Matias took on
a popular corpus that claimed to contain “every publicly available Reddit comment.”35
Their work showed the that the supposedly complete corpus is missing at least thirty-­
six million comments and twenty-­eight million submissions.
   Exploring and analyzing what is missing from a dataset is a powerful way to gain
insight into the cooking process—­of both the data and of the phenomenon it purports
to represent. In some of Lauren’s historical work, she looks at actual cooks as they are
recorded (or not) in a corpus of thirty thousand letters written by Thomas Jefferson, as
shown in figure 6.5.36 Some may already know that Jefferson is considered the nation’s
“founding foodie.”37 But fewer know that he relied upon an enslaved kitchen staff to
prepare his famous food.38 In “The Image of Absence,” Lauren used named-entity rec-
ognition, a natural language processing technique, to identify the places in Jefferson’s
personal correspondence where he named these people and then used social network
analysis to approximate the extent of the relationships among them. The result is a
visual representation of all of the work that Jefferson’s enslaved staff put into prepar-
ing his meals but that he did not acknowledge—­at least not directly—­in the text of the
letters themselves.
The Numbers Don’t Speak for Themselves                                                        161




Figure 6.5
In “The Image of Absence” (2013), Lauren used machine learning techniques to identify the
names of the people whom Thomas Jefferson mentioned in his personal correspondence and
then visualized the relationships among them. The result demonstrates all of the work that his
enslaved staff put into preparing Jefferson’s meals but that was not directly acknowledged by Jef-
ferson himself. Visualization by Lauren F. Klein.


   On an even larger scale, computer scientists and historians at Stanford University
used word embeddings—­another machine learning technique—­to explore gender and
ethnic stereotypes across the span of the twentieth century.39 Using several large data-
sets derived from sources such as the Google Books and the New York Times, the team
showed how words like intelligent, logical, and thoughtful were strongly associated with
men until the 1960s. Since that time, however, those words have steadily increased
in association with women. The team attributed this phenomenon to the “women’s
movement in the 1960s and 1970s,” making their work an interesting example of an
attempt to quantify the impact of social movements. The paper is also notable for
162                                                                             Chapter 6



openly acknowledging how their methods, which involved looking at the adjectives
surrounding the words man and woman, limited the scope of their analysis to the gen-
der binary. Furthermore, the researchers did not try to assert that the data represent
how women and men “are,” nor did they try to “remove the bias” so that they could
develop “unbiased” applications in other domains. They saw the data as what they
are—­cultural indicators of the changing face of patriarchy and racism—­and interro-
gated them as such.
   So, how do we produce more work like this—­work that understands data as already
“cooked” and then uses that data to expose structural bias? Unfortunately for Chris
Anderson, the answer is that we need more theory, not less. Without theory, survey
designers and data analysts must rely on their intuition, supported by “common sense”
ideas about the things they are measuring and modeling. This reliance on “common
sense” leads directly down the path to bias. Take the case of GDELT. Decades of research
has demonstrated that events covered by the media are selected, framed, and shaped
by what are called “news values”: values that confirm existing images and ideologies.40
So what is it really that GDELT is measuring? What events are happening in the world,
or what the major international news organizations are focusing their attention on?
The latter might be the most powerful story embedded in the GDELT database. But it
requires deep context and framing to draw it out.
   Refusing to acknowledge context is a power play to avoid power. It’s a way to assert
authoritativeness and mastery without being required to address the complexity of
what the data actually represent: the political economy of the news in the case of
GDELT, entrenched gender hierarchies and flawed reporting environments in the case
of the Clery data, and so on. But deep context and computation are not incompatible.
For example, SAFElab, a research lab at Columbia run by scholar and social worker
Desmond Patton, uses artificial intelligence to examine the ways that youth of color
navigate violence on and offline. He and a team of social work students use Twitter data
to understand and prevent gang violence in Chicago. Their data are big, and they’re
also complicated in ways that are both technical and social. The team is acutely aware
of the history of law enforcement agencies using technology to surveil Black people,
for example, and acknowledges that law enforcement continues to do so using Twitter
itself. What’s more, when Patton started his research, he ran into an even more basic
problem: “I didn’t know what young people were saying, period.”41 This was true even
though Patton himself is Black, grew up in Chicago, and worked for years in many
of these same neighborhoods. “It became really clear to me that we needed to take a
deeper approach to social media data in particular, so that we could really grasp culture,
The Numbers Don’t Speak for Themselves                                                 163



context and nuance, for the primary reason of not misinterpreting what’s being said,”
he explains.42
   Patton’s approach to incorporating culture, context, and nuance took the form of
direct contact with and centering the perspectives of the youth whose behaviors his
group sought to study. Patton and doctoral student William Frey hired formerly gang-­
involved youth to work on the project as domain experts. These experts coded and cat-
egorized a subset of the millions of tweets, then trained a team of social work students
to take over the coding. The process was long and not without challenges. It required
that Patton and Frey create a new “deep listening” method they call the contextual
analysis of social media to help the student coders mitigate their own bias and get closer
to the intended meaning of each tweet.43 The step after that was to train a machine
learning classifier to automatically label the tweets, so that the project could categorize
all of the millions of tweets in the dataset. Says Patton, “We trained the algorithm to
think like a young African American man on the south side of Chicago.”44
   This approach illustrates how context can be integrated into an artificial intelligence
project, and can be done with an attention to subjugated knowledge. This term describes
the forms of knowledge that have been pushed out of mainstream institutions and the
conversations they encourage. To explain this phenomenon, Patricia Hill Collins gives
the example of how Black women have historically turned to “music, literature, daily
conversations, and everyday behavior” as a result of being excluded from “white male-­
controlled social institutions.”45 These institutions include academia, or—­for a recent
example raised by sociologist Tressie McMillan Cottom—­the op-­ed section of the New
York Times.46 And because they circulate their knowledge in places outside of those
mainstream institutions, that knowledge is not seen or recognized by those institu-
tions: it becomes subjugated.
   The idea of subjugated knowledge applies to other minoritized groups as well, includ-
ing the Black men from Chicago whom Patton sought to understand. An approach that
did not attend to this context would have resulted in significant errors. For example,
a tweet like “aint kill yo mans & ion kno ya homie” would likely have been classified
as aggressive or violent, reflecting its use of the word “kill.” But drawing on the knowl-
edge provided by the young Black men they hired for the project, Frey and Patton were
able to show that many tweets like this one were references to song lyrics, in this case
the Chicago rapper Lil Durk. In other words, these tweets are about sharing culture, not
communicating threats.47
   In the case of SAFElab, as with all research projects that seek to make use of subju-
gated knowledge, there is also significant human, relational infrastructure required.
164                                                                              Chapter 6



Frey and Patton have built long-­term relationships with individuals and organizations
in the community they study. Indeed, Frey lives and works in the community. In addi-
tion, both Frey and Patton are trained as social workers. This is reflected in their com-
putational work, which remains guided by the social worker’s code of ethics.48 They
are using AI to broker new forms of human understanding across power differentials,
rather than using computation to replace human relationships. This kind of social
innovation often goes underappreciated in the unicorn-­wizard-­genius model of data
science. (For more on unicorns, see chapter 5.) As Patton says, “We had a lot of chal-
lenges with publishing papers in data science communities about this work, because it
is very clear to me that they’re slow to care about context. Not that they don’t care, but
they don’t see the innovation or the social justice impact that the work can have.”49
Hopefully that will change in the future, as the work of SAFElab and others demon-
strates the tremendous potential of combining social work and data science.


Communicating Context


It’s not just in the stages of data acquisition or data analysis that context matters. Con-
text also comes into play in the framing and communication of results. Let’s imagine
a scenario. In this case, you are a data journalist, and your editor has assigned you to
create a graphic and short story about a recent research study: “Disparities in Mental
Health Referral and Diagnosis in the New York City Jail Mental Health Service.”50 This
study looks at the medical records of more than forty-­five thousand first-­time incar-
cerated people and finds that some groups are more likely to receive treatment, while
others are more likely to receive punishment. More specifically, white people are more
likely to receive a mental health diagnosis, while Black and Latinx people are more
likely to be placed in solitary confinement. The researchers attribute some of this diver-
gence to the differing diagnosis rates experienced by these groups before becoming
incarcerated, but they also attribute some of the divergence to discrimination within
the jail system. Either way, the racial and ethnic disparities are a product of structural
racism.
   Consider the difference between the two graphics shown in figure 6.6. The only
variation is the title and framing of the chart.
   Which one of these graphics would you create? Which one should you create? The
first—­Mental Health in Jail—­represents the typical way that the results of a data analy-
sis are communicated. The title appears to be neutral and free of bias. This is a graphic
about rates of mental illness diagnosis of incarcerated people broken down by race and
ethnicity. The people are referred to as inmates, the language that the study used. The
The Numbers Don’t Speak for Themselves                                                        165




Figure 6.6
Two portrayals of the same data analysis. The data are from a study of people incarcerated for the
first time in NYC jails between 2011 and 2013. Graphics by Catherine D’Ignazio. Data from Fatos
Kaba et al., “Disparities in Mental Health Referral and Diagnosis in the New York City Jail Mental
Health Service.”
166                                                                              Chapter 6



title does not mention race or ethnicity, or racism or health inequities, nor does the
title point to what the data mean. But this is where additional questions about context
come in. Are you representing only the four numbers that we see in the chart? Or are
you representing the context from which they emerged?
   The study that produced these numbers contains convincing evidence that we
should distrust diagnosis numbers due to racial and ethnic discrimination. The first
chart does not simply fail to communicate that but also actively undermines that main
finding of the research. Moreover, the language used to refer to people in jail as inmates
is dehumanizing, particularly in the context of the epidemic of mass incarceration
in the United States.51 So, consider the second chart: Racism in Jail: People of Color
Less Likely to Get Mental Health Diagnosis. This title offers a frame for how to inter-
pret the numbers along the lines of the study from which they emerged. The research
study was about racial disparities, so the title and content of this chart are about racial
disparities. The people behind the numbers are people, not inmates. In addition, and
crucially, the second chart names the forces of oppression that are at work: racism
in prison.
   Although naming racism may sound easy and obvious to some readers of this book,
it is important to acknowledge that fields like journalism still adhere to conventions
that resist such naming on the grounds that it is “bias” or “opinion.” John Daniszewski,
an editor at the Associated Press, epitomizes this view: “In general our policy is to try
to be neutral and precise and as accurate as we possibly can be for the given situation.
We’re very cautious about throwing around accusations of our own that characterize
something as being racist. We would try to say what was done, and allow the reader to
make their own judgement.”52
   Daniszewski’s statement may sound democratic (“power to the reader!”), but it’s
important to think about whose interests are served by making racism a matter of indi-
vidual opinion. For many people, racism exists as a matter of fact, as we have discussed
throughout this book. Its existence is supported by the overwhelming empirical evi-
dence that documents instances of structural racism, including wealth gaps, wage gaps,
and school segregation, as well as health inequities, as we have also discussed. Naming
these structural forces may be the most effective way to communicate broad context.
Moreover, as the data journalist in this scenario, it is your responsibility to connect
the research question to the results and to the audience’s interpretation of the results.
Letting the numbers speak for themselves is emphatically not more ethical or more
democratic because it often leads to those numbers being misinterpreted or the results
of the study being lost. Placing numbers in context and naming racism or sexism when
The Numbers Don’t Speak for Themselves                                                167



it is present in those numbers should be a requirement—­not only for feminist data
communication, but for data communication full stop.
   This counsel—­to name racism, sexism, or other forces of oppression when they are
clearly present in the numbers—­particularly applies to designers and data scientists
from the dominant group with respect to the issue at hand. White people, including
ourselves, the authors of this book, have a hard time naming and talking about racism.
Men have a hard time naming and talking about sexism and patriarchy. Straight people
have a hard time seeing and talking about homophobia and heteronormativity. If you
are concerned with justice in data communication, or data science more generally, we
suggest that you practice recognizing, naming, and talking about these structural forces
of oppression.53
   But our work as hypothetical anti-­oppression visualization designers is not over yet.
We might have named racism as a structural force in our visualization, but there are
still two problems with the “good” visualization, and they hinge on the wording of the
subtitle: People of Color Less Likely to Get Mental Health Diagnosis. The first problem
is that this is starting to look like a deficit narrative, which we discuss in chapter 2—a
narrative that reduces a social group to negative stereotypes and fails to portray them
with creativity and agency. The second issue is that by naming racism and then talking
about people of color in the title, the graphic reinforces the idea that race is an issue
for people of color only. If we care about righting the balance of power, the choice of
words matters as much as the data under analysis. In an op-­ed about the language used
to describe low-­income communities, health journalist Kimberly Seals Allers affirms
this point: “We almost always use a language of deficiency, calling them disadvantaged,
under-­resourced and under-­everything else. ... It ignores all the richness those commu-
nities and their young people possess: the wealth of resiliency, tenacity and grit that
can turn into greatness if properly cultivated.”54
   So let’s give it a third try, with the image in figure 6.7.
   In this third version, we have retained the same title as the previous chart. But
instead of focusing the subtitle on what minoritized groups lack, it focuses on the
unfair advantages that are given to the dominant group. The subtitle now reads, White
People Get More Mental Health Services. This avoids propagating a deficit narrative
that reinforces negative associations and clichés. It also asserts that white people have
a race, and that they derive an unfair advantage from that race in this case.55 Finally,
the title is proposing an interpretation of the numbers that is grounded in the context
of the researchers’ conclusions on health disparities.
168                                                                                    Chapter 6




Figure 6.7
A third portrayal of the same data, with only the framing title and subtitle changed. Source: Data
from Kaba et al., “Disparities in Mental Health Referral and Diagnosis in the New York City Jail
Mental Health Service.” Graphic by Catherine D’Ignazio. Data from Fatos Kaba et al., “Disparities
in Mental Health.”


Restoring Context


Three iterations on a single chart title might feel excessive, but it also helps to under-
score the larger point that considering context always involves some combination of
interest and time. Fortunately, there is a lot of energy around issues of context right
now, and educators, journalists, librarians, computer scientists, and civic data publish-
ers are starting to develop more robust tools and methods for keeping context attached
to data so that it’s easier to include in the end result.
   For example, remember figure 6.3, that confusing chart of government procure-
ments in São Paulo that we discussed earlier in this chapter? Gisele Craveiro, a professor
at the University of São Paulo, has created a tool called Cuidando do Meu Bairro (Caring
for My Neighborhood) to make that spending data more accessible to citizens by add-
ing additional local context to the presentation of the information.56 In the classroom,
Heather Krause, a data scientist and educator, has developed the concept of the “data
biography.”57 Prior to beginning the analysis process, Krause asks people working with
The Numbers Don’t Speak for Themselves                                                  169



data, particularly journalists, to write a short history of a particular dataset and answer
five basic questions: Where did it come from? Who collected it? When? How was it
collected? Why was it collected? A related but slightly more technical proposal advo-
cated by researchers at Microsoft is being called datasheets for datasets.58 Inspired by the
datasheets that accompany hardware components, computer scientist Timnit Gebru
and colleagues advocate for data publishers to create short, three-­to five-­page docu-
ments that accompany datasets and outline how they were created and collected, what
data might be missing, whether preprocessing was done, and how the dataset will be
maintained, as well as a discussion of legal and ethical considerations such as whether
the data collection process complies with privacy laws in the European Union.59
   Another emerging practice that attempts to better situate data in context is the
development of data user guides.60 Bob Gradeck, manager of the Western Pennsylva-
nia Regional Data Center, started writing data user guides because he got the same
questions over and over again about popular datasets he was managing, like prop-
erty data and 311 resident reports in Pittsburgh. Reports Gradeck, “It took us some
time to learn tips and tricks. ... I wanted to take the stuff that was in my head and
put it out there with additional context, so other data users didn’t have to do it
from scratch.”61 Data user guides are simple, written documents that each contain a
narrative portrait of a dataset. They describe, among other things, the purpose and
application of the data; the history, format, and standards; the organizational con-
text; other analyses and stories that have used the dataset; and the limitations and
ethical implications of the dataset. This is similar to the work that data journalists
are doing to compile datasets and then make them available for reuse. For example,
the Associated Press makes comprehensive national statistics about school segregation
in the United States available for purchase.62 The spreadsheets are accompanied by a
twenty-­page narrative explainer about the data that includes limitations and sample
story ideas.
   These developments are exciting, but there is further to go with respect to issues of
power and inequality that affect data collection environments. For example, professor
of political science Valerie Hudson has worked for decades to trace the links between
state security and the status of women. “I was interested in whether forms of oppres-
sion or subordination or violence against women were related to national, and perhaps
international, instability and conflict,” she explains. She and geographer Chad Emmett
started the project WomanStats as a modest Excel spreadsheet in 2001. It has since
grown to a large-­scale web database with over a quarter of a million data points, includ-
ing over 350 variables ranging from access to health care to the prevalence of rape to
the division of domestic labor.63
170                                                                               Chapter 6



   Notably, their sources are qualitative as well as quantitative. Says Hudson, “If you
want to do research on women, you have to embrace qualitative data. There’s no two
ways about it, because the reality of women’s lives is simply not captured in quanti-
tative statistics. Absolutely not.”64 At the present, WomanStats includes two types of
qualitative variables: practice variables are composed from women’s reports of their
lived experiences, and law variables are coded from the legal frameworks in a particular
country. Indeed, the WomanStats codebook is a context nerd’s dream that outlines
measurement issues and warns about the incompleteness of its own data, especially
with respect to difficult topics.65 In regard to the data that records reports of rape, for
example—­a topic upsetting enough to even consider, let alone contemplate its scale
and scope in an entire country—­the codebook states: “CAVEAT EMPTOR! Users are
warned that this scale only reflects reported rape rates, and for many, if not most, coun-
tries, this is a completely unreliable indicator of the actual prevalence of rape within a
society!”66 Instead of focusing on a single variable, users are directed to WomanStats’s
composite scales, like the Comprehensive Rape Scale, which look at reported preva-
lence in the context of laws, whether laws are enforced, reports from lived experience,
strength of taboos in that environment, and so on.
   So tools and methods for providing context are being developed and piloted. And
WomanStats models how context can also include an analysis of unequal social power.
But if we zoom out of project-­level experiments, what remains murky is this: Which
actors in the data ecosystem are responsible for providing context?
   Is it the end users? In the case of the missing Reddit comments, we see how even
the most highly educated among us fail to verify the basic claims of their data source.
And datasheets for datasets and data user guides are great, but can we expect individual
people and small teams to conduct an in-­depth background research project while
on a deadline and with a limited budget? This places unreasonable expectations and
responsibility on newcomers and is likely to lead to further high-­profile cases of errors
and ethical breaches.
   So is it the data publishers? In the case of GDELT, we saw how data publishers, in
their quest for research funding, overstated their capabilities and didn’t document the
limitations of their data. The Reddit comments were a little different: the dataset was
provided by an individual acting in good faith, but he did not verify—­and probably did
not have the resources to verify—­his claim to completeness. In the case of the campus
sexual assault data, it’s the universities who are responsible for self-­reporting, and they
are governed by their own bottom line.67 The government is under-­resourced to verify
and document all the limitations of the data.
The Numbers Don’t Speak for Themselves                                               171



   Is it the data intermediaries? Intermediaries, who have also been called infomedi-
aries, might include librarians, journalists, nonprofits, educators, and other public
information professionals.68 There are strong traditions of data curation and man-
agement in library science, and librarians are often the human face of databases for
citizens and residents. But as media scholar Shannon Mattern points out, librarians
are often left out of conversations about smart cities and civic technology.69 Examples
of well-­curated, verified and contextualized data from journalism, like the Associated
Press database on school segregation or other datasets available in ProPublica’s data
store, are also promising.70 The nonprofit Measures for Justice provides comprehen-
sive and contextualized data on criminal justice and incarceration rates in the United
States.71 Some data intermediaries, like Civic Switchboard in Pittsburgh, are build-
ing their own local data ecosystems as a way of working toward sustainability and
resilience.72 These intermediaries who clean and contextualize the data for public use
have potential (and have fewer conflicts of interest), but sustained funding, significant
capacity-­building, and professional norms-­setting would need to take place to do this
at scale.
   Houston, we have a public information problem. Until we invest as much in provid-
ing (and maintaining) context as we do in publishing data, we will end up with public
information resources that are subpar at best and dangerous at worst. This ends up get-
ting even more thorny as the sheer quantity of digital data complicates the verification,
provenance, and contextualization work that archivists have traditionally undertaken.
Context, and the informational infrastructure that it requires, should be a significant
focus for open data advocates, philanthropic foundations, librarians, researchers, news
organizations, and regulators in the future. Our data-driven lives depend on it.


Consider Context


The sixth principle of data feminism is to consider context. The bottom line for numbers
is that they cannot speak for themselves. In fact, those of us who work with data must
actively prevent numbers from speaking for themselves because when those numbers
derive from a data setting influenced by differentials of power, or by misaligned collec-
tion incentives (read: pretty much all data settings), and especially when the numbers
have to do with human beings or their behavior, then they run the risk not only of
being arrogantly grandiose and empirically wrong, but also of doing real harm in their
reinforcement of an unjust status quo.
   The way through this predicament is by considering context, a process that includes
understanding the provenance and environment from which the data was collected,
172                                                                          Chapter 6



as well as working hard to frame context in data communication (i.e., the numbers
should not speak for themselves in charts any more than they should in spreadsheets).
It also includes analyzing social power in relation to the data setting. Which power
imbalances have led to silences in the dataset or data that is missing altogether? Who
has conflicts of interest that prevent them from being fully transparent about their
data? Whose knowledge about an issue has been subjugated, and how might we begin
to recuperate it? The energy around context, metadata, and provenance is impressive,
but until we fund context, then excellent contextual work will remain the exception
rather than the norm.
7    Show Your Work


Principle: Make Labor Visible

The work of data science, like all work in the world, is the work of many hands. Data feminism
makes this labor visible so that it can be recognized and valued.




If you work in software development, chances are that you have a GitHub account.
As of June 2018, the online code-­management platform had over twenty-­eight mil-
lion users worldwide. By allowing users to create web-­based repositories of source code
(among other forms of content) to which project teams of any size can then contribute,
GitHub makes collaborating on a single piece of software or a website or even a book
much easier than it has ever been before.
    Well, easier if you’re a man. A 2016 study found that female GitHub users were less
likely to have their contributions accepted if they identified themselves in their user
profiles as women. (The study did not consider nonbinary genders.)1 Critics of GitHub’s
commitment to inclusivity, or the lack thereof, also point to the company’s internal poli-
tics. In 2014, GitHub’s cofounder was forced to resign after allegations of sexual harass-
ment were brought to light.2 More recently, in 2018, Agnes Pak, a former top attorney
at GitHub, sued the company for allegedly altering her performance reviews after she
complained about her gender and race contributing to a lower compensation package,
giving them the grounds to fire her.3 Pak’s suit came only shortly after transgender soft-
ware developer Coraline Ada Ehmke, in 2017, declined a significant severance package so
that she could talk publicly about her negative experience of working at GitHub.4 Clearly,
GitHub has several major issues of corporate culture that it must address.
    But a corporate culture that is hostile to women does not necessarily preclude other
feminist interventions. And here GitHub makes one important one: its platform helps
show the work of writing collaborative code. In addition to basic project management
tools, like bug tracking and feature requests, the GitHub platform also generates visu-
alizations of each team member’s contributions to a project’s codebase. Area charts,
arranged in small multiples, allow viewers to compare the quantity, frequency, and
duration of any particular member’s contributions (figure 7.1a). A line graph reveals
patterns in the day of the week when those contributions took place (figure 7.1b). And
a flowchart-­like diagram of the relationships between various branches of the project’s
174                                                                                    Chapter 7




Figure 7.1
(a) The first of three visualizations of the code associated with a project from Lauren’s research
group, showing the significant contributions of student researchers between the years 2014 and
2019. (b) A bar chart shows the frequency of code commits over time, and a line graph shows any
patterns in the day of the week when the commits were made. Screenshot by Lauren F. Klein. (c) A
flowchart-like diagram documents the relationships between the various branches of the project’s
codebase. Screenshots by Lauren F. Klein.
Show Your Work                                                                       175




Figure 7.1 (continued)


code helps to acknowledge any sources for the project that might otherwise go uncred-
ited, as well as any additional projects that might build upon the project’s initial work
(figure 7.1c).
   Coding is work, as anyone who’s ever programmed anything knows well. But it’s not
always work that is easy to see. The same is true for collecting, analyzing, and visual-
izing data. We tend to marvel at the scale and complexity of an interactive visualiza-
tion like the Ship Map, in figure 7.2, which plots the paths of the global merchant fleet
over the course of the 2012 calendar year.5 By showing every single sea voyage, the
Ship Map exposes the networks of waterways that constitute our global product sup-
ply chain. But we are less often exposed to the networks of processes and people that
176                                                                              Chapter 7




Figure 7.1 (continued)


help constitute the visualization itself—­from the seventy-­five corporate researchers at
Clarksons Research UK who assembled and validated the underlying dataset, to the
academic research team at University College London’s Energy Institute that developed
the data model, to the design team at Kiln that transformed the data model into the
visualization that we see. And that is to say nothing of the tens of thousands of com-
mercial ships that served as the source of data in the first place. Visualizations like the
Ship Map involve the work of many hands.
   Unfortunately, however, when releasing a data product to the public, we tend not
to credit the many hands who perform this work. We often cite the source of the
dataset, and the names of the people who designed and implemented the code and
graphic elements. But we rarely dig deeper to discover who created the data in first
Show Your Work                                                                                 177




Figure 7.2
A time-­based visualization of global shipping routes, designed by Kiln in 2016, based on data from
the University College London Energy Institute (UCL EI). The Ship Map website was created by
Duncan Clark and Robin Houston from Kiln, and the dataset was compiled by Julia Schaumeier
and Tristan Smith from the UCL EI. The website also includes a soundtrack: Bach’s Goldberg
Variations, played by Kimiko Ishizaka.
178                                                                              Chapter 7



place, who collected the data and processed them for use, and who else might have
labored to make creations like the Ship Map possible. Admittedly, this information is
sometimes hard to find. And when project teams (or individuals) are already operating
at full capacity, or under budgetary strain, this information can—­ironically—­simply be
too much additional work to pursue.6 Even in cases in which there are both resources
and desire, information about the range of the contributors to any particular project
sometimes can’t be found at all. But the various difficulties we encounter when trying
to acknowledge this work reflects a larger problem in what information studies scholar
Miriam Posner calls our data supply chain.7 Like the contents of the ships visualized
on the Ship Map, about which we only know sparse details—­the map can tell us if a
shipping container was loaded onto the boat, but not what the shipping container
contains—­the invisible labor involved in data work, as Posner argues, is something that
corporations have an interest in keeping out of public view.
   To put it more simply, it’s not a coincidence that much of the work that goes into
designing a data product—­visualization, algorithm, model, app—­remains invisible and
uncredited. In our capitalist society, we tend to value work that we can see. This is
the result of a system in which the cultural worth of any particular form of work is
directly connected to the price we pay for it; because a service costs money, we rec-
ognize its larger value. But more often than not, the reverse also holds true: we fail to
recognize the larger value of the services we get for free. When, in the early 1970s, the
International Feminist Collective launched the Wages for Housework campaign, it was
this phenomenon of invisible labor—­labor that was unpaid and therefore unvalued—­
that the group was trying to expose (figure 7.3).8 The precise term they used to describe
this work was reproductive labor, which comes from the classical economic distinction
between the paid and therefore economically productive labor of the marketplace, and
the unpaid and therefore economically unproductive labor of everything else. By refram-
ing this latter category of work as reproductive labor, rather than simply (and inac-
curately) unproductive labor, groups like the International Feminist Collective sought
to emphasize how the range of tasks that the term encompassed, like cooking and
cleaning and child-­rearing, were precisely the tasks that enabled those who performed
“productive” labor, like office or factory work, to continue to do so.
   The Wages for Housework movement began in Italy and migrated to the United
States with the help of labor organizer and theorist Silvia Federici. It eventually claimed
chapters in several American cities, and did important consciousness-­raising work.9
Still, as prominent feminists like Angela Davis pointed out, while housework might
have been unpaid for white women, women of color—­especially Black women in the
United States—­had long been paid, albeit not well, for their housework in other peo-
ple’s homes: “Because of the added intrusion of racism, vast numbers of Black women
Show Your Work                                                                        179




Figure 7.3
A Wages for Housework march, 1977. Photograph by Bettye Lane. Courtesy of the Schlesinger
Library, Radcliffe Institute/Bettye Lane.


have had to do their own housekeeping and other women’s home chores as well.”10
Here, Davis is making an important point about racialized labor: just as housework is
structured along the lines of gender, it is also structured along the lines of race and
class. The domestic labor of women of color was and remains underwaged labor, as femi-
nist labor theorists would call it, and its low cost was what permitted (and continues to
permit) many white middle-­and upper-­class women to participate in the more lucra-
tive waged labor market instead.11
  Since the 1970s, the term invisible labor has come to encompass the various forms
of labor, unwaged, underwaged, and even waged, that are rendered invisible because
they take place inside of the home, because they take place out of sight, or because they
lack physical form altogether.12 Visit WagesforFacebook.com and you’ll find a version
of the Wages for Housework argument updated for a new form of invisible work. This
invisible labor can be found all over the web, as digital labor theorists such as Tiziana
Terranova have helped us to understand.13 “They call it sharing. We call it stealing,” is
one of the statements that scrolls down the screen in large black type. The word it refers
to work that most of us perform every day, in the form of our Facebook likes, Instagram
180                                                                                Chapter 7



posts, and Twitter tweets. The point made by Laurel Ptak, the artist behind Wages for
Facebook—­a point also made by Terranova—­is that the invisible unpaid labor of our
likes and tweets is precisely what enables the Facebooks and Twitters of the world to
profit and thrive.


The Invisible Labor of Data Science


The world of data science is able to profit and thrive because of unpaid invisible labor
as well. How did Netflix improve its movie recommendation algorithm? The com-
pany crowdsourced it.14 How did the Guardian, the British newspaper, determine which
among two million leaked documents might contain incriminating information about
government misspending? The paper crowdsourced it.15 The optical character recogni-
tion (OCR) error correction performed on the dataset of early modern books that you
downloaded for your text-­analysis project? That was crowdsourced, too.16
   Each of these crowdsourcing projects were framed as acts of benevolence (and, in
the case of Netflix, an opportunity to win a million-­dollar prize). People should want
to contribute to these projects, their proponents claimed, since their labor would fur-
ther the public good.17 However, Ashe Dryden, the software developer and diversity
consultant, points out that people can only help crowdsource if they have the inclina-
tion and the time.18 Think back to that study of GitHub. If you were a woman and you
knew your contributions to a programming project were less likely to be accepted than
if you were a man, would that motivate you to contribute the project? Or, for another
example, consider Wikipedia. Although the exact gender demographics of Wikipedia
contributors are unknown, numerous surveys have indicated that those who contrib-
ute content to the crowdsourced encyclopedia are between 84 percent and 91.5 percent
men.19 Why? It could be that there, too, edits are less likely to be accepted if they come
from women editors.20 It could also be attributed to Wikipedia’s exclusionary editing
culture and technological infrastructure, as science and technology studies (STS) schol-
ars Heather Ford and Judy Wajcman have argued.21 And there is also reason to go
back to the housework argument. Dryden cites a 2011 study showing that women in
twenty-­nine countries spend more than twice as much time on household tasks than
men do, even when controlling for women who hold full-­time jobs.22 The study did
not consider nonbinary genders or same-­sex (or other non-­hetero-­typical) households.
But even as a rough estimate, it seems that women simply don’t have as much time.23
   In capitalist societies, it’s very often the case that time is money. But it’s also impor-
tant to remember to ask whose time is being spent and whose money is being saved.
                                            or MTurk, as the crowdsourcing
The premise behind Amazon’s Mechanical Turk—­
Show Your Work                                                                         181



platform is more commonly known—­is that data scientists want to save their own
time and their own bottom line.24 The MTurk website touts its access to a global mar-
ketplace of “on-­demand Workers,” who are advertised as being more “scalable and
cost-­effective” than the “time consuming [and] expensive” process of hiring actual
employees.25 But the data-­entry and data-­processing tasks performed by these workers
earn them less than minimum wage, even as a recent study by the Pew Research Center
showed that 51 percent of US-­based Turkers, as they are known, hold college degrees,
and 88 percent are below the age of fifty, among other metrics that would otherwise
rank them among the most desired demographic for salaried employees.26 This form
of underwaged work is also increasingly outsourced from the United States to coun-
tries with fewer (or worse) labor laws and fewer (or worse) opportunities for economic
advancement. A 2010 University of California, Irvine study measured a 20 percent drop
in the number of US-­based Turkers over the eighteen months that it monitored.27 This
trend has continued, the real-­time MTurk tracker shows. (The gender split, interest-
ingly, has evened out over time.)
   Even at resource-­rich companies like Amazon and Google, the work of data entry
is profoundly undervalued in proportion to the knowledge it helps to create. Andrew
Norman Wilson’s 2011 documentary Workers Leaving the Googleplex (figure 7.4) exposes
how the workers tasked with scanning the books for the Google Books database are
hired as a separate but unequal class of employee, with ID cards that restrict their access
to most of the Google campus and that prevent them from enjoying the company’s
famed employee perks.28 (Evidently, working overtime to preserve the world’s cultural
heritage still does not entitle you to a free lunch, let alone a free class on how to cook
Pad Kee Mao.)29
   Wilson also observes that Google’s book-­scanning workers are disproportionately
women and people of color—­a fact that would not surprise the long line of women
of color scholar-­activists, including Angela Davis, Patricia Hill Collins, and Evelyn
Nakano Glenn, who have insisted that economic oppression be recognized as a vector
that cuts across the matrix of domination as a whole. Information studies scholar Lilly
Irani confirms that “today’s hierarchy of data labor echoes older gendered, classed, and
raced technology hierarchies.”30 Here, Irani compares the hierarchy of data labor to
the hierarchy encountered by the first generation of female computers, like Christine
Darden, whom we discussed in this book’s introduction.31 But Irani’s own research also
considers contemporary digital labor practices, and in particular, Amazon’s Mechanical
Turk, the people it employs, and the people it exploits. In 2008, Irani and collabora-
tors built a web tool called the Turkopticon, which enabled Turkers to anonymously
report unfair labor conditions, as well as any additional information that might help
182                                                                                  Chapter 7




Figure 7.4
Andrew Norman Wilson’s Workers Leaving the Googleplex (2011) documents the hidden inequities
at Google’s Mountain View headquarters. Still courtesy of Andrew Norman Wilson.


them decide whether to accept future tasks.32 Irani envisioned the Turkopticon as a
worker-­led project. However, the same unfair labor conditions that necessitated the
tool also ultimately limited its reach. In 2018, after ten years of service, its all-­volunteer
team of moderators called it quits. “We’re all burned out,” they wrote on Twitter.33 And
amid tagging images and correcting error-­laden text, no additional Turkers could find
the time.
   The people who perform this cultural data work, as Irani terms it, are not only found
on the MTurk platform, however. They’re also increasingly the people on whom the
entire information economy depends. Cultural data workers are responsible for the
invisible labor involved in moderating the veritable deluge of content produced online
every day, ensuring that your Facebook feed is free of, for example, child pornography
and violent propaganda videos. When a 2014 exposé in Wired magazine documented
Show Your Work                                                                         183



the emotional costs of this labor, performed by some of the least empowered of these
workers—­women in the Global South—­it was met with an outpouring of shock and
outrage.34 But subsequent studies like Ghost Work, by anthropologist Mary Gray and
computer scientist Siddharth Suri, have documented the existence of a large “global
underclass” performing this work of content moderation, transcription, and caption-
ing.35 They make the point that the so-­called automation of artificial intelligence relies
on a vast number of human beings in the loop.36 Moreover, while the demographics
of Silicon Valley tech workers remain steadily young, white, and male, these global
“ghost workers” are often older women of color, and always required to accept precari-
ous labor conditions.
   Those who study the human costs of global capitalism would be quick to point out
that this exploitation of precarious, racialized, colonial labor has a long history, one
that has its roots in the original form of human exploitation: slavery. Slavery and capi-
talism are closely connected, after all, and one infamous story is often told to illustrate
this point: in 1781, the British slave ship Zong made a series of navigational errors
while crossing the Atlantic, resulting in a shortage of drinking water for the seventeen
crew members and 133 captives on board.37 After performing a cost-­benefit analysis,
the captain decided to throw the crew’s enslaved human “cargo” overboard so that the
crew members could consume all of the remaining water and rations themselves. The
decision was made because the captain calculated that he could collect enough insur-
ance money on his captives’ loss of life to come out ahead, even if he couldn’t sell them
once they landed ashore. He was thinking about human lives solely in terms of their
market value—­the notion that capitalism holds in highest regard.
   The stark inhumanity of this calculation has prompted numerous scholars and artists
to return to the Zong as they reckon with what Christina Sharpe calls, in the language
of the ship, the “wake” of slavery.38 Poet M. NourbeSe Philip, for example, composed
a book-­length poem, Zong!, using only the words of the legal case that serves as the
sole documentation of the original event.39 Written over the course of many years and
published in 2011, Philip’s poem plucks words and short phrases out of the language
of the court case, arranging them across the printed page. Philip’s poem regularly shifts
tenses from past to present, and from present to past, lending an additional voice to
Sharpe’s claim that the effects of that originary crime—­the exploitation of Black bodies
for white financial gain—­are far from resolved.40
   Our present technological infrastructure follows this same pattern of exploitation.
In the United States, the scarcely paid or altogether unpaid labor of those who endure
a contemporary form of enslavement—­incarceration—­has been used for everything
from packaging Windows software to cleaning up the 2010 BP oil spill.41 In a global
184                                                                             Chapter 7



colonial context, we might consider how the cobalt required to produce the lithium-­
ion batteries that power our cell phones and laptops is associated with significant
human rights violations, including coercing labor from Congolese children as young
as seven.42 The unregulated disposal of electronics has resulted in roadside salvage and
repair shops in places like Agbogbloshie, just outside of Accra, Ghana, which have long
served as sites of invention and ingenuity, being transformed into toxic “e-­waste” sites,
with profound consequences for the health of those who live and work there, as well
as for the environment.43 The humanitarian and ecological stakes of our attachments
to data and technology cannot be higher, nor can their source be any more clear: the
capitalist and colonial forces that encourage the exploitation of Black and brown bod-
ies so that white bodies can thrive.44


Examining Data Production


The forces of global capitalism can feel overwhelming. And as people who use data and
technology in our everyday work, we are each complicit to varying degrees. But there
are certain small things we can do in our work, and in our work with others, to push
back against this weight. In prior chapters, we have described some of these possibili-
ties: incorporating an examination of power into a data analysis project (chapters 1 and
2); pushing back against false binaries and hierarchies (chapter 4); including multiple
and marginalized voices in the design process (chapter 5); and contextualizing data so
that they are not imagined to “speak for themselves” (chapter 6).
   Along with these starting points, we can also begin to carve out additional space for
the scholars, journalists, and other researchers who are explicitly studying the labor of
data science—­those who are examining and challenging power by tracing visualiza-
tions and algorithms and bots back to their human and material sources. This growing
area of research might be called data production studies, borrowing a rubric from the
field of production studies that currently sits at the intersection of film and media
studies and labor studies. The primary focus of production studies, as it relates to film
and media, is how media artifacts are produced. Media studies scholar Miranda Banks
has asserted that “production studies is a feminist methodology” because it pays par-
ticular attention to the power differentials involved in the media production process,
as well as the material conditions of media workers.45 Work focused on data production
is already happening in fields like STS, the digital humanities, library and informa-
tion science, and archival studies, among others.46 It looks at the production process
of datasets, algorithms, and models, and traces those products back to the people and
conditions that enabled their creation.
Show Your Work                                                                          185



   As an example of work in this emerging area, we might consider “Anatomy of an AI
System,” a project by technology researcher Kate Crawford and design scholar Vladlan
Joler that seeks to describe and diagram the human labor, data dependencies, and mate-
rial resources that contribute to a single Amazon Echo. The project was published online
as a diagram of Borgesian proportions, too big to view in its entirety on a standard laptop
screen (figure 7.5a); it was accompanied by a nine-­thousand-­word essay. Viewers are first
introduced to the mineral extraction required to produce the electronics components
for the device and made aware of the hard labor (and sometimes child labor) this task
requires. The chart (and narrative) proceeds through processes of refining, assembling,
and distributing these components, then transporting them physically, then transport-
ing them virtually—­through the infrastructure of the internet. Once within the Ama-
zon corporate boundary, the chart depicts the layers of workers who provide everything
from network maintenance to training datasets (figure 7.5b). Crawford and Joler also
diagram patterns in the organization of Amazon’s labor force, which they describe in
terms of “fractal chains of production and exploitation.” But what is required for this
replication is people: “At every level contemporary technology is deeply rooted in and
running on the exploitation of human bodies,” the essay concludes.47
   “Anatomy of an AI System” is an investigation and exposé of the invisible labor
involved in making a single product on a global scale. In this way, it is an ambitious
example of the seventh principle of data feminism: show the work. Behind the magic
and marketing of data products, there is always hidden labor—­often performed by
women and people of color, which is both a cause and effect of the fact that this labor
is both underwaged and undervalued. Data feminism seeks to make this labor visible
so that it can be acknowledged and appropriately valued, and so that its truer cost—­for
people and for the planet—­can be recognized.


Crediting Data Work


The emphasis on giving formal credit for a broad range of work derives from feminist
practices of citation. Feminist theorist Sara Ahmed describes this practice as a way of
resisting how certain types of people—­usually cis and white and male—­“take up spaces
by screening out others.”48 When those other people are screened out, they become


Figure 7.5 (following three pages)
Overview (a) and detail (b) of “Anatomy of an AI System” (2018)—­a diagram and essay by Kate
Crawford and Vladan Joler that attempts to chart all of the human labor, data, and planetary
resources used to create an Amazon Echo device. Courtesy of Kate Crawford and Vladan Joler.
188                                                                             Chapter 7




Figure 7.5 (continued)


invisible, and their contributions go unrecognized. The screening techniques that lead to
their erasure, as Ahmed terms them, are not always intentional, but they are, unfortu-
nately, self-­perpetuating. Ahmed gives the example of sinking into a leather armchair
that is comfortable because it’s molded to the shape of your body over time. You prob-
ably wouldn’t notice how the chair would be uncomfortable for those who haven’t
spent time sitting in it—­those with different bodies or with different demands on
their time. Which is why those of us who occupy those comfortable leather seats—­or,
more likely in the design world, molded plastic Eames chairs—­must remain vigilant in
reminding ourselves of the additional forms of labor, and the additional people, that
our own data work rests upon.
   This gets complicated quickly even on the scale of a single data science project. The
names of all the people and the work they perform are not always easy to locate—­if
they can be located at all. But taking steps to document all the people who work on a
particular project at the time that it is taking place can help to ensure that a record of
that work remains after the project has been completed. In fact, this is among the four
Show Your Work                                                                         189



core principles that comprise the Collaborators’ Bill of Rights, a document developed
by an interdisciplinary team of librarians, staff technologists, scholars, and postdoctoral
fellows in 2011 in response to the proliferation of types of positions, at widely divergent
ranks, that were being asked to contribute to data-­based (and other digital) projects.49
   When designing data products from a feminist perspective, we must similarly aspire
to show the work involved in the entire lifecycle of the project. This remains true even
as it can be difficult to name each individual involved or when the work may be collec-
tive in nature and not able to be attributed to a single source. In these cases, we might
take inspiration from the Next System Project, a research group aimed at document-
ing and visualizing alternative economic systems.50 In one report, the group compiled
information on the diversity of community economies operating in locations as far-­
ranging as Negros Island, in the Philippines; Quebec province, in Canada; and the state
of Kerala, in India. The report employs the visual metaphor of an iceberg (figure 7.6), in
which wage labor is positioned at the tip of the iceberg, floating above the water, while
dozens of other forms of labor—­informal lending, consumer cooperatives, and work
within families, among others—­are positioned below the water, providing essential
economic ballast but remaining out of sight.
   With the idea of underwater labor in mind, we might return to the example of
GitHub, which began this chapter, to ask what additional forms of labor might contrib-
ute to the production of code but cannot be represented by the visualization scheme
that GitHub currently employs. We might think of the work of the project manager,
which is not directly expressed in a particular number or size or frequency of contri-
butions, but nevertheless ensures the quality and consistency of all project code. We
might wonder about the work of the designer on a project or of the technical writer—­
both of whom might have helped to shape the project in its initial phases, but who
have likely moved on to other tasks. In the case of a consumer-­facing project, we might
also consider the contributions of the customer support teams. Or in a community-­
oriented project, we might include organizers who have spent years developing strong
relationships with community members. These forms of labor, both productive and
reproductive, are essential to the success of any project but are not currently rendered
visible, nor could they ever be easily visualized, by a scheme that considers project
contributions to consist of code alone.51
   But in more instances than you might think, the labor associated with data work can
be surfaced through the data themselves. For instance, historian Benjamin Schmidt,
whose research centers on the role of government agencies in shaping public knowl-
edge, decided to visualize the metadata associated with the digital catalog of the US
Library of Congress, the largest library in the world (figure 7.7).52 Schmidt’s initial
goal was to understand the collection and the classification system that structured the
Figure 7.6
The “Diverse Economies Iceberg” (2017), a diagram of multiple labor practices created by the
Next System Project for a report on cultivating community economies. Image courtesy of J. K.
Gibson-­Graham, Jenny Cameron, Kelly Dombrowski, Stephen Healy, and Ethan Miller for the
Next System Project.
Figure 7.7
“A Brief Visual History of MARC Cataloging at the Library of Congress” (2017) visualizes when
books at the Library of Congress entered their digital catalog. Image courtesy of Benjamin M.
Schmidt.
192                                                                             Chapter 7



catalog. But in the process of visualizing the catalog records, he discovered something
else: a record of the labor of the cataloguers themselves. When he plotted the year that
each book’s record was created against the year that the book was published, he saw
some unusual patterns in the image: shaded vertical lines, step-­like structures, and dark
vertical bands that didn’t match up with what one might otherwise assume would be a
basic two-­step process of (1) acquire a book and (2) enter it in.
   The shaded vertical lines, Schmidt soon realized, showed the point at which the
cataloguers began to turn back to the books that had been published before the library
went digital, filling in the online catalogue with older books. The step-­like patterns
indicated the periods of time, later in the process, when the cataloguers returned to
specific subcollections of the library, entering in the data for the entire set of books
in a short period of time. And the horizontal lines? Well, given that they appear only
in the years 1800 and 1900, Schmidt inferred that they indicated missing publication
information, as best practices for library cataloguing dictate that the first year of the
century be entered when the exact publication date is unknown.
   With an emphasis on showing the work, these visual artifacts should also prompt
us to consider just how much physical work was involved in converting the library’s
paper records to digital form. The darker areas of the chart don’t just indicate a larger
number of books entered into the catalog, after all. They also indicate the people who
typed them all in. (Schmidt estimates the total number of records at ten million and
growing.) Similarly, the step-­like formations don’t just indicate a higher volume of data
entry. They indicate strategic decisions made by library staff to return to specific parts
of the collection and reflect those staff members’ prior knowledge of the gaps that
needed to be filled—­in other words, their intellectual labor as well. Schmidt’s visual-
ization helps to show how the dataset always points back to the data setting—­to use
Yanni Loukissas’s helpful phrase—­as well as to the people who labored in that setting
to produce the data that we see.53


Crediting Emotional Labor and Care Work


In addition to the invisible labor of data work, there is also labor that remains hid-
den because we are not trained to think of it as labor at all. This is what is known
as emotional labor, and it’s another form of work that feminist theory has helped to
bring to light.54 As described by feminist sociologist Arlie Hochschild, emotional labor
describes the work involved in managing one’s feelings, or someone else’s, in response
to the demands of society or a particular job.55 Hochschild coined the term in the
late 1970s to describe the labor required of service industry workers, such as flight
Show Your Work                                                                           193



attendants, who are required to manage their own fear while also calming passengers
during adverse flight conditions, and generally work to ensure that flight passengers
feel cared for and content. In the decades that followed, the notion of emotional labor
was supplemented by a related concept, affective labor, so that the work of projecting a
feeling (the definition of emotion) could be distinguished from the work of experienc-
ing the feeling itself (the definition of affect).56
   We can see both emotional and affective labor at work all across the technology
industry today. Consider, for instance, how call center workers and other technical
support specialists must exert a combination of affective and emotional labor, as well
as technical expertise, to absorb the rage of irate customers (affective labor), reflect back
their sympathy (emotional labor), and then help them with—­for instance—­the con-
figuration of their wireless router (technical expertise).57 In the workplace, we might
also consider the affective labor required by women and minoritized groups, in all situ-
ations, who must take steps to disprove (or simply ignore) the sexist, racist, or otherist
assumptions they face—­about their technical ability or about anything else. And they
must do so while also performing the emotional labor that ensures that they do not
threaten those who hold those assumptions, who often also hold positions of power
over them.58 Are there ways to visualize these forms of labor, giving visual presence—­
and therefore acknowledgement and credit—­to these outlays of work?
   One example that strives to visualize emotional and affective labor is the Atlas of
Caregiving (figure 7.8), an ongoing project that aims to document the work involved
in caring for a chronically ill family member. The project’s name plays on the concept
of the anatomy atlas, a compendium of illustrations of the human body that doctors
can consult for information and reference. In this case, the goal was to illustrate the
sometimes physical and sometimes emotional or affective work of care. The research
team outfitted its participants with a variety of biometric sensors, including acceler-
ometers and heart rate monitors, as well as with body cameras programmed to take a
picture every fifteen minutes. They then visualized these data alongside excerpts from
personal interviews and from the activity logs they asked the caregivers in the study
to complete.
   The result is a complex picture of caregiving, one that marshals data in the interest
of creating a comprehensive view of the range of labor involved in caregiving work.59
The stress of serving as a caregiver—­a form of affective labor—­is broken down into six
distinct levels, and then visualized as a gradient (figure 7.8a). The work of caregiving
itself is divided into seven subtypes of work, including concrete tasks like healthcare
management and household chores, and more abstract forms of labor like being avail-
able and social support (figure 7.8b). This, too, helps others recognize the wide range
                                  194                                                                                                                                                             Chapter 7



                                                                                                                                                                                                                                                  Caregiving
                                                                                                                                                                                                                                                  Stress level 5
                                                                                                                                                                                                                                                  Stress level 4
                                                                                                                                                                                                                                                  Stress level 3
                                                                                                                                                                                                                                                  Stress level 2
                                                                                                                                                                                                                                                  Stress level 1
                                                                                                                                                                                                                                                  Stress level 0
                                                                                                                                                                                                                                                  Self Care

                                                                                                                                                                                                                                                  Leisure

                                                                                                                                                                                                                                                  Work

                                                                                                                                                                                                                                                  Other

                                                                                                                                                                                                                                                  Sleep

 9    10     11   12    1     2    3    4   5   6       7       8        9        10     11      12   1   2   3   4   5         6    7     8    9     10       11      12        1       2        3       4     5       6      7       8      9
Day 1             PM                                                                            Day 2                                                                  PM


        HOURS SPENT ON ALL ACTIVITIES                                                                                           MINUTES SPENT ON CAREGIVING

                                                                                                                           6                        Medications and supplements (including injections, IVs, oxygen, etc.)                     Medical Activities
                                                                                                                          90                        Exercise, physical therapy
                                                                                                                                                    Equipment preparation and maintenance
                                                                                                                                                    Wound management

                                                                                                                                                    Tracking symptoms and body measurements (weight, temp, etc.)
 6.51                                                                                   Caregiving
                                                                                                                                                    Preparing special meals

   0                                                                                    Self Care
                                                                                                                                                    Arranging appointments                                                              Healthcare Management

 1.58                                                                                   Leisure                                                     Communicating with health professionals
                                                                                                                                                    Visits with health professionals
   0                                                                                    Work
                                                                                                                          23                        Buying prescriptions and supplies

 4.36                                                                                   Other                                                       Insurance and payments
                                                                                                                      102                           Researching conditions and treatments
 6.08                                                                                   Sleep
                                                                                                                                                    Researching healthcare costs


        0    1     2    3     4     5   6   7       8       9       10       11    12                                                               Keeping family and friends informed                                     Care Communication & Coordination
                                                                                                                          23                        Managing family and paid caregivers

                                                                                                                                                    Managing community services (paratransit, meals on wheels, etc.)


                                                                                                                          32                        Bathing and toileting                                                   "ADLs" Help with Personal Activities
                                                                                                                          11                        Dressing and grooming
                                                                                                                          10                        Feeding
                                                                                                                                                    Getting in/out of bed, chair, etc
                                                                                                                          17                        Moving around the home

                                                                                                                          13                        Cleaning                                                                         "IADLs" Household Chores
                                                                                                                          12                        Cooking

                                                                                                                           1                        Laundry

                                                                                                                                                    Shopping

                                                                                                                          67                        Getting/Moving/Using things

                                                                                                                                                    Managing bills and savings
                                                                                                                                                    Transportation to/from home


                                                                                                                                                    Companionship                                                                                 Social Support
                                                                                                                          54                        Emotional support
                                                                                                                                                    Plan and support participation in social activities


                                                                                                                          21                        Be constantly "on alert" for any needs                                                          Be Available
                                                                                                                                                    Be "on-call" for problems

                                                                                                                          482                                                                                                                               Total




                                  Figure 7.8
                                  The Atlas of Caregiving visualizes the labor of caring for chronically ill family members. (a) A
                                  thirty-­six-­hour log of caregiving activities; (b) caregiving activities separated by type; (c) a photo
                                  log created during that same time. Image courtesy of the Atlas of Caregiving, 2016.
                 Show Your Work                                                                                195



9:00    9:15   9:30      9:45    10:00    10:15   10:30   10:45    11:00   11:15    11:30   11:45    12:00   12:15    12:30   12:45
Day 1




1:00    1:15   1:30      1:45    2:00     2:15    2:30    2:45     3:00    3:15     3:30    3:45     4:00    4:15     4:30    4:45




5:00    5:15   5:30      5:45    6:00     6:15    6:30    6:45     7:00    7:15     7:30    7:45     8:00    8:15     8:30    8:45




9:00    9:15   9:30      9:45    10:00    10:15   10:30   10:45    11:00   11:15    11:30   11:45    12:00   12:15    12:30   12:45
                                                                                                     Day 2




1:00    1:15   1:30      1:45    2:00     2:15    2:30    2:45     3:00    3:15     3:30    3:45     4:00    4:15     4:30    4:45




5:00    5:15   5:30      5:45    6:00     6:15    6:30    6:45     7:00    7:15     7:30    7:45     8:00    8:15     8:30    8:45




                 Figure 7.8 (continued)


                 of work—­indeed, expertise—­associated with caregiving. And as some of the study’s
                 participants reported, it helped them to recognize that work for themselves.60
                      Of course, a diagram of work is only a proxy for the work itself—­and that is to say
                 nothing about the complexity of human feelings. This understanding served as the gen-
                 esis for “Bruises—­the Data we Don’t See.”61 This artful visualization, created by designer
                 Giogia Lupi and accompanied by a musical score composed by Kaki King, attempts to
                 get closer to a visual representation of the emotional toll of caregiving (figure 7.9). The
                 project began when King’s daughter was diagnosed with a rare autoimmune disease,
                 idiopathic thrombocytopenic purpura (ITP). ITP is described as a “very visual disease,”
                 and presents as bruises and burst blood vessels all over the body. For this reason, King
                 was instructed to watch her daughter’s skin and record any significant changes. She
                 also recorded her own feelings in terms of hope, stress, and fear, creating subjective
                 data to complement the hard numbers she received from the blood tests her daughter
                 was required to endure.
                      When Lupi, who knew King from previous collaborations, set out to design her
                 visualization, her goal was to “evoke empathy” and help her audience “feel a part of



                 Figure 7.9 (following three pages)
                 Still from Bruises—­the Data We Don’t See (2018) and the legend that helps decode the data visual-
                 ization. Image courtesy of Giorgia Lupi and Kaki King.
198                                                                              Chapter 7




Figure 7.9 (continued)


a story of a human’s life.”62 In contrast to the Atlas of Caregiving, which relies upon
standard visualization techniques like radial timelines and Gantt-­style charts to legiti-
mate the work of care, Lupi sought alternative visualization strategies to emphasize the
particularity and specificity of a single family’s situation. She employed a fluid timeline
to reflect the subjective nature of what disability studies scholars call crip time. With
this term, as Ellen Samuels explains it, “Sometimes we just mean that we’re late all the
time—­maybe because we need more sleep than nondisabled people, maybe because the
accessible gate in the train station was locked.”63 But it can also mean something more
profound, as described by Alison Kafer: “Rather than bend disabled bodies and minds
to meet the clock, crip time bends the clock to meet disabled bodies and minds.”64
   In Lupi’s depiction of how the clock met King’s daughter’s body and King’s own
mind, days became white aspen-­shaped leaves, segmented not by weeks or years but by
hospital visits. Red dots were employed to indicate platelet counts, with color deployed
mimetically to convey the intensity of the bruises, as well as the visuality of the data
recorded by King. Lupi also employed color to represent King’s record of her feelings,
with black corresponding to stress and fear and yellow to signify hope. King’s fear
and hope were also visualized by hand-­drawn lines that reflected each on a scale of
one to ten. The result is rendered as an animation that unfolds over time and is set to
Show Your Work                                                                          199



music, a visually and aurally affecting composition of the affective labor of mothering
and care.
   Of course, neither Lupi and King nor the Atlas of Caregiving project team are the
first to want to identify and make visible the work of care. As early as 1969, shortly after
the birth of her own child, artist Mierle Laderman Ukeles penned the Manifesto for
Maintenance Art, which called on the art world to elevate the care and maintenance
of human life to an art, over and above the solitary creative (male) genius.65 In the
years that followed, care work would become a significant topic of interest for feminist
scholars—­especially after the mid-­1990s, when Nancy Folbre formalized the term. Fol-
bre’s primary model of care work was the everyday work of caring for a child, although
care work, like housework, isn’t necessarily performed for free. It can also include the
underwaged work performed by daycare workers or home health aids, as well as the
waged work of doctors, nurses, physical therapists, mental health professionals, and so
on. What binds these forms of work together across economic lines is their motivation.
As theorized by Folbre, care work is undertaken out of a sense of compassion or respon-
sibility for others, rather than with a goal of monetary gain. But when it comes to the
market, altruism is a double-­edged sword. These same professional care workers—­who
are predominantly women and people of color—­are often paid less than they would be
in other fields.66 Why? Because they care.
   So how do we “show the work” of care workers? How do we ensure that this work
is sufficiently recognized and valued? And can we do anything more to challenge the
root cause of this undervalued work? In the academy, groups like the Maintainers have
sought to learn from the theories of care developed by feminist labor studies scholars
such as Folbre as they attempt to make visible and value the labor of data work.67
Through workshops, conferences, and publications, the Maintainers seek to counter
the current tendency in technology fields to celebrate innovation and discovery alone.
The work that maintains and sustains the world we live in today should also be cel-
ebrated, they insist. Among their current areas of research are the people they call Info-
Maintainers: the people who work in libraries and archives and in related preservation
fields to ensure that the knowledge of the present remains accessible for generations to
come. Because the work of librarians and archivists and curators is focused on facilitat-
ing access to future knowledge, the Maintainers argue, it can be viewed as a form of
care work too.
   Across many technical fields, there is an increasing amount of attention paid to
care work, and to other forms of invisible labor, now that so much work is virtual
rather than physical; as well as to issues of job insecurity now that white-­collar jobs
have begun to be outsourced to freelancers as well. In this context, it is important to
200                                                                             Chapter 7



recall that professional care workers have long dealt with issues of undercompensated
and precarious work; and for just as long, they have been involved with efforts to
resist and organize against the inequities they have faced. Today, these efforts are being
enhanced by data and technology, as unions and other advocacy groups are making use
of new platforms and data streams for their work. But they are also being obstructed,
as Uber-­style apps to connect caregivers and employers increasingly abound. These
apps do nothing to solve the systemic problems that caregivers face. A 2016 study of
on-­demand domestic worker apps by the UK’s Overseas Development Institute (ODI)
reports that because they displace risk onto workers, these platforms potentially rein-
force discrimination and “further entrenchment of unequal power relations within the
traditional domestic work sector.”68
  As a corrective, we might look to emerging prototypes that center the needs of work-
ers, those that are developed by and with workers themselves. In the US, for example,
the National Domestic Workers Alliance (NDWA) has developed an app, Alia, in order
to serve as a portable benefits platform.69 It allows clients to contribute a small amount
into the worker’s benefits account each time that worker provides them with a ser-
vice. Workers can then pool contributions from multiple clients to purchase benefits
on-­demand, such as paid time off and various forms of insurance. Caveats remain, of
course: Shouldn’t the government require that all workers receive paid time off as a
matter of course? Shouldn’t we be advocating for a single-­payer healthcare system? Yes
and yes. But while the NDWA continues to lobby for systemic change, its app offers one
way to provide essential benefits to domestic workers right now. It is a harm reduction
strategy, one that can be pursued while simultaneously advocating for more transfor-
mative change. Thinking back to Kimberly Seals Allers’s app, Irth, discussed in chapter
1, we might also begin to imagine how its successful use would contribute to a dataset
that could be used to support future advocacy efforts.


Show Your Work


Data work is part of a larger ecology of knowledge, one that must be both sustain-
able and socially just. Like the ship paths visualized on the Ship Map or the source
code stored on GitHub or the global assemblage of people and materials that make an
Amazon Echo device, the network of people who contribute to data projects is vast
and complex. Showing this work is an essential component of data feminism, and it
is the reason why “show your work” is the seventh and final principle in this book.
An emphasis on labor opens the door to the interdisciplinary area of data produc-
tion studies: taking a data visualization, model, or product and tracing it back to its
Show Your Work                                                                       201



material conditions and contexts, as well as to the quality and character of the work
and the people required to make it. This kind of careful excavation can be undertaken
in academic, journalistic, or general contexts, in all cases helping to make more clearly
visible—­and therefore to value—­the work that data science rests upon.
   We can also look to the data themselves in order to honor the range of forms of
invisible labor involved in data science. Who is credited on each project? Whose work
has been “screened out”? While one strategy is to show the work behind making data
products themselves, another strategy for honoring work of all forms is to use data
science to show the work of people (mostly women) who labor in other sectors of the
economy, those that involve emotional labor, domestic work, and care work. We see
this in action in the Atlas of Caregivers, which focuses on legitimizing care work, and
the Alia app, which provides more financial security for domestic workers. Designing
in solidarity with domestic workers can begin to challenge the structural inequalities
that relegate their work to the margins in the first place.
   This point brings us back to the ideas about power that began this book. Power
imbalances are everywhere in data science: in our datasets, in our data products, and in
the environments that enable our data work. Showing the work is crucial to ensure that
undervalued and invisible labor receives the credit it deserves, as well as to understand
the true cost and planetary consequences of data work.
Conclusion: Now Let’s Multiply




On November 1, 2018, at 11:10 a.m. local time, workers at Google offices in fifty cities
around the world closed their browser tabs, shut their laptops, and walked off their
jobs.1 The walkout included both full-­time employees and freelancers. It was women-­
led at a company that, despite years of lip-­service to inclusion, only has 31 percent
women employees.2 And it was massive—­more than twenty thousand workers par-
ticipated (figure 8.1). Why did workers at one of the most powerful companies on the
planet take to the streets?
   One week earlier, the New York Times broke a story about the $90 million exit pack-
age that Andy Rubin, the creator of Google’s Android mobile operating system, had
received after he was accused of sexual misconduct (and after an internal investiga-
tion had found the claim to be credible).3 The story mentioned two other executives
accused of sexual misconduct whom Google had similarly protected. As journalists
Daisuke Wakabayashi and Katie Benner wrote, “In settling on terms favorable to two
of the men, Google protected its own interests.” Evidently, Rubin’s package had been
paid out in installments of $2 million per month over the course of four years. His final
payment was scheduled for later that month.
   As soon as the New York Times article was published, additional stories of discrimi-
nation faced by women, as well as men and nonbinary people, began pouring out on
company email lists and chat channels and in face-­to-­face forums. The stories pointed
to patterns of toxic behavior.4 Within a week, the massive walkout—­initially floated as
an idea on a Google moms list—­had been planned. “Tech industry business as usual
is failing us,” said Meredith Whittaker, the founder of Google’s Open Research Group.
“Google paying $90M to Andy Rubin is one example among thousands, which speak
to a company where abuse of power, systemic racism, and unaccountable decision-­
making are the norm. ... It’s clear that we need real structural change, not adjustments
to the status quo.”5
204                                                          Conclusion: Now Let’s Multiply




Figure 8.1
The Sunnyvale, California, Google campus during the Google Walkout for Real Change on
November 1, 2018. Employees turned out en masse to protest the company’s handling of sexual
misconduct cases. Courtesy of Wikimedia Commons user Grendelkhan.


   A group of seven core organizers, including Whittaker, came together to craft five
concrete demands, including ending forced arbitration in cases of discrimination and
sexual harassment and promoting the chief diversity officer to report directly to the
CEO.6 When November 1 arrived, employees congregated first in indoor atriums, and
then in courtyards and on streets. They carried signs that said, “Not OK, Google,” “I
Reported, He Got Promoted,” and “Happy to Quit for $90 Million, No Sexual Harass-
ment Required.” Google management started paying attention.
   Although the Google Walkout for Real Change, as the protest was formally known,
was framed in the media as a milestone for big tech, there are clear precedents for white-­
collar tech organizing. Historian Mar Hicks has connected the Google walkout to a strike
among computer workers—­then a workforce that was comprised mainly of women—­
that took place in the United Kingdom in the 1970s. The strike took down twenty-­six
government computer centers and disrupted the work of nine others. These were the
centers that enabled the government to process its value-­added tax (VAT), and without
the computers online, the tax couldn’t be collected. The government was required to
pay attention. Writes Hicks, “Even though many of these workers were women, and
limited in their pay, promotion, and work opportunities due to sexism, their proximity
to the literal machinery of government gave them a great deal of power.”7
   The organizers of the Google walkout recognized their proximity to another source
of power: Google itself. A single worker might have limited power, but their collective
organizing drew attention to the proximity of a relatively small number of people—­
Google employees—­to the global digital infrastructure of everyday life. Part of the rea-
son that data and computation have proved to be so lucrative is their ability to scale.
Conclusion: Now Let’s Multiply                                                          205



As journalist Moira Weigel points out, “This kind of scale means these companies can
make extraordinarily high profits. But it also means the core workers they rely on have
an extraordinary amount of bargaining power.”8 They also have messaging power,
interruption power, and subversion power.
   How might tech workers marshal these strengths to mass-­occupy digital infrastruc-
ture? To teach algorithms to “work to rule” in the style of assembly-­line slow-­downs?
To slow the flow of everyday capitalism to gather attention? To channel digital solidari-
ties back into physical spaces and human relationships?
   There are already many examples that point to how these questions might begin
to be answered. In an article about tech organizing in the magazine n+1, for instance,
an anonymous software developer points out, “If the developers from Slack decided to
strike, they could, without too much difficulty, push out a change that made it so that
any message that got sent would push a message about the purpose of the strike” to its
ten million daily users.9 And just for a minute, imagine if they did.
   Although Slack developers haven’t hacked their own platform (yet), collective orga-
nizing around data and technology has already taken a range of powerful forms. Groups
like the Tech Workers Coalition are building bridges between the programmers who
code the search engines and the cafeteria workers who prepare their food. They have
also helped popularize the hashtag #TechWontBuildIt to indicate a collective refusal
to work on ethically compromised software.10 Platforms like Coworker.org are helping
gig-­economy workers, like Uber drivers, get organized. Other organizations, such as
Tech Solidarity, are focusing on electoral politics. Some projects are taking explicitly
political stands; the Lerna JavaScript library briefly added a clause to its license prohib-
iting entities that collaborate with US Immigration and Customs Enforcement (ICE)
from using it.11 Individuals are forming worker-­owned tech cooperatives in the United
States and around the globe and drafting values statements, such as the Design Action
Collective’s Points of Unity, that guide their work together and help them decide which
projects to take on.12 Other collective organizing efforts are working to draft codes of
ethics like the Toronto Declaration13 and statements of values like those guiding the
Canadian government’s action plan for open government.14 Note that these efforts are
not limited to white-­collar workers, nor to employees of the big five technology com-
panies, nor to large-­scale events.
   Some groups are using movement-­building strategies to effect change across entire
industries. For example, Una Lee, Wesley Taylor, Victoria Barnett, Ebony Dumas, Car-
los (L05) Garcia, and Sasha Costanza-­Chock are coordinating a networked community
of practice called design justice.15 The idea for design justice emerged from a workshop
at the Allied Media Conference in Detroit in 2015, where thirty people assembled to
206                                                          Conclusion: Now Let’s Multiply



challenge the idea of “design for good.” As co-­organizer Una Lee put it, “How could
we redesign design so that those who are normally marginalized by it, those who are
characterized as passive beneficiaries of design thinking, become co-­creators of solu-
tions, of futures?”16
   Since then, the design justice group has produced dozens of workshops, pop-­up
educational forums, and scholarly texts. One of its central projects is a set of ten Design
Justice Network Principles, which guide designers in navigating inequality and achiev-
ing justice through design.17 Principle 1, for example, reads: “We use design to sustain,
heal, and empower our communities, as well as to seek liberation from exploitative and
oppressive systems.” Principle 5 reads: “We see the role of the designer as a facilitator
rather than an expert.” The Design Justice Network promotes these principles through
its workshops and other events at which designers meet, discuss, and co-­conspire. To
date, more than 350 additional designers have signed on.
   Data for Black Lives (D4BL) is another example of inspired organizing and move-
ment building at a national scale. Founded by veteran organizer Yeshimabeit Milner,
who was herself trained by Black Lives Matter organizers, D4BL is “a network [of] over
4,000 scientists and activists working to harness the power of data and technology to
make real change in the lives of Black people.”18 D4BL organizes annual conferences,
runs online communities, and helps connect people in its network. The group pursues
two simultaneous strategies: pushing back against the harmful impacts of data as they
are currently deployed, and creating new spaces for organizers, data scientists, and
engineers to come together to generate meaningful research questions. The group’s
emphasis on abolition and liberation, rather than a generic form of social good, leads it
to design projects that actively work to overturn the data-­driven discrimination experi-
enced in Black communities. Milner’s vision is “to make data a tool for profound social
change instead of a weapon of oppression.”19
   The vision of D4BL will take time to realize, as is true of all visions that motivate
transformative work. The organizers of the Google Walkout for Real Change are discov-
ering this as we write. When they first assembled, they envisioned a world in which
executives would listen to the demands of their workers and would undertake immedi-
ate measures for change. Although Google publicly expressed support for the workers
involved, and the CEO issued a memo that read, “We are taking in all their feedback so
we can turn these ideas into action,” that action has yet to transpire. Claire Stapleton,
the woman who originally floated the idea of taking mass action, stated: “We’re almost
three months out from the walkout and exactly zero of the five demands have been
met.” The corporation did end forced arbitration—­and it led to other tech companies
Conclusion: Now Let’s Multiply                                                         207



doing the same—­but it was only a partial win because it covered cases of sexual miscon-
duct alone, not all discrimination cases. As Amr Gaber, another key organizer, added,
“it’s also the cheapest thing, the most minor thing they could’ve done.”20
   These paltry actions, clearly motivated by the bottom line, underscore the unyield-
ing influence of profit and power and the need for a feminism that is intersectional as
a matter of course. In this book, we have described intersectional feminism—­a vibrant
body of knowledge and action that challenges the unequal distribution of power—­and
how it can be applied to the field of data science today. In doing this work, we have
drawn heavily from the work of Black feminist theorists and activists, to reflect both
their central role in defining and elaborating intersectionality and our own position as
white scholars and white women in the United States. Here, we want to reiterate our
appreciation for this foundational work, as well as to once again acknowledge that we
cannot speak directly from the life experiences that motivate it. We hope that you, our
readers, will use this work in order to reflect on your own identities, as well as to exam-
ine how power and privilege operate in data science and in the world..
   As we write this conclusion, in July 2019, issues of power and privilege continue to
loom large. The other four and a half demands issued by the organizers of the Google
walkout included “a commitment to end pay and opportunity inequity” at all levels of
the corporation and the collection of “transparent data on the gender, race and ethnic-
ity compensation gap, across both level and years of experience,” as well as access to
extant sexual misconduct reporting mechanisms by all Google employees, including
its contract workers (who make up around half of the company’s total employees).21
Yet the public memo stated blandly that Google would continue to work on “creating
a more inclusive culture for everyone.”22 Meanwhile, Google’s lawyers have been filing
legal documents that urge the US National Labor Relations Board to overturn the 2014
ruling that allows workers to use company email to organize without fear of retalia-
tion.23 If the ruling were overturned, it would seriously impede any efforts to organize
future actions at Google, or at any large corporation, because company email lists are
the primary way that a distributed workforce can organize across office locations and
time zones.
   Further complicating future organizing efforts, numerous Google employees, includ-
ing lead organizers Whittaker and Stapleton, have faced retaliation and even demotion
in the months following the walkout. These internal actions have been documented
by Wired magazine, Bloomberg News, and the tech news site Packt, among other news
outlets. For example, Stapleton was told to go on medical leave even though she was
not sick, and the decision was only reversed after she hired a lawyer; and Whittaker
208                                                          Conclusion: Now Let’s Multiply



was told that she would be required to “abandon her work” with the AI Now Insti-
tute, an independent research group focused on issues of AI and ethics.24 Stapleton left
Google in June 2019 and Whittaker left in July of that year, two high-­profile depar-
tures that the Guardian surmised would “have a chilling effect” on tech workplace
activism.25
   This is the deployment of the structural and disciplinary domains of the matrix of
domination, which we introduced in chapter 1. Google’s legal team is well-­resourced
and has the power to shape both federal laws and company policies. This confirms
the need to monitor dominant groups and institutions that wield outsized power in
the world (and tend to use it to secure their positions). It also affirms the need to col-
laborate with the groups most impacted by differentials of power. In chapter 2, and
throughout this book, we have attempted to heed our own advice, featuring the voices
and ideas of those with direct experience of injustice. In so doing, we have sought to
feature the sites of energy that have inspired us in our work—­ranging from new activ-
ist networks to data journalism startups, from librarians authoring data user guides to
engineers interrogating human-­reporting bias. We’ve drawn from the work of sociolo-
gists who theorize digital power, artists who challenge technological neutrality, educa-
tors who teach statistics in real-­world settings, and individuals who are single-­handedly
compiling spreadsheets of missing data. It is from all these locations, using all these
methods, and including all these people—­and more—­that we can challenge the matrix
of domination in data science at its source.
   As should now be clear, our definition of data science includes more than quantita-
tive methods, more than “big” data, more than “artificial” intelligence, and more than
“neutral” displays. We explored the limitations of such a narrow view of data science
and its communication in chapter 3. There and throughout the book, we have argued
that an expansive conception of data science is essential if we are to work toward our
goal of remaking the world.
   Enabling this feminist data science to flourish and thrive will require deliberate
interventions in each phase of data work, and in our received ideas about the people
and communities who perform it. In chapter 4, we showed how the decisions that
are made when first collecting data go on to impact future results. In chapter 5, we
debunked the myth that data science is a solo enterprise, undertaken by genius wizards
working alone. Data science involves collaboration and community, as well as deep
context, as we discussed in chapter 6. Equally important is the acknowledgment, as
explored in chapter 7, that data science is the work of many hands.
                                                                                                                                             Conclusion: Now Let’s Multiply




Figure 8.2
Reported Internally Displaced People, a 2016 map of internally displaced people in Colombia from 1985 to 2015. From the project Conflict
                                                                                                                                             209




Urbanism: Colombia by the Center for Spatial Research at Columbia University, which looked at land-­use patterns and displacement in
Colombia over thirty years of armed internal conflict. The researchers worked with the organization Unidad para la Atención y Repara-
ción Integral a las Víctimas, a massive data collection effort that documented millions of individuals. Courtesy of the Center for Spatial
Research, Columbia University.
210                                                             Conclusion: Now Let’s Multiply




Figure 8.3
Detail of a model card, from a 2019 paper titled “Model Cards for Model Reporting” by AI
researcher Margaret Mitchell and coauthors that proposes short documents called model cards that
would accompany machine learning models as a form of documentation. Model cards detail who
developed the model, for what purpose, and how the model performs, including intersectional
identity metrics. Model cards would also specify known limitations of a model and use cases for
which the model is not suitable. Courtesy of Margaret Mitchell.
Conclusion: Now Let’s Multiply                                                               211




Figure 8.4
Feminindex is a civic media project that documents and visualizes where all Argentine political
candidates stand on gender and LGBTQ+ issues, including reproductive rights, femicides, and
trans rights. The first version was released in 2017 and the second in 2019. Courtesy of Economía
Femini(s)ta, including Mercedes D’Alessandro, Andrés Snitcofsky, Lina Castellanos, Aldana Vales,
and the Economía Femini(s)ta team. See http://economiafeminita.com/activismo/feminindex/.
Figure 8.5
Decoding Possibilities (2017) by Ron Morrison and Treva Ellison, is an artistic examination of
redlining’s effects in the landscape as well as a celebration of creative resistance to redlining. (a)
Contemporary maps of Boston are combined with historic redlining maps, as well as maps created
from the Combahee River Collective’s writings. (b) The installation includes quotes on the endur-
ing effects of redlining in the landscape. Courtesy of Ron Morrison and Treva Ellison.
Conclusion: Now Let’s Multiply                                                         213



   Throughout the book, we have described our seven principles of data feminism:
examine power, challenge power, elevate emotion and esmbodiment, rethink binaries and
hierarchies, embrace pluralism, consider context, and make labor visible. We derived these
principles from the major ideas that have emerged in the past several decades of inter-
sectional feminist activism and critical thought. At the same time, we welcome the
notion that there are many other possible starting points that share the end goal of
using data (or refusing data) in order to end oppression.26
   Those other starting points might come from within the academy. For example,
the work of the Center for Spatial Research at Colombia, led by Laura Kurgan, uses a
uniquely transdisciplinary approach that includes data science and AI, the humani-
ties, geography, and design to investigate complicated phenomena like urban/rural dis-
placement due to conflict (figure 8.2). Scholars like Dean Spade are using queer theory
to challenge the institutions that wield data. And media studies scholars are examining
the intersections of race, gender, sexuality, and data, as Shaka McGlotten does through
their Black data project.27 Researchers are writing books about Indigenous statistics
and Indigenous data sovereignty,28 developing decolonial design methods,29 and lead-
ing dynamic conversations about decolonizing data in both the Global North and the
Global South.30 Computer scientists and AI researchers are conducting important stud-
ies on bias, as well as developing new ways to promote transparent and responsible use
of AI. For example, Margaret Mitchell and her coauthors have recently proposed model
cards (figure 8.3), a form of documentation that would accompany machine learning
models to detail their intended uses and their technical and ethical limitations.31
   There are many other possible starting points for challenging oppression in data
          ­ ut of the arts, activism, community organizing, and consciousness-­raising.
that come o
Cartographer Margaret Pearce’s next mapping project indigenizes the Mississippi River
map to make new spaces for public dialogue about flood management. Mimi Onu-
oha and Mother Cyborg’s People’s Guide to AI, designed for newcomers, provides an
accessible introduction to the ideas behind artificial intelligence. The activist group
Economía Femini(s)ta in Argentina has an ongoing civic accountability project called
Feminindex in which the group visualizes where each candidate stands in relation to
a range of gender and LGBTQ+ issues (figure 8.4). The group has even produced digital
trading cards for politicians, which it circulates on social media. And artist-­researchers
Ron Morrison and Treva Ellison disrupt Boston redlining maps from 1935 with over-
lays of “black queer, trans, and feminist geographies” created by the Combahee River
Collective (figure 8.5). These require viewers to put on special glasses called Racialized
Space Reduction Lenses (RSRL) to see beneath the surface.32
214                                                       Conclusion: Now Let’s Multiply



  These projects are not intended to be exhaustive, and the list could go on. What
is most important is not that we all share the same starting point, but rather that we
nurture all of these emerging ecosystems and build links between them. We will need
all of them for mobilizing resistance to the differentials of power embedded in our
current datasets and data systems. And we will also need them for mobilizing cour-
age and creativity—­to imagine what data science and artificial intelligence beyond the
matrix of domination might look like. The best time for resistance and reimagination
is before the norms and structures and regulations of the data economy have been fully
determined.
  So now let’s multiply. Let’s multiply now.
Our Values and Our Metrics for Holding Ourselves Accountable




A note to readers: We created this document as we began the writing of this book and
included it as part of the manuscript draft that was posted online as part of the open
peer review process. We were prompted to write it because of work on a prior project—­
the Make the Breast Pump Not Suck Hackathon—­with equity consultant Jenn Roberts
of Versed Education, and because of the values statements (and related statements of
principles) published by groups such as the University of Maryland’s African American
History, Culture, and Digital Humanities Initiative (AADHum) and the University of
Delaware’s Colored Conventions Project.1 From these projects, we saw how statements
of shared values can become important orientation points, guiding internal decisions
at challenging junctures and making ethical commitments public and transparent. The
idea to accompany our values with a set of metrics was also proposed by Jenn Roberts
for the breast pump hackathon. We discuss that project, and the uses and limits of met-
rics for accountability, in chapter 4. The metrics below were calculated two times: first
on the basis of the draft posted online, and second on the basis of the copyedited book
manuscript. Aside from the addition of the second set of metrics, and a short reflection
on our successes and failures, the language of the document remains unchanged from
the version posted as part of the manuscript draft.


We Insist on Intersectionality


Feminism has always been multivocal and multiracial, but the movements’ diverse
voices have not always been valued equally. The women’s suffrage movement largely
excluded Black women and the abolition of slavery from its agenda. In the 1970s,
lesbian feminists were called “the purple menace” by straight feminists. But feminism
fails altogether if it is only for elite, white, straight, Christian, Anglo women. The work
of activists and scholars, particularly Black feminists, over the past forty years insists on
216                             Our Values and Our Metrics for Holding Ourselves Accountable



a feminism that is intersectional, meaning it looks at issues of social power related not
just to gender, but also to race, class, ability, sexuality, immigrant status, and more. It
does so, moreover, by looking to collectives as well as individuals, to structural issues as
well as specific instances of injustice.


We Advocate for Equity


Equity is both an outcome and a process. Future justice must account for an unjust past
in which some groups’ knowledges have been valued and others have been subjugated,
as Patricia Hill Collins teaches us. In the process of achieving equity, those of us in
positions of relative power must learn to listen deeper and listen differently—­with the
ultimate goal of taking action against the status quo that benefits us at the expense of
others. For this reason, we listen and give priority in the text to voices who speak from
marginalized perspectives, whether because of their gender, ability, race, class, colonial
status, or other aspects of their identity.


We Prioritize Proximity


As Kimberly Seals Allers, women’s health advocate, says, “Whatever the question, the
answer is in the community.” People in a community know its problems intimately,
and they know which phenomena go uncounted, underreported, or neglected by insti-
tutions in power (or, conversely, who is overly surveilled by institutions in power).
They also know what interventions will work to solve those problems. In this book, we
try to prioritize voices with closer and more direct experience of issues of injustice over
those that study a data injustice from a distance.


We Acknowledge the Humanity of Data


We recognize that the transformation of human experience into data often entails a
reduction in complexity and context. We further acknowledge that there is a long his-
tory of data being “all too often wielded as an instrument of oppression, reinforcing
inequality and perpetuating injustice,” as the group Data for Black Lives explains. We
keep these inherent constraints in mind as we write, attempting to introduce context
and complexity whenever possible, and acknowledge the limits of the methods we
discuss, as well as their strengths.
Our Values and Our Metrics for Holding Ourselves Accountable                                     217



We Are Reflexive, Transparent, and Accountable


Acknowledging that our knowledge is shaped by our own perspectives and limita-
tions (see more in the About Us section ahead), we strive to be reflexive, transpar-
ent, and accountable for our work. We are on a journey toward justice, and that
inevitably involves making mistakes. We are grateful to those who have shown us
generosity in letting us learn up to this point. And we respectfully say to our future
teachers that you will find in us open listeners: we recognize direct and critical words
as a generous offer and a vote of confidence in our ability to hear and be transformed
by you.
   To that end, we have an evolving table of explicit metrics (table A.1) that will guide
us in auditing our citations and the examples that we elevate in the book. We note,
here, that our foregrounding of race and racism reflects our location in the United
States, where the most entrenched issues of inequality and injustice have racism at
their source.


About Us


Feminist standpoint theory recognizes the value of situated knowledge—­acknowledging
the perspectives and experiences of the knower and how those have shaped the knowl-
edge they produce. Accordingly, we situate ourselves and the learning contexts in
which we work.

Catherine D’Ignazio is an assistant professor at Massachusetts Institute of Technology, a private
research university in Cambridge, MA. Before moving to MIT, she worked at Emerson College, a
private college in Boston focused on communications and the arts. From a middle-­class, Italian-­
American and Scotch-­Irish background, she grew up primarily in the American South, with some
formative years spent in Latin America and Europe. She is a mother, an experience that has
sharpened her understanding of how women’s bodies are stigmatized and underserved by main-
stream institutions. Working mainly in urban New England, she experiences significant privilege
from her whiteness, ability, institutional affiliation, and education, among other things, and
experiences some oppression based on her gender. With decades of professional work in software
programming, art/design, and digital media education, she comes to data feminism based on a
commitment to democratize information and include more people and professions in contempo-
rary conversations about data and power.

Lauren F. Klein is an associate professor at Emory University, a private university in Atlanta, Geor-
gia, in the Southern United States. Before moving to Emory, she worked at Georgia Tech, a large
public research university also in Atlanta. From a middle-­class New York Jewish family, she grew
218                                 Our Values and Our Metrics for Holding Ourselves Accountable



Table 1
Aspirational, draft, and final metrics and the structural problems they address

                       Aspirational metrics to                                Final metrics
Structural             live our values for              Draft metrics         (copyedited
problem                this book                        (open peer review)    manuscript)

Racism                 •    5 percent of citations
                           7                            Scholarship: 36       Scholarship: 32
                           of feminist scholarship      percent from          percent from
                           from people of color         people of color       people of color
                       •   75 percent of examples      Projects: 49          Projects: 42
                            of feminist data projects   percent led by        percent led by
                            discussed led by people     people of color       people of color
                            of color
Patriarchy             •   7
                            5 percent of all           67 percent of         62 percent of
                           citations and examples       citations and         citations and
                           from women and               examples from         examples from
                           nonbinary people             women and             women and
                                                        nonbinary people      nonbinary people
Cissexism              •    enter trans perspectives
                           C                            Three of ten          Nine of nine
                           in discussions of the        chapters feature      chapters feature
                           gender binary                transgender           transgender
                       •   Use transinclusive          example and/or        example and/or
                            language throughout         theorist              theorist
                            the book
                       •    Example or theorist in
                             every chapter from a
                             transgender perspective
Heteronormativity      •   Resist assumptions          Ten of ten chapters   Nine of nine
                            about family structure      feature communal      of ten chapters
                            and gender roles            example and/or        feature communal
                       •    Example or theorist        theorist              example and/or
                             in every chapter that                            theorist
                             illustrates the power of
                             communal (vs. family)
                             support networks
Ableism                •   Challenge the               Nine of ten           TK of ten chapters
                            dominance of                chapters feature      feature nonvisual
                            visualization in the        nonvisual example     example and/or
                            presentation of data        and/or theorist       theorist
                       •    Example or theorist
                             in every chapter that
                             employs nonvisual
                             methods of presenting
                             data
Our Values and Our Metrics for Holding Ourselves Accountable                                    219



Table 1 (continued)

                       Aspirational metrics to                                Final metrics
Structural             live our values for            Draft metrics           (copyedited
problem                this book                      (open peer review)      manuscript)

Colonialism            •   3
                            0 percent of projects    Projects: 8.5           Projects: 7 percent
                           discussed come from the    percent from the        from the Global
                           Global South               Global South            South
                       •   Example or theorist in    Five of ten             Seven of nine
                            every chapter about       chapters feature        chapters feature
                            Indigenous knowledges     indigenous              indigenous
                            and/or activism           example and/or          example and/or
                                                      theorist                theorist
Classism               •    cknowledge that data
                           A                          Projects: 88            Projects: 78
                           science, as a field, is    percent from            percent from
                           premised on economic,      outside academy         outside academy
                           educational, and           Ten of ten              Nine of nine
                           technological privilege    chapters feature        chapters feature
                       •   50 percent of feminist    nonacademic             nonacademic
                            projects discussed        example and/or          example and/or
                            come from outside the     theorist                theorist
                            academy
                       •    Example or theorist
                             in every chapter that
                             demonstrates how the
                             ideas can be applied
                             without expensive
                             technology and/or
                             formal training
Proximity              •    0 percent of feminist
                           5                          Projects: 49            Projects: 34
                           projects discussed         percent feature         percent feature
                           feature and quote          people directly         people directly
                           people directly impacted   impacted                impacted
                           by an issue (vs. those
                           who study or report on
                           the phenomena from a
                           distance)



up in suburban New Jersey and lived in New York for much of her adult life, with some time spent
in Boston. Like D’Ignazio, she is also a mother. Working in the US South, she experiences signifi-
cant privilege from her whiteness, ability, education, and institutional affiliation, among other
things, and experiences some oppression based on her gender. She worked in web development
before becoming an academic and comes to data feminism through her desire to convert theory
into practice and to create more opportunities for humanities research (and researchers) to enter
into conversation with communities, activists, organizers, and others working toward justice.
220                           Our Values and Our Metrics for Holding Ourselves Accountable




1967 Detroit Rebellion, 1                            “Anatomy of an AI System,” 185
                                                     data collection and, 12
AADHum (University of Maryland’s African             invisible labor and, 200
    American History, Culture, and Digital           legible urban spaces and, 133
    Humanities Initiative), 215                      Mechanical Turk (MTurk) in, 180–­185
Ableism 5, 8, 218                                    Rekognition and, 64
Accessibility, 85, 88 115 137 152, 155, 198,        APIs (application programming interface),
    213                                                133, 152–­153, 270n17, 284n5, 288n34
Accountability, 12, 23, 39, 42, 47, 53–­65, 72,     Apollo 1 disaster, 1
    82–­83, 97, 102, 114–­123, 137, 153–­155,       Apollo 11 mission, computer systems and, 1,
    159, 173, 200–­203, 213–­224                       11
Administrative violence, 108, 272n33                Artificial Intelligence (AI)
AEMP (Anti-­Eviction Mapping Project, the),          diversity crisis and, 15, 27–­30, 103,
    125–­129, 136, 148, 231f                           162
AI4Good Foundation, 141                              drone strike accuracy and 64 (see also
AirBeat, 34                                            Google, Project Maven and)
Algorithms                                           expansive definition of, 208, 214
 accountability beat and, 39, 47, 53–­65, 72         People’s Guide to AI, A, and, 213
 Algorithmic Justice League, 30–­32                  subjugated knowledge and, 163
 bias in, 13, 28–­32, 39, 47, 52, 56–­57, 60, 62,   Atlas of Caregiving, 193–­199
    72, 163                                         Augmented reality, 64
 face recognition and, 29–­32, 52–­56, 65, 106,
    118, 180                                        Bias
 reverse-­engineering of, 13, 47                     bail and, 53
 risk assessment and, 13, 39, 54–­59, 109, 118       birth justice and (see Seals Allers, Kimberly)
Alia (app), 200–­201                                 discrimination in, 29, 63, 123, 162–­166
Allied Media Conference, 205                         encoded in, 16, 28, 34, 123, 213
Amazon                                               human reporting in, 16, 208, 227
 Algorithms of Oppression and, 28                    privacy and, 111
308                                                                                   Subject Index



Bias (cont.)                                        Coming Home to Indigenous Place Names in
 privilege hazard and, 28–­33, 47, 57, 61–­63,        Canada, 92–­96
    221                                             COMPAS (Correctional Offender Management
 racial capitalism and, 52–­62                        Profiling for Alternative Sanctions), 54–­55.
 scarcity and, 40                                     See also Bias, bail and
Big Dick Data, 151–­152, 284n4                      Computational categories, 103–­104
Binaries, rethinking of, 18, 77, 97, 105,           Computational fluid dynamics, 5
    111–­113, 122–­123, 145, 184                    Confidence intervals, 88, 267n44
Birth justice, 5, 21–­23, 28, 34, 45–­46, 97–­98,   Contextual analysis, 118, 163, 172
    109–­111, 115, 121, 199, 223. See also          Counterdata, collection of, 34–­39, 52–­53, 59,
    Medicine, maternal health in                      72, 130
Black Mamas Matter Alliance, 22                     Counting, healing and, 120
Black Mothers Breastfeeding Association, 62         Coworker.org, 205
Boolean variables. See Computational                Cuidando do Meu Bairro (Caring for My
    categories                                        Neighborhood), 168
Box plotting, 88
British Petroleum oil spill of 2010, 137, 183       D4BL (Data for Black Lives), 206
Brogrammers, 61                                     Data2X, 34, 59, 256n32
                                                    Data-­driven clothing, 87
Campus sexual violence, data setting and, 159,      Data for Good conferences, 140, 206
   170                                              Data-­ink ratio, 75–­76, 96
Capitalism, 52, 183–­198, 205 253n6, 263n6.         DataKind, 140, 257n37
   See also Global capitalism                       Data science
CCP (Colored Conventions Project), 118–­119,         bias and, 13–­15, 28–­34, 39–­40, 46–­47, 52,
   215                                                 56–­57, 60–­63, 72, 82–­83, 11, 123, 163–­166,
Center for Spatial Research at Colombia, 209,          208, 213
   213                                               biometrics and, 12, 193, 240n29
Choropleth maps, 68                                  cleaning and, 64, 130–­135
Cissexism, 5, 8, 28, 36–­37, 62, 106–­115, 173,      collection demographics and, 12, 144, 276
   185, 211, 218, 220, 224                           commodification in, 12, 42, 45, 142, 144
Citizen Potawatomi Nation, 92                        computational categories and, 103–­104
Classification systems, oppression and, 15–­18,      context in, 45, 91, 95–­97, 109–­111,115, 119,
   97, 100–­106, 111, 118, 123                         131–­132, 137, 149–­172, 184, 199–­201, 208,
Classism, 8, 26, 40, 219                               213, 216–­217, 237–­238, 249n56
CloudWalk, 31–­32                                    datafication and, 12–­13
CLUI (Center for Land Use Interpretation),           ethics and, 12, 60–­61, 66, 91, 96, 153, 164,
   42–­43                                              205, 208
Coding, as invisible labor, 112, 163, 175            eugenics and (see Eugenics)
Co-­liberation 5, 9, 53, 59–­65, 72, 137,            journalism in (see Journalism, data science
   140–­146                                            in)
Collaborators’ Bill of Rights, 189, 296n49           knowledge infrastructure in, 140–­142,
Colonialism, 8, 156, 219, 263n6, 292n11                153–­155, 163, 185
Combahee River Collective Statement, 4               novel presentation in, 88
Subject Index                                                                                   309



 perceived neutrality and, 18, 39, 45, 53, 55,    Equity justice, 5, 62–­67, 72, 216
   60 75–­82, 149, 155, 159–­165                  Equivant, 53–­56
 privacy and (see Privacy)                        Ethics, as co-­equal to reason, 96
 as reductive, 77, 122                            Eugenics, 12, 131, 278n19
 sampling in, 39, 65, 155–­156, 169               Eviction Defense Collaborative, 277
 setting and, 72, 131–­132, 159, 170–­172
 social good and, 29, 47, 67, 70, 140–­142,167    Facebook
 supply chain in, 178                               data commodification and, 12
 user guides and, 169–­170, 205, 208                diversity crisis and, 27
 visceralization in (see Visceralization)           energy costs and, 42
 visualization in (see Visualization)               gender identification and, 98–­102
 zombie data in, 165                                real name policy and, 115–­118
Deep mapping, 85                                    Serena Williams and (see Williams, Serena)
Deficit narratives, 58–­59, 145                     Wages for Facebook in, 179–­182
DeGraffenreid v. General Motors, 7                Facial analysis software, face harvesting and,
Delta Analytics, 140–­141                             29–­32, 65
Design                                            Federal Home Loan Bank Board, 50–­51
 Design Action Collective’s Points of Unity       Femicides, 35–­38, 53, 56 211
   in 205                                         Feminindex, 211–­213
 ethics in, 12, 60–­72, 91, 96, 106, 111, 118,    Feminism
   136, 140, 167, 205–­206                          collective effort and, 4–­7, 125, 130, 213
Design justice, 108, 140, 205–­206                  co-­liberation and (see Co-­liberation)
DGEI (Detroit Geographic Expedition and             emotion and, 6, 18, 24, 73, 76–­77, 82, 85,
   Institute), 49–­50, 65, 252n1, 254n12              95–­97, 105, 192, 207, 213, 262n71, 263n6,
DiF (Diversity in Faces). See IBM, Diversity in       264n35, 268n58
   Faces and                                        intersectionality and, 4–­8, 14, 17, 30, 46, 61,
Digital Defense Playbook, 64                          65, 111, 121–­123, 137, 140, 207, 210,
Digital Democracy, 144, 281n46                        215–­216, 224, 236n4, 258n47
DoD (U.S. Department of Defense), 64                objectivity in, 76, 82–­83
“Do good with data.” See Periscopic                 qualitative data and 13, 98, 241n42,
                                                      276n64
Economía Femini(s)ta, 211–­213, 234               First Nations peoples, 92–­93
EJ Atlas (Global Atlas of Environmental           FiveThirtyEight. See GDELT
   Justice), 145–­148, 283n59                     Floating-­point math. See Apollo 11 mission,
Elevator Repair Service (theater troupe), 85          computer systems and
Embodiment, leveraging of, 77, 88
Emotion, 6, 18, 24. 73, 76–­77, 82, 85, 95–­97,   GDELT (Global Database of Events, Language
   105, 192, 207, 213, 262n71, 263n6,                and Tone), 149–­153, 162, 170, 232f
   264n35, 268n58                                 Gender
ENIAC computer, 65                                 bias in, 15, 28, 34, 47, 56, 62, 83, 111,
Environmental justice, 34, 145–­146, 250n64,         123, 162, 213, 221, 245n25, 251n77,
   261n65, 280n38, 283n62                            264n23–­24
Epistemic violence, 133, 147                       crash test dummies and, 34
310                                                                                    Subject Index



Gender (cont.)                                     IBM
 expression in, 87, 96                               704 in, 2
 gender fluidity 5, 8, 28, 36–­37, 62, 106–­115,     Apollo 11 and, 11
   173, 185, 211, 218, 220, 224                      Diversity in Faces and, 31
 mosaic genetics and, 114                          Identity
 real names policy and, 92, 111, 115–­118            citational metrics and, 220, 224, 258n47
GitHub                                               disciplinary domain and, 25
 collaboration and, 173                              equity and, 216, 236n10
 gender demographics and, 180                        gender expression and, 5, 8, 28, 36–­37, 62,
 inclusivity and, 173                                  106–­115, 173, 185, 211, 218, 220, 224,
 invisible labor and, 189, 200                         246n32
Global capitalism                                    geography and, 92
 data production and, 183–­184                       intersectionality and, 122, 210f8.3
 slavery and, 118, 183, 215, 237n13, 240n36,         multiracial people and, 104
   285n17, 292n10                                    positionalities and, 5–­7, 24, 52, 121, 136,
 toxic “e-­waste” sites and, 184                       207, 216, 220, 224
 turking and, 180–­182, 293n24                       privilege hazard and, 28
Global IT sector, energy use in, 42                  proximity and, 52–­53, 70
God trick, 76–­83, 91–­96                            public information and, 47
Google                                               transparency and, 136, 152
 data collection and, 12                           Image-­processing algorithms, 11, 65, 160
 data sets and, 152                                Infomediaries, 171
 diversity crisis and, 27, 42                      Information design, 28, 42, 67, 73–­77, 87, 115,
 encoded bias and, 28–­31, 134                         128, 132, 145, 178, 189, 217
 Google Brain and, 141                             Information Is Beautiful award, 75
 Google buses and, 126–­128                        Instagram
 Google Flu Trends, 160–­161                         proprietary data delivery and, 160
 Open Research Group and, 203                        sharing as uncompensated labor in, 179
 Project Maven and, 42                               Williams and (see Williams, Serena)
 Rubin and (see Rubin, Andy)                       Intelligent technologies, 30, 161
 search algorithms and, 156                        Inter-­American Human Rights Court, 35
 Walkout for Real Change at, 204–­207              International Feminist Collective, 178, 291n8
 Workers Leaving the Googleplex and, 181–­182      Intersectionality, 4–­8, 14, 17, 30, 46, 61, 65,
Gradient plotting, 88–­909, 193                        111, 121–­123, 137, 140, 207, 210, 215–­216,
Greenpeace. See Global IT sector, energy use           224, 236n4, 258n47
   in, 42                                          Invisible labor, 178–­188, 192, 199–­201
Guerilla Girls, 85–­86                             Invisible Women: Exposing Data Bias in a World
                                                       Designed for Men (Criado Perez), 34
HCI (human-­computer interaction), 95,             Irth. See Birth justice
   243n21
Heteronormativity, 167, 218, 273n43, 274n46        Jeanne Clery Act, 157
HoloLens, 64                                       Journalism. See also specific journalists, journals
Human Rights Watch, 32, 299n13                        by name
Subject Index                                                                                   311



 binary reevaluation and, 113                       Matrix of domination, 18, 24–­29, 32–­35,
 computational in, 3, 57                               39–­42, 47, 61–­65, 106–­110, 146–­148, 181,
 criminal justice and, 54–­57                          208, 214, 221, 243n14, 254n7
 data access in, 137                                Medicine
 data science in, 13–­18, 22, 30, 34–­35, 39,        maternal health in, 14, 22–­23, 45–­47, 59, 98,
   78–­82, 152, 157–­159, 164, 184, 201, 208,          103, 121
   246n38                                            racism in, 22–­23, 45–­46, 59, 103, 111, 164,
 fashion and, 107, 112                                 184, 218, 252n78
 maternal health in, 22–­23, 45, 59,                 sexism in, 5, 23, 39, 46, 109–­110, 164, 167,
   98                                                  169, 193, 200
 neutrality and, 166–­171                           Metadata, 85, 153, 172, 189
 tech beat in, 41, 203–­205                         Métis people, 92–­93
 transparency and, 137, 155                         #metoo, 56
                                                    Microsoft
Kiln, 176–­177, 223, 233                             datasheets for data sets and, 169
                                                     HoloLens and, 64
Labor                                               Model cards, 210–­213
  childcare and, 4                                  Moore’s law, 11
  child exploitation and, 125, 182–­185
  collective bargaining across tech sector and,     NASA, 1–­6
    205                                             NDWA (National Domestic Workers Alliance,
  crowdsourcing and, 34–­37, 180                       the), 200
  emotional labor in, 5, 16, 183, 192–­195          Neutral visualization. See Visualization,
  human rights violations and, 184                     neutrality and
  Info-­Maintainers and, 199                        New Jim Code, 55–­59
  incarcerated persons and, 110, 126                Next System Project, 189–­190
  parental leave and, 99, 120, 207                  Nineteenth Amendment, 25, 235n4
  as productive, 4, 178, 189                        Ni Una Más (Not One More), 35
  as racialized, 38, 179, 183, 213, 224             Nonbinary persons, counting of, 97–­98,
  turking and, 180–­182, 293n24                        106–­107, 110–­114, 173, 180, 203, 218–­224,
  visibility of, 18, 24, 110, 126, 137, 173–­192,      269n4, 273n41
    199–­201, 213, 256n27                           Nuestras Hijas de Regreso a Casa (Our
Langley Research Center, 1–­10                         Daughters Back Home), 35
Legible urban spaces, 133
Library of Missing Datasets, The, 33                OCR (optical character recognition) error,
Lottery systems, 67–­71                               180
                                                    ODB (Our Data Bodies) project, 64
Make the Breast Pump Not Suck Hackathon,            ODI (Overseas Development Institute), 200
  120, 215, 275n61                                  Open data, 155
Man factory, 66–­70                                 OPP (other people’s problems), 31
Map redlining, 50–­53, 61, 76, 92, 212–­213,        Oppression, United States culture and, 5–­13,
  253n3                                               18, 23–­32, 50–­65, 72, 97–­98, 111, 123,
Markup, 59                                            156–­157, 166–­167, 181, 206, 213–­219
312                                                                                     Subject Index



Paradox of exposure, 105, 115                          epistemic violence and, 134, 167
Partial perspective, 76–­77, 83, 130, 136, 263n6       labor and, 178, 193
Patriarchy, 8, 108, 162, 167, 218, 238n18              maternal health in, 14, 22–­23, 45–­47, 59, 98,
People’s Guide to AI, A, 213                             103, 121
Periscopic, 73–­76, 84, 96, 229                        map redlining and, 92, 125
Pluralism, embrace of, 18, 125, 130, 136,              NASA and 1
    147–­148                                           predictive policing and, 13
Positionality, 7, 83–­84, 95, 136, 237n16              privilege hazard and, 31, 47, 58
Positive Voices, 110                                   prison justice and, 53, 164–­168
Power                                                  racial capitalism and, 52, 183
  challenges to, 8, 14–­19, 52–­57, 60–­65, 71–­72,    transatlantic slave trade and, 12, 25, 103,
    92, 201, 206–­208, 218                               118, 160–­161,183, 215, 237–­239
  examination of, 184, 282n54                          United States culture and, 4, 19, 33, 217
Predicative policing, 13                               United States Federal Census and, 104, 125
Predictive analytics                                   white supremacy and, 8, 26, 64
  recidivism and, 55                                  Raising Our Sisters Everywhere (ROSE), 22
  reinforcement of the status quo and, 41             Reddit, comments and, 160, 170
  Target and, 41                                      Reflexivity, 60, 64, 72, 136–­137, 217
Privacy, 40–­41, 64, 122, 135, 169, 247n46            Reported Internally Displaced People, 209
Privilege hazard, 8, 28–­33, 47, 57, 115, 118,        Reproductive justice, 236, 242
    141, 221                                          Restorative justice, 61–­62
Project Maven, 64                                     RSRL (Space Reduction Lenses), 213
Provenance rhetoric, 82
Proximity, 53, 137, 204, 216–­219                     SAFElab, 162–­164
Public Laboratory for Open Technology and             Sandy Hook shootings, 73
    Science (Public Lab), 137                         Scarcity bias, 40
                                                      Self-­advocacy, 5, 21, 96
Quantitative methodologies                            Server farms, 42
 exclusion and, 98, 142                               Sexism
 false binaries and, 145, 170, 208, 241n42              algorithms and, 29, 47, 56–­60, 156
 ground truthing and, 140, 146                          corporate culture and, 29–­31, 41, 64, 156,
 persuasion and, 70, 76                                   160, 170, 204, 233
 uncertainty and, 91                                    data history and, 9
                                                        data setting and, 159
Racism                                                  gender oppression in, 5–­8, 24, 31, 56
 accountability and, 218                                intersectionality and (see Intersectionality)
 algorithms and, 29, 57–­61, 150, 162, 203              NASA and, 1
 bad apples and, 63                                     privilege hazard and, 31, 47, 58
 binary discourse and, 111                              violence and, 1, 4, 35, 157–­159
 co-­liberation and, 5, 24                            Ship Map, 176–­178
 correlation and, 156–­157                            #SiMeMatan (If they kill me), 36
 data history and, 9                                  SisterSong, 22–­23
 discipline and, 55                                   Situated knowledge, 16, 83, 136, 152–­153, 217
Subject Index                                                                                   313



Social media. See also specific websites by        Surveillance infrastructure, 31, 39–­42, 64, 109
    name                                           Surveillance of black bodies, 239n28–­29,
  birth justice and, 22                               272n30
  classification and, 224
  context and, 160                                 Target, pregnancy prediction scores and,
  environmental justice and, 147                      40–­45
  GDELT and (see GDELT)                            Targeted violence 32
  privilege hazard and, 30–­31                     Technochauvinism, 248n38
  victim blaming and, 36                           #TechWontBuildIt 205. See also Labor,
Sonic boom minimization, 5                            collective bargaining across tech sector
Sort of Joy, A (Thousands of Exhausted Things),       and
    85–­87, 96                                     Toronto Declaration, 206
Space race, 1–­3                                   Transparency, as principle of data feminism,
Standpoint theory 75, 83, 217                         60–­64, 82, 136–­137, 154–­155
Statistics
  2016 United States election and, 88              Uncertainty, representation of, 88–­91, 128
  counterdata and, 34–­39, 52–­53, 59, 72, 85,     Unitary Plan Wind Tunnel, 2
    130                                            University of Delaware’s Colored Conventions
  data-­driven decisions and, 19                      Project, 118, 215
  deficit narratives in, 45, 58–­59, 67, 71, 145   US Bureau of Labor Statistics, 27
  data sovereignty and, 213                        US Immigration and Customs Enforcement
  equal opportunity and, 6                            (ICE), 64, 205
  equity and, 5, 62–­67, 72, 216                   Utrecht Data School, 141
  eugenics and ,131
  on-­campus crime and, 157                        Violence
  perceived neutrality and, 169–­170                on campus, 157–­159
  preservation of the status quo and, 17, 58–­59    domestic in, 4, 144, 169
  representation and, 39                            femicide, 35–­38, 53, 56
  sampling and, 155                                 gun in, 72–­75
  school segregation and, 169                       hate crime in, 1, 4, 8, 34–­35, 55, 57, 157,
  settler colonialism and, 58                         162, 166–­167, 203
  STEM and, 208                                     against trans women 32, 35, 106, 110, 144
  US Bureau of Labor and, 27                       Violin plotting 89, 229, 267n46
STEM education                                     Visceralization, 84–­96
  Courtesy of the City Digits Project Team,        Visualization. See also Visceralization
    68–­69                                          accessibility and, 85, 88 115 137 152, 198,
  domain expertise in, 121, 140                       213
  lottery and, 67–­72                               AEMP and, 125–­130, 136, 148, 23110
Stop LAPD Spying coalition, 13                      anti-­oppression and, 167, 174–­184
Structural oppression, 10, 24–­25, 29–­32,          binaries, rethinking of, 111–­115
    50–­52, 57, 60–­63, 72, 98, 111, 166–­167,      categorization in, 103
    216                                             conventions of, 73, 78, 82, 128, 189–­196,
Subjugated knowledge, 163, 172, 216                   218
314                                              Subject Index



Visualization (cont.)
 counterdata and, 72, ,189, 195, 198, 200
 emotion in, 73–­96, 192–­195, 213
 god trick and (see God trick)
 global warming and, 137
 gun violence in, 72–­75 (see also Periscopic)
 haptic recreation and, 9
 invisible labor and, 10
 neutrality and, 75–­82
 Next System Project in, 189
 rhetorical objects and, 78
 situated knowledge and, 152
Voter’s rights, 26


Wage gaps, 8, 166
Wages for Housework, 178–­180
WALT (Westside Atlanta Land Trust), 144
Wikipedia, editing culture of, 180, 227, 250,
  261
WomanStats, 169–­170


Yelp, 46
Those were the days, when we were all at sea. It seems like yesterday to me. Species, sex, race, class: in those days none of this meant anything at all. No parents, no children, just ourselves, strings of inseparable sisters, warm and wet, indistinguishable one from the other, gloriously indiscriminate, promiscuous and fused. No generations. No future, no past. An endless geographic plane of micromeshing pulsing quanta, limitless webs of interacting blendings, leakings, mergings, weaving through ourselves, running rings around each other, heedless, needless, aimless, careless, thoughdess, amok. Folds and foldings, plying and multiplying, plicatihg and replicating. We had no definition, no meaning, no way of telling each other apart. We were whatever we were up to at the time. Free exchanges, microprocesses finely tuned, polymorphous transfers without regard for borders and boundaries. There was nothing to hang on to, nothing to be grasped, nothing to protect or be protected from. Insides and outsides did not count. We gave no thought to any such things. We gave no thought to anything at all. Every-

thing was there for the taking then. We paid no attention: it was all for free. It had been this way for tens, thousands, millions, billions of what were later defined as years. If we had thought about it, we would have said it would go on forever, this fluent, fluid world.
And then something occurred to us. The climate changed. We couldn’t breathe. It grew terribly cold. Far too cold for us. Everything we touched was poisonous. Noxious gases and thin toxic airs flooded our oceanic zone. Some said we had brought it on ourselves, that all our activity had backfired, that we had destroyed our environment by an accident we had provoked. There were rumors of betrayal and sabotage, whisperings of alien invasion and mutant beings from another ship.
Only a few of us survived the break. Conditions were so terrible that many of those who did pull through wished they had died. We mutated to such an extent that we were unrecognizable to ourselves, banding together in units of a kind which, like everything, had been unthinkable before. We found ourselves working as slave components of systems whose scales and complexities we could not comprehend. Were we their parasites? Were they ours? Either way we became components of our own imprisonment. To all intents and purposes, we disappeared.
“Subtly, subtly, they become Invisible; wondrously, won- drously, they become soundless—they are thus able to be their enemies’ Fates."

Sun Tzu, The Art of War

ada
In 1833, a teenage girl met a machine which she came to regard “as a friend.” It was a futuristic device which seemed to have dropped into her world at least a century before its time.
Later to be known as Ada Lovelace, she was then Ada Byron, the only child of Annabella, a mathematician who had herself been dubbed Princess of Parallelograms by her husband. Lord Byron. The machine was the Difference Engine, a calculating system on which the engineer Charles Babbage had been working for many years. “We both went to see the thinking machine (for such it seems) last Monday,” Annabella wrote in her diary. To the amazement of its onlookers, it “raised several Nos. to the 2nd & 3rd powers, and extracted the root of a quadratic Equation.” While most of the audience gazed in astonishment at the machine, Ada “young as she was, understood its working, and saw the great beauty of the invention.”
When Babbage had begun work on the Difference Engine, he was interested in the possibility of “making machinery to compute arithmetical tables.” Although he struggled to persuade the British government to fund his work, he had no doubt about the feasibility and the value of such a machine. Isolating common mathematical differences between tabulated numbers, Babbage was convinced that this “method of differences supplied a general principle by which all tables might be computed through limited intervals, by one uniform process.” By 1822 he had made a small but functional machine, and “in the year 1833, an event of great importance in the history of the engine occurred. Mr. Babbage had directed a portion of it.

consisting of sixteen figures, to be put together. It was capable of calculating tables having two or three orders of differences; and, to some extent, of forming other tables. The action of this portion completely justified the expectations raised, and gave a most satisfactory assurance of its final success.”
Shortly after this part of his machine went on public display, Babbage was struck by the thought that the Difference Engine, still incomplete, had already superseded itself. “Having, in the meanwhile, naturally speculated upon the general principles on which machinery for calculation might be constructed, a principle of an entirely new kind occurred to him, the power of which over the most complicated arithmetical operations seemed nearly unbounded. On reexamining his drawings . . . the new principle appeared to be limited only by die extent of the mechanism it might require.” If the simplicity of the mechanisms which allowed the. Difference Engine to perform addition could be extended to thousands rather than hundreds of components, a machine could be built which would “execute more rapidly the calculations for which the Difference Engine was intended; or, that the Difference Engine would itself be superseded by a far simpler mode of construction.” The government officials who had funded Babbage’s work on the first machine were not pleased to learn that it was now to be abandoned in favor of a new set of mechanical processes which “were essentially different from those of the Difference Engine.” While Babbage did his best to persuade them that the. “fact of a new superseding an old machine, in a very few years, is one of constant occurrence in our manufactories; and instances might be pointed out in which the advance of invention has been so rapid, and the demand for machinery so great, that half-finished machines have been thrown aside as useless before their completion,” Babbage’s decision to proceed with his new

machine was also his break with the bodies which had funded his previous work. Babbage lost the support of the state, but he had already gained assistance of a very different kind.
“You are a brave man,” Ada told Babbage, “to give yourself wholly up to Fairy-Guidance!—I advise you to allow yourself to be unresistingly bewitched . . .” No one, she added, “knows what almost awful energy & power He yet undevelopped in that wiry Htde system of mine.”
In 1842 Louis Menabrea, an Italian military engineer, had deposited his Sketch of the Analytical Engine Invented by Charles Babbage in the BibHotheque Universelle de Geneve. Shortly after its appearance, Babbage later wrote, the “Countess of Lovelace informed me that she had translated the memoir of Menabrea.” Enormously impressed by this work, Babbage invited her to join him in the development of the machine.. “I asked why she had not herself written an original paper on a subject with which she was so intimately acquainted? To this Lady Lovelace rephed that the thought had not occurred to her. I then suggested that she should add some notes to Menabrea’s memoir; an idea which was immediately adopted.”
Babbage and Ada developed an intense relationship. “We discussed together the various illustrations that might be introduced,” wrote Babbage. “I suggested several, but the selection was entirely her own. So also was the algebraic working out of the different problems, except, indeed, that relating to the numbers of BernpulH, which I had offered to do to save Lady Lovelace the trouble. This she sent back to me for an amendment, having detected a grave mistake which I had made in the process.”

“A strong-minded womanl Much like her mother, eh? Wears green spectacles and writes learned books . . . She wants

to upset the universe, and play dice with the hemispheres. Women never know when to stop ..
William Gibson and Bruce Sterling, The Difference Engine
Babbage’s mathematical errors, and many of his attitudes, gready irritated Ada. While his tendency to blame other bodies for the slow progress of his work was sometimes well founded, when he insisted on prefacing the publication of the memoir and her notes with a complaint about the attitude of the British authorities to his work, Ada refused to endorse him. “I never can or will support you in acting on principles which I consider not only wrong in themselves, but suicidal.” She declared Babbage ‘‘one of the most impracticable, selfish, & intemperate persons one can have to do with,” and laid down several severe conditions for the continuation of their collaboration. “Can you,” she asked, with undisguised impatience, “undertake to give your mind wholly and undividedly, as a primary object that no engagement is to interfere with, to die consideration of all those matters in which I shall at times require your intellectual assistance & supervision; & can you promise not to slur & hurry things over; or to mislay & allow confusion & mistakes to enter into documents &c?”
Ada was, she said, “very much afraid as yet of exciting the powers I know I have over others, Sc the evidence of which I have certainly been most unwilling to admit, in fact for a long/time considered quite fanciful and absurd ... I therefore carefully refrain from all attempts intentionally to exercise unusual powers.” Perhaps this was why her work was simply attributed to A.A.L. “It is not my wish to proclaim who has written it,” she wrote. These were just a few afterthoughts, a mere commentary on someone else’s work. But Ada did want them to bear some name: “I rather wish to append anything that may tend hereaf-

j

ter to individualize it & identify it, with other productions of the said A.A.L.” And for all her apparent modesty, Ada knew how important her notes really were. “To say the truth, I am rather amazed at them; & cannot help being struck quite malgre moi, with the really masterly nature of the style, & its Superiority to that of the Memoir itself.” Her work was indeed vasdy more influential—and three times longer—than the text to which they were supposed to be mere adjuncts. A hundred years before the hardware had been built, Ada had produced the first example of what was later called computer programming.

matrices
Distinctions between the main bodies of texts and all their peripheral detail—indices, headings, prefaces, dedications, appendices, illustrations, references, notes, and diagrams—have long been integral to orthodox conceptions of nonfiction books and articles. Authored, authorized, and authoritative, a piece of writing is its own mainstream. Its asides are backwaters which might have been—and often are—compiled by anonymous editors, secretaries, copyists, and clerks, and while they may well be providing crucial support for a text which they also connect to other sources, resources, and leads, they are also sidelined and downplayed. ^
When Ada wrote her footnotes to Menabrea’s text, her work was implicitly supposed to be reinforcing these hierarchical divisions between centers and margins, authors and scribes. Menabrea’s memoir was the leading article; Ada’s work was merely a compilation of supporting detail, secondary commentary, material intended to back the author up. But her notes

made enormous leaps of both quantity and quality beyond a text which turned out merely to be providing the occasion for her work.
Only when digital networks arranged themselves in threads and links did footnotes begin to walk all over what had once been (he bodies of organized texts. Hypertext programs and (he Net are webs of footnotes without central points, organizing principles, hierarchies. Such networks are unprecedented in terms of their scope, complexity, and the pragmatic possibilities of their use. And yet they are also—and have always been—immanent to all and every piece of written work. “The frontiers of a book,” wrote Michel Foucault long before these modes of writing hypertext or retrieving data from the Net emerged, “are never clear-cut: beyond the tide, the first lines, and the last full stop, beyond its internal configuration and its autonomous form, it is caught up in a system of references to other books, other texts, other sentences: it is a node within network.”
Such complex patterns of cross-referencing have become increasingly possible, and also crucial to dealing with the floods of data which have burst the banks of traditional modes of arranging and retrieving information and are now leaking through the covers of articles and books, seeping past the boundaries of the old disciplines, overflowing all the classifications and orders of libraries, schools, and universities. And the sheer weight of data with which (he late twentieth century finds itself awash is only (he beginning of the pressures under which traditional media are buckling. If the “treatment of an irregular and complex topic cannot be forced in any single direction without curtailing the potential for transfer,” it has suddenly become obvious (hat no topic is as regular and simple as was once assumed. Reality does not rim along the neat straight lines of the

printed page. Only by “criss-crossing the complex topical landscape” can the “twin goals of highlighting mnltifacetedness and establishing multiple connections” even begin to be attained. Hypertext makes it possible for “single (or even small numbers of) connecting threads” to be assembled into a “ ‘woven’ interconnectedness” in which “strength of connection derives from the partial overlapping of many different strands of connectedness across cases rather than from any single strand running through large numbers of cases ...”
“It must be evident how multifarious and how mutually complicated are the considerations,” wrote Ada in her own footnotes. “There are frequently several distinct sets of effects going on simultaneously; all in a manner independent of each other, and yet to a greater or less degree exercising a mutual influence. To adjust each to every other, and indeed even to preceive and trace them out with perfect correctness and success, entails difficulties whose nature partakes to a certain extent of those involved in every question where conditions are very numerous and inter-complicated; such as for instance the estimation of the mutual relations amongst statistical phenomena, and of those involved in many other classes of facts.”
She added, “All, and everything is naturally related and interconnected. A volume I could write on this subject.”
tensions
Just as individuated texts have become filaments of infinitely tangled webs, so the digital machines of the late twentieth century weave new networks from what were once isolated words, numbers, music, shapes, smells, tactile textures, architectures.

and countless channels as yet unnamed. Media become interactive and hyperactive, the multiplicitous components of an immersive zone which “does not begin with writing; it is directly related rather to the weaving of elaborate figured silks.” The yarn is neither metaphorical nor literal, but quite simply material, a gathering of threads which twist and turn through die history of computing, technology, the sciences and arts. In and out of the punched holes of automated looms, up and down through the ages of spinning and weaving, back and forth through the fabrication of fabrics, shuttles and looms, cotton and silk, canvas and paper, brushes and pens, typewriter^ carriages, telephone wires, synthetic fibers, electrical filaments. silicon strands, fiber-optic cables, pixeled screens, telecom lines, the World Wide Web, the Net, and matrices to come.
“Before you run out the door, consider two things:
The future Is already set, only the past can be changed, and If It was worth forgetting. It's not worth remembering. ”
Pat Cadlgan, Foots
When the first of the cyberpunk novels, William Gibson’s Neuromancer was published in 1984, the cyberspace it described was neither an actually existing plane, nor a zone plucked out of the thin airs of myth and fantasy. It was a virtual reality which was itself increasingly real. Personal computers were becoming as ubiquitous as telephones, military simulation technologies and telecommunications networks were known to be highly sophisticated, and arcade games were addictive and increasingly immersive. Neuromancer was a fiction, and also another piece of the jigsaw which allowed these components to converge. In the course of the next decade, computers lost their significance as

isolated calculators and word processors to become nodes of the vast global network called the Net. Video, still images, sounds, voices, and texts fused into the interactive multimedia which now seemed destined to converge with virtual reality helmets and data suits, sensory feedback mechanisms and neural connections, immersive digital realities continuous with reality itself. Whatever that was now supposed to be.
At the time, it was widely assumed that machines ran on more or less straightforward lines. Fictions might be speculative and inspire particular developments, but they were not supposed to have such immediate effects. Like all varieties of cultural change, technological development was supposed to proceed step after step and one at a time. It was only logical, after all. But cyberspace changed all this. It suddenly seemed as if all the components and tendencies which were now feeding into this virtual zone had been made for it before it had even been named; as though all the ostensible reasons and motivations underlying their development had merely provided occasions for the emergence of a matrix which Gibson’s novel was nudging into place; as though the present was being reeled into a future which had always been guiding the past, washing back over precedents completely unaware of its influence.
Neuromancer was neither the first nor the last of such confusions between fiction and fact, future and past. When Gibson described '“bright lattices of logic unfolding across that colorless void,” his cyberspace was already implementing earlier—or later—works of nonfiction: Alan Turing’s universal machine had drawn the devices of his day—calculators and typewriters— into a virtual system which brought itself on-line in the Second World War; Ada’s Analytical Engine, which backed the punched-card processes of the automated weaving machine;

and Jacquaid’s loom, which gathered itself on the gathering threads of weavers who in turn were picking up on the threads of the spiders and moths and webs of bacterial activity.

on the cards

Until the early eighteenth century, when mechanisms which allowed looms to automatically select their own threads were introduced, it could take a weaver “two or three weeks to set up a drawloom for a particular pattern.” The new devices used punched-paper rolls, and then punched cards which, when they were strung together in the early nineteenth century, made the loom into the first piece of automated machinery. It was Joseph Marie Jacquard, a French engineer, who made this final move. “Jacquard devised the plans of connecting each group of threads that were to act together, with a distinct lever belonging exclusively to that group. All these levers terminate in rods” and a “rectangular sheet of pasteboard” moves “with it all the rods of the bundle, and consequently the threads that are connected with each of them.” And if this board, “instead of being plain, were pierced with holes corresponding to the extremities of the levers which meet it, then, since each of the levers would pass through the pasteboard during the motion of the latter, they would all remain in their places. We thus see that it is easy so to determine the position of the holes in the pasteboard, that, at any given moment, there shall be a certain number of levers, and consequently parcels of threads, raised, while the rest remain where they were. Supposing this process is successively repeated according to a law indicated by the pattern to be

■ 

executed, we perceive that this pattern may be reproduced on the stuff.”
As a weaving system which “effectively withdrew control of the weaving process from human workers and transferred it to the hardware of the machine,” the Jacquard loom was “bitterly opposed by workers who saw in this migration of control a piece of their bodies literally being transferred to the machine. ” The new frames were famously broken by Luddite rioters to whom, in his maiden speech in the House of Lords in 1812, Lord Byron offered his support. “By the adoption of one species of frame in particular,” he said, “one man performed the work of many, and the superfluous laborers were thrown out of employment. Yet it is to be observed that the work thus executed was inferior in quality; not marketable at home, and merely hurried over with a view to exportation. It was called, in the cant of the trade, by the name of ‘Spider-work.’ ”
Byron was concerned that his peers in the Lords would think him “too lenient towards these men, & half ajramebreaker myself.” But, unfortunately for both his argument and the handloom weavers who were thrown out of work, the fabrics woven on the new looms soon surpassed both the quantity and quality of those which had been made by hand. And the Spider- work did not stop here. These automated processes were only hints as to the new species Byron’s daughter had in store.
“I do not believe that my father was (or ever could have been) such a Poet as I shall be an Analyst.”
Ada Lovelace, July 1843
Babbage had a long-standing interest in the effects of automated machines on traditional forms of manufacture, publishing his research on the fate of cottage industries in the Midlands and

North of England, The Economy of Manufactures and Machinery, in 1832. The pin factory with which Adam Smith had illustrated his descriptions of the division of labor had made a great impression on him and, like his near contemporary Marx, he could see the extent to which specialization, standardization, and systematization had made both factories and economies into enormous automated machines themselves. Babbage was later to look back on the early factories as prototype “thinking machines,” and he compared the two main functions of the Analytical Engine—storage and calculation—to the basic components of a textiles plant. “The Analytical Engine consists of two parts,” wrote Babbage. “1st. The store in which all the variables to be operated upon, as well as all those quantities which have arisen from the result of other operations, are placed,” and “2nd. The mill into which the quantities about to be operated upon are always brought.” Like the computers which were later to run, and still do, the Engine had a store and mill, memory and processing power.
It was the Jacquard loom which really excited and inspired this work. Babbage owned a portrait of Jacquard, woven on one of his looms at about 1,000 threads to the inch and its production had demanded the use of some 24,000 punched cards, each one capable of carrying over 1,000 punch-holes, and Babbage was fascinated by the fine-grained complexity of both the cloth and the machine which had woven it. “It is a known fact,” he wrote, “that the Jacquard loom is capable of weaving any design which the imagination of man may conceive.” The portrait was a five-feet-square “sheet of woven silk, framed and glazed, but looking so perfectly like an engraving, that it had been mistaken for such by two members of the Royal Academy.”
While it was “generally supposed that the Difference Engine, after it had been completed up to a certain point, suggested

the idea of the Analytical Engine; and that the second is in fact the improved offspring of the first, and grew out of the existence of its predecessor,” Ada insisted that the Analytical Engine was an entirely new machine: “the ideas which led to the Analytical Engine occurred in a manner wholly independent of the latter engine, and might equally have occurred had it never existed nor been even thought of at all.” The Difference Engine could “do nothing but add; and any other processes, not excepting those of simple subtraction, multiplication and division, can be performed by it only just to that extent in which it is possible, by judicious mathematical arrangement and artifices, to reduce them to a series of additions.” As such, it is “the embodying of one particular and very limited set of operations, which . . . maybe expressed thus (+,+,+,+,+,+), or thus, 6 (+). Six repetitions of the one operation, +, is, in fact, the whole sum and object of that engine.” But if the Difference Engine could simply add up, the Analytical Engine was capable of performing the “whole of arithmetic.”
Women can’t add, he said once, Jokingly. When I asked him what he meant, he said, For them, one and one and one and one don’t make four.
What do they make? I said, expecting five or three.
Just one and one and one and one, he said.
Margaret Atwood, The Handmaid’s Tale
“If we compare together the powers and the principles of construction of the Difference and of the Analytic Engines,” she wrote, “we shall perceive that the capabilities of the latter are immeasurably more extensive than those of the former, and that they in fact hold to each other the same relationship as that of analysis to arithmetic.” It was, as Babbage wrote, “a machine of

the most general nature.” This machine could not merely synthesize the data already provided by its operator, as the Difference Engine had done, but would incarnate what Ada Lovelace described as the very “science of operations.”

second sight
Babbage’s attempts to build an adding machine were not without precedent. Wilhelm Leibniz’s seventeenth-century Stepped Reckoner was marketed on the basis that it would “be desirable to all who are engaged in computations . . . the managers of financial affairs, the administrators of others’ estates, merchants, surveyors, geographers, navigators, astronomers, and those connected with any of the crafts that use mathematics.” His work was in part inspired by the Pascaline, developed by Blaise Pascal in 1642. This machine used rotating wheels and a ratchet to perform addition and subtraction and was also designed as a device “by means of which you alone may, without any effort, perform all the operations of arithmetic, and may be relieved of all the work which has often times fatigued your spirit when you have worked with the counters or with the pen.”
While Babbage’s Difference Engine had already improved on these earlier designs, the Analytical Engine was a vasdy superior machine. And it was, as Ada wrote, “the introduction of the principle which Jacquard devised for regulating, by means for punched cards, the most complicated patterns in the fabrication of brocaded stuffs,” which gave the Analytical Engine its “distinctive characteristic” and “rendered it possible to endow mechanism with such extensive faculties as bid fair to make this engine the executive right-hand of abstract algebra.

“The mode of application of the cards, as hitherto used in the art of weaving, was not found, however, to be sufficiently powerful for all the simplifications which it was desirable to attain in such varied and complicated processes as those required in order to fulfil the purposes of an Analytical Engine. A method was devised of what was technically designated backing the cards in certain groups according to certain laws. The object of this extension is to secure the possibility of bringing any particular card or set of cards into use any number of times successively in the solution of one problem.” This sophistication of the punched-card system caused “the prism over which the train of pattern cards is suspended to revolve backwards instead of forwards, at pleasure, under the requisite circumstances; until, by so doing, any particular card, or set of cards, that has done duty once; and passed on in the ordinary regular succession, is brought back to the position it occupied just before it was used the preceding time. The prism then resumes its forward rotation, and thus brings the card or set of cards in question into play a second time.” The cards were selected by the machine as it needed them, and effectively functioned as a filing system, a means of storage and retrieval which allowed the engine to draw on its own information as required without having to make a linear run through all its cards.
“There is no limit to the number of cards that can be used. Certain stuff require for their fabrication not less than twenty thousand cards,” and because their repetition “reduces to an immense extent the number of cards required,” the Engine could “far exceed even this quantity.” This was an improvement “especially applicable wherever cycles occur in mathematical operations,” so that “in preparing data for calculations by the engine,” wrote Ada, “it is desirable to arrange the order and combination of the processes with a view to obtain them as

much as possible symmetrically and in cycles.” Ada defined any “recurring group” as “a cycle. A cycle of operations, then, must be understood to signify any set of operations which is repeated more than once. It is equally a cycle, whether it be repeated twice only, or an indefinite number of times; for it is the fact of a repetition occurring at all that constitutes it such. In many cases of analysis there is a recurring group of one or more cycles; that is, a cycle of a cycle, or a cycle of cycles ...”
The Engine’s capacity to circulate its data also meant that it was always “eating its own tail,” as Babbage described it, so that “the results of the calculation appearing in the table column might be made to affect the other columns, and thus change the instructions set into the machine.” The Engine “could make judgements by comparing numbers and then act upon the result of its comparisons—thus proceeding upon lines not uniquely specified in advance by the machine’s instructions.”
When Babbage had talked about the Analytical Engine’s ability to anticipate the outcomes of calculations it had not yet made, it was felt that his “intellect was beginning to become deranged.” But Babbage’s forward thinking was not a patch on Ada’s own anticipative powers. “I do not think you possess half m y forethought, & power of seeing all possible contingencies (probable & improbable, just alike),” she told Babbage.
“I am a Prophetess bom Into the world, & this conviction fills me with humility, with fear and trembling!”
Ada Lovelace, November 1844
Ada hoped that the difficulties in the way of constructing either the Difference Engine or the Analytical Engine “will not ultimately result in this generation’s being acquainted with these inventions through the medium of pen, ink, and paper merely,”

but she also had no doubt that the immediate construction of the machine was not the only key to its influence. Any such development, she writes, will have “various collateral influences, beside the main and primary object attained.” And “in so distributing and combining the truths and the formulae of analysis, that they may become most easily and rapidly amenable to the mechanical combinations of the engine, the relations and the nature of many subjects in that science are necessarily thrown into new lights, and more profoundly investigated. This is a decidedly indirect, and a somewhat speculative, consequence of such an invention. It is however pretty evident, on general principles, that in devising for mathematical truths a new form in which to record and throw themselves out for actual use, views are likely to be induced, which should again react on the more theoretical phase of the subject.”
The Engine was left on the nineteenth-century drawing board, and it was a hundred years before anything akin to Ada’s software would find the hardware on which to run. Even the most interested parties tend to think that Ada, for all her foresight, had no influence on the machines which were to come, regarding both her programs and the Analytical Engine itself as aberrant works of genius so untimely as to be more or less irrelevant to the future course of the machines.
But technical developments are rarely simple matters of cause and effect, and Ada was right to assume that the Engine would have more than an immediate influence. While they may have left few trails of the kind which can easily be followed and packaged into neat and linear historical accounts, Ada and her software did not evaporate. The programs began to run as soon as she assembled them.
Lack of public support, funding, Babbage’s own eccentricities, and ill health all contributed to the abandonment of the

machine. But the greatest obstacle to the construction of the Analytical Engine was simply technical capacity. The Engine demanded an attention to both precision and abstraction which earlier, single-purpose machines had not required, and for all its sophistication, nineteenth-century engineering was neither accurate nor diverse enough to produce even the machines capable of manufacturing the components for such a machine. While Henry Maudslay, for example, had developed screw cutting at the end of the eighteenth century, the absence of universal standards for its threads constituted an enormous obstacle to the construction of a machine as precise as the Analytical Engine. But if the Analytical Engine suffered at the time for the want of precision engineering, it also played a leading role in the development of the capacity necessary to its own construction. An 1846 reference work on the lathe included Babbage’s “On the Principles of Tools for Turning and Planing Metals” and, eager to acquire the components, Babbage collaborated with a number of engineers, including Joseph Clement, who had worked with Maudslay on the first mechanized lathes, and Joseph Whitworth, whose 1841 paper “On a Universal System of Screw Threads” was already a consequence of Babbage’s exacting demands for his machines. This text also triggered a process of standardization which was in widespread use by the late 1850s and was crucial to all subsequent engineering, scientific experiment, and of course, computing itself. The Engine was assembling the processes and components from which it would eventually be built.
The Analytical Engine also fed back into the practices from which it had most immediately emerged. It was, wrote Ada, such a superb development of automated weaving that its discoveries were used “for the reciprocal benefit of that art.” The “introduction of the system of backing into the Jacquard-

loom itself” meant that “patterns which should possess symmetry, and following regular laws of any extent, might be woven by means of comparatively few cards.”
"Unbuttoning the coat, he thrust his hands Into the trouser- pockets, the better to display the waistcoat, which was woven In a dizzy mosaic of tiny blue-and-whlte squares. Ada Chequers, the tailors called them, the Lady having created the pattern by programming a Jacquard loom to weave pure algebra
William Gibson and Bruce Sterling, The Difference Engine

anna 1
In 1933, Sigmund Freud made his final attempt to solve the riddle of femininity: “to those of you who are women,” he wrote, “this will not apply—you are yourselves the problem.” Having dealt with its wants and deficiencies and analyzed its lapses and absences, he had only a few more points to make. “It seems,” he wrote, “that women have made few contributions to the inventions and discoveries of the history of civilization.” They lacked both the capacity and the desire to change the wprld. They weren’t logical, they couldn’t think straight, they flitted around and couldn’t concentrate.
Distracted by the rhythmic beat of a machine, Freud looked up to see his daughter at her loom. She had wandered off, she was miles away, lost in her daydreams and the shuttle’s flight. But the sight of her gave him second thoughts. When he took up the thread, he had changed his mind: “There is, how-

ever, one technique which they may have invented—that of plaiting and weaving.
“If that is so, we should be tempted to guess the unconscious motive for the achievement,” he writes. “Nature herself would seem to have given the model which this achievement imitates by causing the growth at maturity of the pubic hair that conceals the genitals. The step that remained to be taken lay in making the threads adhere to one another, while on the body they stick into the skin and are only matted together.” Since she has only a hole where the male has his source of creativity, the folding and interlacing of threads cannot be a question of a thrusting male desire. Unless she was hiding something else, the processes which so engrossed her must, of course, be a matter of concealing the shameful “deficiency” of the female sex.
Take Anna: a weaver and a spinster too, working to cover her wounded pride, her missing sense of self, the holes in her life and the gaps in her mind. She simply doesn’t have what it takes to make a difference to the civilized world. Her work is a natural compensation for a natural flaw. All she can discover is her own incompletion; all she can invent are ways and means to process and conceal her sense of shame.
If weaving was to count as an achievement, it was not even one of women’s own. Their work is not original or creative: both the women and their cloths are simply copying the matted tangles of pubic hair. Should they have pretensions to authority, they would only be faking this as well. Women “can, it seems, (only) imitate nature. Duplicate what nature offers and produces. In a kind of technical assistance and substitution.” Weaving is an automatic imitation of some bodily function already beyond the weaver’s control. She is bound to weave a costume for the masquerade: she is an actress, a mimic, an impersonator, with no authenticity underneath it all. She has nothing to re-

veal, no soul to bare, not even a sex or a self to please. He pulls aside the veils, the webs of lies, the shrouds of mystery, and the layers of deception and duplicity, and finds no comfort, no there there. Only “the horror of nothing to be seen.” Good of her to cover it up for him.
This tale of absence, castration, deficiency, negativity, substitution was composed by one whom Gilles Deleuze and Felix Guattari describe as “an overconscious idiot who has no understanding of multiplicities.” From Freud’s point of view, there is one and its other, which is simply what one sees of it. And what one sees is nothing at all. “Because the path it traces is invisible and becomes visible only in reverse, to the extent that it is travelled over and covered by the phenomena it induces within the system, it has no place other than that from which it is ‘missing,’ no identity other than that which it lacks.”
Anna Freud’s biographer describes her as a woman who “specialized in reversals, in making the absent present, the lost found, the past current . . . she could also make the undone done, or—even more valuable—doable. When she was tired and faced with a stack of letters to answer, for example, she would simply set her pen down on a blank page and scurry it along, making quick mountain ranges of scribble. Then she would sign her name under the rows of scribble in her characteristic way, as one flourishing word: annafreud.”
After that, it was downhill all the way. “Having thus written a letter in fantasy with complete ease, she wrote a real letter helped by the sense that the task was accomplished anyway.” It’s easy to complete a job already done. “Her lectures were composed in the same way. First she lectured in her imagination, enjoying the thunderous applause, and then she made an outline of what she had said, adjusting it if she needed to for greater simplicity and coherence. Later, with her outline in hand, she

would give die lecture extempore. The method—if it can be called that—also supplemented her pleasure in sprints of thought. Intellectually she was ... a quick sketcher.”
No doubt Freud despaired at such unorthodox approaches to her work. It seemed she did everything in reverse, backward, upside down, contrary to any rational approach. But if Anna’s techniques appeared to be the random tactics of a scattered brain, knowing something backward and inside out is far in advance of any straightforward procedure. And she was hardly alone in her topsy-turvy ways. This ability to win “victories in advance, as if acquired on credit” may not figure in the history of discoveries and inventions familiar to Freud, but this is only because it underlies the entire account. According to Marshall McLuhan, “the technique of beginning at the end of any operation whatever, and of working backwards from that point to the beginning’’ was not merely an invention or discovery to be added to the list: it was “the invention of invention” itself.
This is hysteresis, the lagging of effects behind their causes. Reverse engineering: the way hackers hack and pirates conspire to lure the future to their side. Starting at the end, and then engaging in a process which simultaneously assembles and dismantles the route back to the start, the end, the future, the past: who’s counting now? As Ada said, she “did everything topsyturvy, & certainly ought to have come into the world feet downwards. ” Mere discoveries were not enough for her: “I intend to incorporate with one department of my labours a complete reduction to a system, of the principles and methods of discovery. ”
The prevalence of these backward moves is not the least of the reasons why histories of technology—and indeed histories of anything at all—are always riddled with delicious gaps, mysteries, and riddles just like those perplexing Freud. No straight-

forward account can ever hope to deal with the tactical advantages gained by such disorderings of linear time. The names and dates and jrreat achievements of the Read Only Memory called history may enjoy their fifteen kilobytes of digital fame on the latest encyclopedic compact disc, but what announce themselves to be founding fathers, points of origin, and defining moments only ever serve as distractions from the ongoing processes, the shifting differences that count. These are subtle and fine grained, often incognito, undercover, in disguise as mere and minor details. If, that is, they show themselves at all.
"Ada's method, as will appear, was to weave daydreams Into seemingly authentic calculations.”
Doris Langley Moore, Ada, Countess of Lovelace

gambling on the future
“That you are a peculiar—very peculiar—specimen of the feminine race, you are yourself aware.” They called her “wayward, wandering . . . deluded.” She didn’t argue; she seemed not to care. “The woman brushed aside her veil, with a swift gesture of habit” and, as though responding to Sigmund Freud, said, “There is at least some amusement in being so curious a riddle.” She didn’t have a name to call her own, but she did have many avatars: Ada Augusta King, Countess of Lovelace; Ada Lovelace, nee Byron; A.A.L., the first programmer. She is also Ada, the language of the United States military machine. “She is the Queen of Engines, the Enchantress of Number.”
Soon after Ada’s birth, Lord Byron went his own opiated way, and Lady Byron brought her daughter up with all the

excesses of stringent discipline to which well-bred girls were supposed to be subject. After rumors of a scandalous affair, she married William, a man in his thirties, when she was still in her teens, and became Ada King in 1835. Three years later, when William inherited his father’s title, she became a countess in name as well as deed.
When she married, her mother instructed her to bid “adieu to your old companion Ada Byron with all her peculiarities, caprices, and self-seeking; determined that as A.K. you will live for others.” She tried to be the dutiful daughter and did her best to lead a domesticated life. She was the mother of two boys and a girl by the age of twenty-four. But it wasn’t long before she was describing her children as “irksome duties Sc nothing more.” Although she had “wished for heirs,” she had never “desired a child,” and described herself as having a “total deficiency in all natural love of children.” She wrote, “To tell the honest truth I feel the children much more nuisance than pleasure & cannot help remembering that I am not naturally or originally fond of children.” She wrote of her husband with affection, describing him as “my chosen pet, ” but also expressed her indifference to any "mortal husband,” even her own. “No man would suit me,” she wrote, “tho’ some might be a shade or two less personally repugnant to me than others.”
One of Ada’s most long-standing and trustworthy friends was the acclaimed mathematician Mary Somerville, who had published the Connection of the Physical Sciences in the early 1830s. Just after her marriage she wrote to Mary, “I now read Mathematics every day, & am occupied on Trigonometry & in preliminaries to Cubic and Biquadratic Equations. So you see that matrimony has by no means lessened my taste for these pursuits, nor my determination to carry them on.” She also gained many new interests after her children were born. She lost

thousands at the races and, seduced by her mathematical prowess and her reassurances that she really did have “a system,” many of her male companions were also encouraged to do the same. This was an illegitimate use of her already dubious interest in mathematics. “The passions suffer no less by this gaming fever than the understandings and the imagination. What vivid, unnatural hope and fear, joy and anger, sorrow and discontent burst out all at once upon a roll of the dice, a turn of the card, a run of the shining gurneys! Who can consider without indignation that all those womanly affections, which should have been consecrated to children and husband, are thus vilely prostituted and thrown away. I cannot but be grieved when I see the Gambling Lady fretting and bleeding inwardly from such evil and unworthy obsessions; when I behold the face of an angel agitated by the heart of a fury!”
Ada was ill for much of her short life, walking with crutches until the age of seventeen, and endlessly subject to the fits, swellings, faints, asthmatic attacks, and paralyses which were supposed to characterize hysteria. “Heaven knows what intense suffering & agony I have gone thro’; & how mad & how reckless & desperate I have at times felt,” she wrote. “There has been no end to the manias & whims I have been subject to, & which nothing but the most resolute determination on my part could have mastered.”
Like many of her ailing contemporaries, Ada had been subjected to a variety of treatments before she developed an “opium system” in the 1840s. This was supposed to bring her down, but it only added to her volatility. “No more laudanum has been taken as yet,” she wrote at one point. “But I doubt another twenty-four hours going over without. I am beginning to be excited, & my eyes burn again.” She would, she wrote, take laudanum “not for ever,” but “as a regular thing once or

twice a week.” The drug had “a remarkable effect on my eyes, seeming to free them, & to make them open & cool. ” In opium lay the vast expanses, orders, and harmonies conjured by mathematics: “It makes me so philosophical,” she wrote, “& so takes off all fretting eagerness & anxieties. It appears to harmonize the whole constitution, to make each function act in a just proportion; (with judgment, discretion, moderation).” Her doctor “seems to think it is not a mere palliative but has a far more radical effect. Since this last dose, I am inclined to think so myself . . . It is a pity that instead of ordering Claret some months ago, he had not advised laudanum or Morphine. I think he has got the thing at last.”
In 1851 a uterine examination revealed “a very deep and extensive ulceration of the womb” which her doctor thought must long have been “the cause of much derangement of health.” She died in 1852 at the age of thirty-six.
They called her complex of diseases hysteria, a diagnosis and a term which indicated wayward reproductive organs: hysteria is derived from the Greek word hystera, and means ‘wandering womb.’ There was a time when it was widely believed that “the womb, though it be so strictly attached to the parts we have described that it may not ehange place, yet often changes position, and makes curious and so to speak petulant movements in the woman’s body. These movements are various: to wit, ascending, descending, convulsive, vagrant, prolapsed. The womb rises to the liver, spleen, diaphragm, stomach, breast, heart, lung, gullet, and head.” Although such direct connections with the womb had fallen out of medical favor by the end of the nineteenth century, hysteria continued to be associated with notions of a wandering womb.
“There is in my nervous system,” wrote Ada, “such utter want of all ballast & steadiness, that I cannot regard my life or

powers as other than precarious.” They said she was a nervous system apparendy unable to setde down. She had what she described as a “vast mass of useless & irritating POWER. OF EXPRESSION which longs to have full scope in active manifestation such as neither the ordinary active pursuits or duties of life, nor the literary line of expression, can give vent to.” She couldn’t concentrate, flitting between obsessions, resdess, searching. At one point she declared, “There is no pleasure in way of exercise equal to that of feeling one’s horse flying under one. It is even better than waltzing.” At another the harp was her greatest love: “I play 4 & 5 hours generally, & never less than 3. I am never tired at the end of it.” Drama was another contender: “Clearly the only one which directs my Hysteria from all its mischievous & irritating channels.” But even this was a short-lived love: “I never would look to the excellence of mere representation being satisfactory to me as an ultimate goal, or exclusive object ...”
Ada was hunting for something that would do more than represent an existing world. Something that would work: something new, somethingelse. Even the doctors agreed that she needed “peculiar & artificial excitements, as a matter of safety even for your life & happiness.” Such stimulations simply did not exist. She had to engineer them to suit herself.
Hysterics were said to have “a hungry look about them.” Like all Luce Irigaray’s women, “what they desire is precisely nothing, and at the same time, everything. Always something more and something else besides that one—sexual organ, for example—that you give them, attribute to them”; something which “involves a different economy more than anything else, one that upsets the linearity of a project, undermines the goal- object of a desire, diffuses the polarization towards a single pleasure, disconcerts fidelity to a single discourse ...”

Ada was by turns sociable and reclusive, cautious and reckless, swinging between megalomaniac delight in her own brilliance and terrible losses of self-esteem. There had been times when she had almost given into the fashionable belief that overexertion of the intellect lay at the root of her hysteria. At one point she wrote, “Many causes have contributed to produce the past derangements; & I shall in future avoid them. One ingredient (but only one among many) has been too much Mathematics.” Not even countesses were supposed to count. But Ada could be very determined, proud of her own staying power, and sometimes absolutely convinced of her mathematical, musical, and experimental genius. “I am proceeding on a track quite peculiar and my own,” she wrote. “I mean to do what I mean to do.” In 1834 she explained that “nothing but very close & intense application to subjects of a scientific nature now seems at all to keep my imagination from running wild, or to stop up the void which seems to be left in my mind from a want of excitement.” And in spite of the prevailing opinion that numbers were bad for her, she was never coaxed into “dropping the thread of science. Mathematics &c. These may be still my ultimate vocation.”

binaries
The postwar settlement was supposed to mark the dawn of a new era of regulation and control: the Central Intelligence Agency, United Nations, welfare states, mixed economies, and balanced superpowers. This was a brave new equilibrated world of self-guiding stability, pharmaceutical tranquillity, white goods, nuclear families, Big Brother screens, and, to keep these

new shows on die road, vast new systems of machinery capable of recording, calculating, storing, and processing everything that moved. Fueled by a complex of military goals, corporate interests, solid-state economies, and industrial-strength testosterone, computers were supposed to be a foolproof means to the familiar ends of social security, political organization, economic or- - der, prediction, and control. Centralized, programmable systems running on impeccably logical lines, these new machines were supposed to make the most complex processes straightforward; But even in the most prosaic terms, this supposedly logical, directed, and controlled of zones has always been wildly unpredictable. In 1950. when the processing power which can now be inscribed on the surface of a silicon chip occupied vast air-conditioned rooms. IBM thought the total global market for computers was five. In 1951 the United States Census Bureau put UNI VAC to work, the Bank of America installed Electronic Recording Machine Accounting (ERMA), and by 1957, when the Type 650 was launched, IBM anticipated sales of somewhere between fifty and 250. Two years later some 2,000 computers were in use in government agencies and private companies, and the figures were drastically revised. Perhaps 200,000 computers would be sufficient to saturate the market. By the early 1990s, IBM alone was selling twice that number of systems a week.
Computers have continued to pursue these accelerating, exponential paths, proliferating, miniaturizing, stringing themselves together into vast telecommunications nets, embedding themselves in an extraordinary variety of commodities, becoming increasingly difficult to define. While the postwar prnpram- mable computers were composed of transistors which used sihcon as a semiconductor of electric current, hv the end of the 1950s. tbf integrated circuit connected the transistors and in-

scribed them a single wafer of silicon. In the same vein of exponential miniaturization, the microprocessor was developed in the early 1970s. effectively putting all solid-state circuits of a computer onto a single silicon chip. The screen migrated IromthieTV set to give the machine a monitor, and by the 1980s what had once been vast room-size systems without windows on the world were desktop microprocessors.
"The calculations taking place within the machine are continuously registered as clicks clicking high-pitched sounds as of tinkling bells, noises like those of a cash-register. There are lights that go out and come on at Irregular Intervals of time. They are red orange blue. The apertures through which they shine are circular. Every divergence Is ceaselessly recorded In the machine. They are scaled to the same unit whatever their nature.”
Monique Wlttlg, Lbs GuMIlires
Whether they are gathering information, telecommunicating, running washing machines, doing sums, or making videos, all digital computers translate information into the zeros and ones of machine code. These binary digits are known as hits and strung together in bytes of eight. The zeros and ones of machine code seem to offer themselves as perfect symbols of the orders of Western reality, the ancient logical codes which make the difference between on and off, right and left, light and dark, form and matter, mind and body, white and black, good and evil, right and wrong, life and death, something and nothing, this and that, here and there, inside and out, active and passive, true and false, ves and no, sanity and madness, health and sickness.. up and down, sense and nonsense, west and east, north and south. And they made a lovely couple when it came to sex. Man

and woman, male and female, masculine and feminine: one and zeio looked just right, made for each other: 1, the definite, upright line; and 0, the diagram of nothing at all: penis and vagina, thing and hole . . . hand in glove. A perfect match.
It takes two to make a binary, but all these pairs are two of a kind, and the kind is always kind of one. 1 and 0 make another 1. Male and female add up to man. There is no female equivalent. No universal woman at his side. The male is one, one is everything, and the female has “nothing you can see.” Woman “functions as a hole, ” a gap, a space, “a nothing—that is a nothing the same, identical, identifiable ... a fault, a flaw, a lack, an absence, outside the system of representations and autorepresentations.” Lacan lavs down the law and leaves no doubt: “There is woman only as excluded by the nature of things,” he explains. She is “not-all,” “not-whole,” “not-one,” and whatever she knows can only be described as “not-knowledge.” There is “no such thing as The woman, where the definite article stands for the universal.” She has no place like home. nothing of her own, “other than the place of the Other which.” writes Lacan, “I designate with a capital O.”

supporting evidence
Man once made himself the point of everything. He organized, she operated. He ruled, she served. He made the great discoveries, she busied herself in the footnotes. He wrote the books, she copied them. She was his helpmate and assistant, working in support of him, according to his plans. She did the jobs he considered mundane, often the fiddling, detailed, repetitive operations with which he couldn’t be bothered; the dirty, mind-

less, semiautomatic tasks to which he thought himself superior. He cut the cloth to fit a salary; she sewed the seams at a piece- rate wage. He dictated and she transcribed. In the newly automated factories and mills she worked on the looms and sewing machines; in the service of the great bureaucratic machines, she processed the words, kept the records, did the sums, and filed the accounts.
With “all the main avenues of life marked ‘male,’ and the female left to be female, and nothing else,” men were the ones who could do anything. Women were supposed to be singlepurpose systems, highly programmed, predetermined systems tooled up and fit for just one thing. They have functioned as “an ‘infrastructure’ unrecognized as such by our society and our culture. The use, consumption, and circulation of their sexual- ized bodies underwrite the organization and the reproduction of the social order, in which they have never taken part as ‘subjects.’ ” Everything depends on their complicity: women are the very “possibility of mediation, transaction, transition,
transference—between man and his fellow-creatures, indeed
_ •
between man and himself.” Women have been his go- betweens, those who took his messages, decrypted his codes, counted his numbers, bore his children, and passed on his genetic code. They have worked as his bookkeepers and his memory banks, zones of deposit and withdrawal, promissory notes, credit and exchange, not merely servicing the social world, but underwriting reality itself. Goods and chattels. The property of man.
That’s what it said in the manual. “It does strike me, though, that there are any number of women who resemble Lady Ada, our Queen of Engines being a queen of fashion as well. Thousands of women follow her mode.”
It takes time and patience. Many seconds pass. But, as it

turns out, women have not merely had a minor part to play in the emergence of the digital machines. When computers were vast systems of transistors and valves which needed to be coaxed into action, it was women who turned them on. They have not made some trifling contribution to an otherwise man-made tale; when computers became the miniaturized circuits of silicon chips, it was women who assembled them. Theirs is not asubsidiary role which needs to be rescued for posterity, a small supplement whose inclusion would set the existing records straight: wfren computers were virtnallv real machines, women wrote the software on which they ran. And when computer was a term applied to flesh and blood workers, the bodies which composed them were female. Hardware, software, wetware— before their beginnings and beyond their ends, women have been the simulators, assemblers, and programmers of the digital machines.

genderquake
"The Idea that a ‘nothing to be seen’. . . might yet have some reality, would Indeed be Intolerable to man.”
Luce Irigaray, Speculum of the Other Woman

In the 1990s, Western cultures were suddenly struck by an extraordinary sense of volatility in all matters sexual: differences, relations, identities, definitions, roles, attributes, means, and ends. All the old expectations, stereotypes, senses of identity and security faced challenges which have left many women with unprecedented economic opportunities, technical skills, cul-

tural powers, and highly rated qualities, and many men in a world whose contexts range from alien to unfamiliar.
This was neither a revolutionary break, nor an evolutionary reform, but something running on far more subtle, wide- ranging, and profound fault lines. Nothing takes the final credit—or the blame—for this shift which, as though in recognition of the extent to which it defies existing notions of cultural change, has been defined as genderquake. But the new machines, media, and means of telecommunication that compose what are variously called high, information, digital, or simply new technologies which have emerged within the last two decades have played an enormous and fascinating role in the emergence pf this new culture. This is far from a question of technological, or any other, determinism. If anything, technologies are only ever intended to maintain or improve the status quo, and certainly not to revolutionize the cultures into which they are introduced. It is in spite of their tendencies to reduce, objectify, and regulate everything that moves that computers and the networks they compose run on lines quite alien to those which once kept women in the home.
In some respects, the impact of these new machines is direct and very obvious. In the West, the decline of heavy industry, the automation of manufacturing, the emergence of the service sector, and the rise of a vast range of new manufacturing and information-processing industries have combined to reduce the importance of the muscular strength and hormonal energies which were once given such high economic rewards. In their place come demands for speed, intelligence, and transferable, interpersonal, and communications skills. At the same time, all the structures, ladders, and securities with which careers and particular jobs once came equipped have been subsumed by patterns of part-time and discontinuous work which

privilege independence, flexibility, and adaptability. These tendencies have affected skilled, unskilled, and professional workers alike. And, since the bulk of the old full-time, lifelong workforce was until recently male, it is men who have found themselves most disturbed and disrupted by these shifts, and, by the same token, women who they benefit.
These tendencies are far from new. Since the industrial revolution, and with every subsequent phase of technological change, it has been the case that the more sophisticated the machines, the more female the workforce becomes. Automa- tion has been accompanied by what is often referred to as the feminization of the workforce ever since the first automatic machines were operated by the first female workers, and the fears of unemployment which have haunted modern discussions of technological innovation have always applied to male workers rather than their female peers.
What is unprecedented is for male workers to be outnumbered by their female counterparts, as will clearly be the case in the United Kingdom and the United States by the end of this century. And with this tipping of the scales comes not only unprecedented degrees of economic power, but also a radical change in the status of female workers, an erosion of the male monopoly on tasks and jobs once reserved for men, and a new standing for the work involved in what were once considered to be pin-money jobs for women supplementing male incomes.
Many of these tendencies are also at work in the emergence of what the West was once in a position to call “the other side of the world.” By the time the cultures of the old white jworld had noticed they were even on the map, many of the so- called “tiger” nations—Singapore, Malaysia, Thailand, Korea, TaiwanT and Indonesia—were already leaping ahead in an eco- nomic game which for at least two hundred years had been

governed bv the West. And they are only the tips of an iceberg of change which brings many regions into play: China, India, East and Southern Africa, Eastern Europe, South America. Given that the populations of China and India alone vasdy outnumber those of the old white world, there seems litde doubt that the days of Western empire have well and truly died.
These regions have genderquakes of their own. And while a variety of political and religious fundamentalisms are doing their best to maintain the status quo, there are few regions of the world in which women are not asserting themselves with unprecedented ingenuity and, very often, great success. If Western women have dreamt of change for three hundred years, Asian women are playing roles which would have been unthinkable 4 only a decade or so ago. By the mid-1990s, 34 percent of 0 China’s self-employed were women, and 38 percent of Singaporean women managers were running companies of their own. Thailand’s leading hotel chain, Indonesia’s largest taxi company, and Taiwan’s two largest newspaper groups were owned by women. Japanese women still found themselves treated as “office flowers,” composed only 0.3 percent of board members of Japanese firms, and made up just 6.7 percent of the Japanese parliament. But the sexual shift was also evident in Japan: 2.5 million women owned businesses, five out of every six new Japanese firms were set up by women, and “a revolution without marches or manifestos” was underway.
There is enormous resistance to these changes whenever and wherever they occur. As their effects began to be felt in the early 1990s there were men who jerked their knees and went on TV to lament the fact that women and robots had apparendy conspired to take their masculinity away. One 1990s survey found one in two fathers still believing that “a husband should be the breadwinner and the wife should look after the home

and children”; the fear, if not the fact, of violent crime still keeps many women in at night; domestic violence was prevalent; and in Britain, the benefits system was still conspiring with the high costs and scarcity of child-care provision to keep many women from working, learning, or—perish the thought—enjoying themselves. As unprecedented numbers of women juggled children, education, and work, many female workers found themselves saddled with the low paying, part-time, insecure jobs rejected by men. In the United States, almost half of employed women worked in technical, sales, and administrative support jobs, and pay differentials were still very large: in 1992 American women still earned only 75 cents for every dollar earned by men, and while their participation in U.S. managerial and professional life rose from 40 percent in 1983 to 47 percent in 1992, it was still the case that women occupied relatively few executive posts and prominent public positions: only 10 percent of the voting members of the United States Congress were women, and the United Kingdom had only sixty women members of parliament. Many sectors of education, politics, and business seemed riddled with enough archaic detail and glass ceilings to make even the most determined women feel unwelcome. In universities, they were averaging higher marks than men, but relatively few gained first-class degrees; they were more numerous and successfiol as undergraduates and in master’s programs, but less prominent when it came to Ph.D. candidacy. Even highly successftd career women were more likely to drop out of their jobs than their male counterparts.
But many women had already set their sights beyond these traditional focal points. While the members of an older male workforce had found a sense of identity in their work, women were not only less able, but also less willing to define themselves through employment or a single career. Many of them were

actively seeking opportunities to make and break their own working lives, not necessarily in favor of family commitments, but also in an effort to free themselves from the imposition of external constraints on their time and economic capacity. There may have been men who still thought they were protecting their own positions of power by locking women out of the higher echelons of the universities, corporations, and public institutions, but it was no longer obvious that top positions were the most importer nr rlf<:irafr)e of roles to be played. High grades and doctorates were no longer enough to guarantee success outside an academic world itself poised on the brink of redundancy, and corporate executives were increasinpjy small pawns in global economic games. As for the attractions of public service, who was going to disagree with the young women who said that “politics is all talk and no action”? They simply felt they had betteLihings to do.
Some of these things were fai more lucrative as well: in the twenty years after 1970, the number of women-owned small businesses went from 5 percent to 32 percent in the United States, and in Britain nearly 25 percent of the self-employed were women by 1994, twice as many as in 1980. Taking the skills, contacts, and experience gained in their periods of paid employment, these women have tended to be far more successful than their self-employed male counterparts: in the United States, where most new businesses failed, those which were owned by women enjoyed an 80 percent success rate and employed more people than the companies on the Fortune 500 list.
Having had little option but to continually explore new avenues, take risks, change jobs, learn new skills, work, independently, and drop in and out of the labor market more frequently than their male colleagues, women seem far “better prepared.

culturally and Psychologically” for the new economic conditions which have emerged at the end of the twentieth renturv. They are advanced players of an economic game for which self- employment, part-time, discontinuous work, multiskilling, flexibility, and maximal adaptability were suddenly crucial to survival. Women had been ahead of the race for all their working lives, poised to meet these changes long before they arrived, as though they always had been working in a future which their male counterparts had only just begun to glimpse. Perhaps they really were the second sex, if seconds come after firsts.
" ‘Let the man get some sleep, Armttage, ’ Molly said from her futon, the components of the hetcher spread on the silk like some expensive puzzle. ‘He’s coming apart at the seams.'"
William Gibson, Neuromancer
But there was much more to come. Abandoned by the economic power and social privilege which once made them such attractive, even necessary, mates, the sperm counts fell, birth rates crashed, and the hormonal energy and muscular strength which once served them so well were now becoming liabilities. Women were becoming mothers on their own terms, or not at all. Heterosexual relations were losing their viability, queer con- nections were flourishing, the carnival had begun for a vast range of paraphilias and so-called perversions, and if there was more than one sex to have, there were also more than two to be. Anything claiming to be normal had become peculiar.
“He was thoroughly lost now; spatial disorientation held a peculiar horror for cowboys.”

William Gibson, Neuromancer

It was falling apart. They were coming undone. Everything was moving much too fast. What had once seemed destined to become a smoothly regulated world was suddenly running away with itself. Control was slipping through the fingers of those who had thought it was in their hands. Something was wrong. They were losing it all: their senses of security and identity, their grip, the plot, and even their jobs. Couldn’t see the point to anything. What else could the masters of the old white world do but redouble their efforts, intensify their drives for security, heighten and perfect their powers? But the more they struggled to adapt and survive, the faster the climate seemed to change. The more they tried to regain control, the more their narrative lost its thread; the closer they came to living the dream, the 4 weaker their grasp on power became. Was it even possible that, 4 regardless of their labors, their hopes and dreams, they had been “the sex organs of the machine world, as the bee of the plant world, enabling it to fecundate and to evolve ever new forms”? All that time, the effort and the pain, the trouble they had taken to maintain control.
“And Instead they watch the machines multiply that push them little by little beyond the limits of their nature. And they are sent back to their mountain tops, while the machines progressively populate the earth. Soon engendering man as their eplphenomenon. ”

Luce Irigaray, Marine Lover

cultures
Nothing takes the credit—or the blame—for either the runaway tendencies at work or the attempts to regulate them. Political struggles and ideologies have not been incidental to these shifts, but cultures and the changes they undergo are far too complex to be attributed to attempts to make them happen or hold them back. This is not because some other determination has come into play. If anything does emerge from the complexity of current shifts, it is the realization that cultures cannot be shaped or determined by any single hand or determining factor. Even conceptions of change have changed. Revolution has been revolutionized. There is no center of operations, no organizing core; there are no defining causes, overriding reasons, fundamental bases, no starting points or prime movers; no easy explanations, straightforward narratives, simple accounts, or balanced books. Any attempt to deal with some particular development immediately opens onto them all.
The impossibility of getting a grip, and grasping the changes underway is itself one of the most disturbing effects to emerge from the current mood of cultural change. The prospect of being in a position to know, and preferably control, changes manifest on the social scale has been crucial to modern conceptions of what used to be called man’s place in the grand scheme of things. Technology itself was supposed to be a vital means of exerting this explanatory and organizational power. But the revolutions in telecommunications, media, intelligence gathering, and information processing they unleashed have coincided with an unprecedented sense of disorder and unease,

not only in societies, states, economies, families, sexes, but also in species, bodies, brains, weather patterns, ecological systems. There is turbulence at so many scales that reality itself seems suddenly on edge. Centers are subsumed by peripheries, mainstreams overwhelmed by their backwaters, cores eroded by the skins which were once supposed to be protecting them. Organizers have found themselves eaten up by whatever they were trying to organize. Master copies lose their mastery, and everything valued for its size and strength finds itself overrun by microprocessings once supposed too small and insignificant to count.

nets
Of all the media and machines to have emerged in the late twentieth century, the Net has been taken to epitomize the shape of this new distributed nonlinear world. With no limit to the number of names which can be used, one individual can become a population explosion on the Net: many sexes, many species. Back on paper, there’s no limit to the games which can be played in cyberspace. Access to a terminal is also access to resources which were once restricted to those with the right face, accent, race, sex, none of which now need be declared. Using the Net quickly became a matter of surfing, a channel- hopping mode facilitated and demanded by information which is no longer bound together in linear texts or library classifications, but instead needs to be laterally traversed.
As the system began to spill out into wider academic usage over the course of the next twenty years, other networks also emerged. Businesses developed local, and then wide area net-

works; commercial on-line services appeared; electronic mail and bulletin boards proliferated alongside fanzines and the samizdat press. While the network was doubling in size every year, the screens were gray, the options limited, and the number of users relatively small until the late 1980s. Access was hardly limited to students, hackers, and academics, but certain skills and commitments to computing were prerequisites of any tangible input into the system, and the users of the network occupied a strange frontline between state institutions and anarchic private use. In the wake of a massive expansion of the Net, the arrival of cybercafes, public terminals, falling costs, and a complex of other economic and cultural tendencies, use of the Net has grown not only in the West but in almost two hundred countries of the world. Usenet gives readers and writers access to thousands of articles in thousands of threads in vast popula- tions of newsgroup conversations, continually adding to themselves and fading out of use. On-line worlds scrolled down the screens in IRC (Internet Relay Chat) networks, MUDs (Multi- User Dungeons, or Domains), and MOOs (MUDs Object Oriented), where .softbots—software robots—and pseudonymous users interact in labyrinthine virtual worlds. With the development of the World Wide Web, a user-friendly, interactive, multimedia interface which uses Hypertext Mar^p T.angnogp (HTML) to map and interlink the information on the screen to another, and in principle, any other site, the Net gained both a gleaming corporate mall, and also a degree of interconnectivity which has continually drawn more computers, pages, links, users, and characters into a network which soon hosted galleries, libraries, shopping malls, company showcases, S&M dungeons, university departments, personal diaries, fanzines . . . every page linked to at least one other, sometimes hundreds, and always proliferating.

The Net has not caught up with the more expansive hopes of unfettered, free-flowing information which were once attached to it. But the technical potential it opens up comes close to the enormous system of lateral cross-referencing which the hypertext networks Ted Nelson first named Xanadu in the 1960s, and the system Vannevar Bush called the memex in the 1940s. Both these conceptions were far more interactive than the system-of the mid-1990s allows. The user of Bush's imagined system left “a trail ... of interest through the maze of materials available,” adding links and connections, inserting passages, and making routes through an immense virtual library whose composition continually shifts as a consequence of the activity of those who are using it. Ted Nelson’s envisaged system, which, to some extent, has been realized by the World Wide Web, has the enormous advantage of facilitating this same level of influence with the introduction of (very) small payments of electronic cash for the use of material on specific sites. With the flat-rate subscription system currently in place, links have to be deliberately made and do not, as with pathways across a field of grass, emerge from the sheer force of numbers making them.
As well as potentially facilitating new modes of information circulation, this grass-roots commerce poses great threats to the corporate interests currently in play. But if large-scale commercial activity tends to turn the Net into a shopping mall, it had its beginnings in 1969 as ARPAnet, a U.S. military defense project which quickly joined cockroaches on the short list of those most likely to survive nuclear attack. Developed at the height of the cold war, the Net had also learned from the Viet Cong, whose networks of tunnels and guerrilla techniques had forced a centralized U.S. military machine to adopt unprecedented tactics of distribution and dispersal in response. These military influences on the Net are betrayed in its messages’

ability to route and reroute themselves, hunting for ways round obstacles, seeking out shortcuts and hidden passages, continually requisitioning supplies and hitching as many rides as possible. The network and its traffic are so dispersed that any damage to one part of the system, or even a particular message, will have litde effect on the whole machinery. Information is transmitted in packets which rarely take the same route twice, and may take many different routes to a destination at which they weave themselves together again. Maps of the network cannot be stolen, not because they are closely guarded, but because there is no definitive terrain. Any map adds to the network and is always already obsolete.
The growth of the Net has been continuous with the way it works. No central hub or command structure has constructed it, and its emergence has been that of a parasite, rather than an organizing host. It has installed none of the hardware on which it works, simply hitching a largely free ride on existing computers, networks, switching systems, telephone lines. This was one of the first systems to present itself as a multiplicitous. bottom-up, piecemeal, self-organizing network which, apart 'Irom a quo^rtY'ofTnilitarv influence, government censorship, and corporate power, could be seen to be emerging without any centralized control. Not that such lateral networks or boot- strapped systems have “an irresistible revolutionary calling . . .” The leading corporations are now expending all their energies on processes of molecularization and virtualization, continually downsizing and turning themselves into flattened horizontal operations and, in effect, getting all such modes of activity on their side. No matter how spontaneous their emergence, self-organizing systems are back in organizational mode as soon as they have organized themselves.
This conflict is inscribed in the double-edged quality of

the word itself. Technology is both a question of logic, the long arm of the law, logos, “the faculty which distinguishes parts (‘on the one hand and^n the other hand’V” and also a mn**™- dlf skills, digits(j5peeds>, and rhythms of techno, engineerings which run with “a completely other distrih»ri<™ •which must h** nomadic, a nomad nomos, without property, enclosure or measure.” The same amhival<»nrp y Scribed in the zeros and ones of computer code. These bits of code are themselves derived from two entirely different sources, and terms: the binary and the digital, or the symbols of a logical identity which does indeed put everything on one hand or the other, and the digits of mathematics, full of intensive potential which are not counted-hv hand but on the fingers and, sure enough, arrange themselves in pieces of eight rather than binary pairs.
The techno and the digital are never perceived to run free of the coordinating eyes and hands of logic and its binary codes. But logic is nothing without their virtual plane. They are the infrastructure to its superstructure: not another order of things, but another mode of operations altogether, the matters of a distribution which is “demonic rather than divine, since it is a peculiarity of demons to operate in the intervals between the gods’ fields of action . . . thereby confounding the boundaries between properties.”
“You know I am a d-------------d ODD anlmall And as my mother
often says, she never has quite yet made up her mind If It Is the Devil or Angel that watches peculiarly over me; only that It IS one or the other, without doubt!
“(And for my part, I am quite Indifferent which.)”
Ada Lovelace, December 1841

digits
The vast majority of what are now assumed to be the West’s mathematical terms and axioms are either Arabic or Hindu. The word algebra is taken from the tide of the Al-gebr we’l mukabala, a book written in the ninth century by one of the most sophisticated Arab mathematicians. Alkarismi. who gave his name to the algorithm. The Al-gebr is in turn based on the work of Brahmagupta, a Hindu mathematician and astronomer who, in the seventh century, consolidated India’s sophisticated but unwieldy arithmetical principles in the form of twenty basic processes “essential to all who wish to be calculators.”
The system of notation and calculation which emerged from this fusion of Hindu and Arabic arithmetic was introduced to the West by both Arabic scholars and Asian traders. Indian arithmetic had already been carried by merchants as far west as Baghdad, and Alkarismi’s own arithmetical prowess is said to have resulted from his own travels in India. It was a great spacesaving device when compared to its far more cumbersome counterparts, most of which had been developed in conjunction with the abacus, a device which was unknown to Hindu culture, but had been widely used in the Egyptian, Babylonian, Greek, and Roman worlds. While the abacus had removed the need to process and store numbers in concise written form, India had developed a sophisticated system of notation which it used both to calculate and record results.
India had effectively developed a written abacus, using its written numbers in place of pebbles t>r beads, giving them the same signs regardless of the positions they assumed, and using 0

or a dot to indicate an empty column of the virtual abacus. Whereas abacists used completely different signs for numbers with different place values—such as I for one and X for ten in Roman numerals—the Hindu system could use the same digit—1—to compose one, ten, hundred, and an obviously vast number of other numbers.
“It is India that gave us the ingenious method of earpness- ine all numbers bv means of ten symbols.” wrote Pierre-Simon Laplace, “each symbol receiving a value of position as well as an absolute value.5’ In other words, the numbers were both cardinal and ordinal, each expressing its place in the string (first, . second, third etc.), as well as a value specific to itself. Unlike the Roman numerals, in which two is simply two ones collected together, the Sanskrit two is a qualitatively different number to one, an entity or character in its own right. As Laplace points out, the new arithmetic was “a profound and important idea which appears so simple to us now that we ip-note its true merit, but its very simplicity, the great ease which it has lent to all computations, puts our arithmerir in the front rank of useful inventions.” Although this statement about “our-arithmetic” subtly appropriates the new system as one of the West’s “inventions,” Laplace continues, “We shall appreciate the grandeur of this achievement when we remember that it escaped the genius of Archimedes and Apollonius, two of the greatest men produced by antiquity.”
“Certainly my troops must consist of numbers, or they can have no existence at all, & would cease to be the particular sort of troops In question.—But then what are these numbers? There Is a riddle."

Ada Lovelace

To a Europe still counting in bundles of Roman sticks, this new
~~ mm
arithmetic, with its alien Sanskrit figures, was an infidel system which posed an extraordinary threat to the stability of the Western world. Although the Eastern system is as widely used as the alphabet today, it was not until the Renaissance that Europe’s new merchants overcame the opposition of the Church to the introduction of the numbers 123456789 and 0. One of the first texts on the new arithmetic—which was also one of the first books in the English language. The Craft of Nombrynge (ca. 1300)—was composed while edicts forbidding the use of the numbers were still being issued in Florence. By 1478, the first manual on the new arithmetic had been printed in Italy on one of the then brand-new Gutenberg presses. “Numeration is the representation of numbers by figures,” it explained. “This is done by means of ten letters or figures as here shown, l.,2.,3.,4.,5.,6.,7.,8.,9.,0. Of these the first figure, 1, is not called a number, but the source of number. The tenth figure, 0, is called cipher or ‘nulla,’ i.e. the figure of nothing, since by itself it has no value, although when joined with others it increases their value.”
In addition to its numbers, the new arithmetic introduced negative numbers and irrational numbers, as well as zero and the decimal point. These were features crucial to the networks of banking and trade which became increasingly important to European culture in the fifteenth century. Trade, which is now widely assumed to be a peculiarly Western invention, was then as new to Europe as these numbers, and there is little doubt that even the simple matters of keeping accounts, setting prices, doing deals, and working with large numbers were simply impossible with Roman numerals. This was not the least of the reasons why the infidel arith-

metic threatened a Christian culture which, even now, demurs at the thought of Sunday trade.
The one of the new arithmetic was also very different to the old straight line which had figured as both a number and the ninth letter of the Roman alphabet. Western philosophy is supposed to be an elucidation and confirmation of the unity of one, a number which had been held in great esteem long before there was one male god. To the ancient Greeks, one was everything and anything, first and last, best and good, universal, unified. It was the symbol of existence, identity, and being. Stricdy speaking, there was nothing else. To be anything at all was to be one.

For all its dreams of self-sufficiency, even one has always needed another of some kind. But since it was the only one as well, it had to ensure that any other options were merely impoverished variations on its theme. The Greeks recognized many as andternative to one but, like the Romans,, even, this was conceived as a collection of many ones. Derived from the Greek term iota, and closely related to atom and jot, this one was taken to symbolize any individuated and indivisible entity, whereas the Sanskrit one functioned in relation to the other eight digits of the Hindu system. But one closely resembled the old Roman line and was easily subsumed into the old paradigm. Any differences between the two were more or less erased.
Zero posed a very different threat. When it first appeared •0
in the new string of infidel figures, the old Church fathers did everything to keep it out of a world which then revolved around one and its multiples: one God, one truth, one way, one one. The numbers 2,3,4,5,6,7,8,9 were subversive enough, but zero was unthinkable. If it wasn’t one of something, it couldn’t be allowed. Then again, the Church could hardly be seen to protest too much about something that, as far as they could see,

wasn’t really there at all. If zero was nothing, it should be as easy to absorb as the Sanskrit one had been. And, sure enough, zero was appropriated as a sign of absence, nonbeing, and nothingness. The ancient unity of something_and-nathmg-waS-apparently undisturbed.

holes
“■Somewhere there Is a siren. Her green body Is covered with scales. Her face Is bare. The undersides of her arms are a rosy colour. Sometimes she begins to sing. The women say that of her song nothing Is to be heard but a continuous 0. That Is why this song evokes for them, like everything that recalls the 0, the zero or the circle, the vulval ring."
Monique Wlttig, Les GuSrIIISres
Having escaped the rigors of an education which would have taught her not to ask such things, Ada wandered off, around and about, and wondered about zero too. One of her earliest enquiries to Augustus De Morgan, her tutor in mathematics, concerned the status of this figure. Did it exist as a “thing,” she asked? Was it something, or nothing, or something else again? He gave her an intriguing answer. “Zero is something, ” he explained, “though not some quantity, which is what you here mean by thing.”
“She does not set herself up as one, as a (single) female unit. She Is not closed up or around one single truth or essence. The essence of a truth remains foreign to her. She neither has nor Is a being. And she does not oppose a feminine truth to a

masculine truth ... the female sex takes place by embracing Itself, by endlessly sharing and exchanging Its lips, Its edges, Its borders, and their ‘content,’ as It ceaselessly becomes other, no stability of essence Is proper to her."
Luce IHgaray, Speculum of the Other Woman
Zero may mean nothing to the Western world, but this has nothing to do with the way it works. It was certainly crucial to the functioning of the Analytical Engine, a machine which, according to Menabrea, used an “occult principle of change” which allowed it to “provide for singular values.” The Engine was able to deal with those functions “which necessarily change in nature when they pass through zero or infinity, or whose values cannot be admitted when they pass these limits. When such cases present themselves, the machine is able, by means of a bell, to give notice that the passage through zero or infinity is taking place, and it then stops until the attendant has again set it in action for whatever process it may next be desired to perform. If this process has been foreseen, then the machine instead of ringing, will so dispose itself as to present Che new .cards which have relation to the operation that is to succeed the passage through zero and infinity.” It is the possibility of this passage which allows the machine “arbitrarily to change its processes at any moment, on the occurrence of any specified contingency.” >
In terms of the pragmatic roles they play, the zeros and ones of machine code do far more than hark back to the binaries their logical symbols represent. If zero is supposed to signify a hole, a space, or a missing piece, and one \ is the sign of positivity, digital machines turn these binaries around. In both electronic systems and the punched cards of weaving machines, a hole is one, and a blank is zero, in which case there are two

missing elements, if missing is where either can be said to go. No longer a world of ones and not-ones, or something and nothing, thing and gap, but rather not-holes and holes, not- nothing and nothing, gap and not-gap. Not that this matters any more than the initial dualism between one and a zero conceived as not-one. Zero was always something very different horn the sign which has emerged from the West’s inability to deal with anything which, like zero, is neither something in particular nor nothing at all. And it is certainly the case that, witH oF vhthout the signs that represent them as inert negativities, holes themselves are never simply absences of positive things. This is a purely psychoanalytical myth. For Deleuze and Guattari, it is not even enough “to say that intense and moving particles pass through holes: a hole is just as much a particle as what passes through it . . Holes are not absences, spaces where there should be something else. “Flying anuses, speeding vaginas, there is no castration.” Adrift in the doped lattices of a silicon crystal, a hole is a positive particle before it is the absence of a negatively charged electron, and the movement of electrons toward the positive terminal is also a flow of holes streaming back the other way. Holes are charged particles running in reverse. For the quantum physicist, “holes are not the absence of particles but particles traveling faster than the speed of light.”
“Transpierce the mountains Instead of scaling them, excavate the land Instead of straltlng It, bore holes In space Instead of keeping It smooth, turn the earth Into swIss cheese.”
Gllles Deleuze and Felix Guattari. A Thousand Plateaus

cyborg manifestos

For years, decades, centuries, it seemed as though women were lagging behind the front runners of the human race, struggling to win the rights attained by men, suffering for want of the status which full membership of the species would supposedly have given them. And as long as human was the only thing to be, women have had litde option but to pursue the possibility of gaining frill membership of the species “with a view to winning back their own organism, their own history, their own subjectivity.” But this is a strategy which “does not function without drying up a spring or stopping a flow.” And there are processes of parallel emergence, noncausal connections and simultaneous developments which suggest that sexual relations continually shift in sympathy with changes to the ways many other aspects of the world work. If Simone de Beauvoir’s Second Sex found itself compelled to call for “men and women” to “univocally affirm their brotherhood” in 1949, this was also the point at which the first sex began to find itself subsumed by self-organizing tendencies beyond its ken or its control. By 1969, when Monique Wittig published Les GuirilUres, these tendencies were emerging as networks which didn’t even try to live up to the existing definitions of what it was to be a proper one of anything at all. And by the 1970s, when Luce Iriaaray wrote This Sex Which Is Not One, fluid complexities were giving a world which had once revolved around ones and others a dynamic which obsolesced the possibility of being one of anything at all.
As personal computers, samplers, and cyberpunk narratives proliferated in the mid-1980s, Donna Haraway’s cyborgs.

were writing manifestos of their own. “Bv the late twentieth century,” they declared, “our time, a mythic time, we are all chimeras, theorized and fabricated hybrids of machine and or- ganism; in short, we are all cyborgs.” And while the shiny ^screens of tKeTate twentieth century continued to present themselves as clean-living products of the straight white lines of a peculiarly man-made world, Haraway’s text excited a wave of subversive female enthusiasm for the new networks and machines. In the early 1990s, a cyberfeminist manifesto appeared on an Australian billboard and declared. “The clitoris is a direct
•Wt "
line to the matrix,” a line which refers to both the womb— matrix is the Latin term, just as hystera is the Greek—and the abstract networks of communication which were increasingly assembling themselves.
“Vbu may not encounter ALL NEW GEN as she has many guises. But, do not fear, she Is always In the matrix, an omnipresent Intelligence, anarcho cvber terrorist acting as a virus of the new world disorder."
VNS Matrix

They say she wears “different veils according to the historic period.” They say her “original attributes and epithets were so numerous ... in the hieroglyphics she is called ‘the many- named,’ ‘the thousand-named’ . . . ‘the myriad-named.’ ” They say, “the future is unmanned.” They say, “let those who call for a new language first learn violence. They say, let those who want to change the world first seize all the rifles. They say that they are starting from zero. They say that a new world is beginning.” They say, “if machines, even the machines of theory, can arouse themselves, why not women?”

programming language
“It Is already getting around—at what rate? In what contexts? In spite of what resistances?—that women diffuse themselves according to modalities scarcely compatible with the framework of the ruling symbolics. Which doesn't happen without causing some turbulence, we might even say whirlwinds, that ought to be reconfined within solid walls of principle, to keep them from spreading to Infinity ..."
Luce Irlgaray, This Sex Which Is Not One
In May 1979, Commander John D. Cooper came up with a name which the United States Department of Defense’s High Order Language Working Group (HOLWG) could accept for their new programming language: Ada, chosen “in honor of an obscure but talented mathematician, Ada, Countess of Lovelace.” When HOLWG approached the Earl of Lytton, one of Ada’s descendants, for permission to use the name, he “was immediately enthsiasdc about the idea and pointed out that the letters ‘Ada’ stood ‘right in the middle of “radar.”

shuttle systems
There is always a point at which, as Freud admits, “our material—for some incomprehensible reason—becomes far more obscure and foil of gaps.” And, as it happens, Freud’s weaving women had made rather more than a small and debatable con-

tribution to his great narrative of inventions and discoveries. Far more than a big and certain one as well. It is their micro- processes which underlie it all: the spindle and the wheel used in spinning yarn are the basis of all later axles, wheels, and rotations; the interlaced threads of the loom compose the most abstract processes of fabrication. Textiles themselves are very literally the softwares linings of all technology.
String, which has been dated to 20,000 b.c., is thought to be the earliest manufactured thread and crucial to “taking the world to human will and ingenuity,” not least because it is such multipurpose material. It can be used for carrying, holding, tying, and trapping, and has even been described as “the unseen weapon that allowed the human race to conquer the earth. ’ ’ Textiles underlie the great canvases of Western art, and even the materials of writing. Paper now tends to be made from wood, but it too was woven in its early form, produced from the dense interlacing of natural fibers. The Chinese, with whom the production of paper is thought to have begun some 2,000 years ago, used bamboo, rags, and old fishing nets as their basic materials; papyrus, from which the word paper is itself derived, was used in ancient Egypt, and later Arab cultures used the same flax from which linen is produced. Wood pulp gradually took over from the rags which Europe used until the nineteenth century, and most paper is now produced from fibers which are pulped and bleached, washed and dried, and then filtered onto a mesh and compressed into a fine felt.
Evidence of sophisticated textile production dates to 6,000 B.C. in the southeast regions of Europe, and in Hungary there is evidence that warp-weighted looms were producing designs of extraordinary extravagance from at least 5,000 B.c. Archaeological investigations suggest that from at least the fourth millennium B.C. Egyptian women were weaving linen on horizontal

looms, sometimes with some two hundred threads per inch, and capable of producing cloths as wide as nine feet and seventy-five feet long. Circular warps, facilitating the production of seamless tubes for clothing, and tapestry looms, able to weave the dense complications of images visible in weft threads so closely woven as to completely conceal the warps, were also in use in ancient Egypt where, long before individual artisans stamped their work with their own signatures, trademarks and logos were woven in to indicate the workshop in which cloths had been produced. Cloths were used as early currency, and fine linens were as valuable as precious metals and stones. In China, where the spinning wheel is thought to have first turned, sophisticated drawlooms had woven designs which used thousands of different warps at least two and a half thousand years before such machines were developed in the West.
It may be a bare necessity of life, but textiles work always goes far beyond the clothing and shelter of the family. In terms of quality, sophistication, and sheer quantity, the production of textiles always seems to put some kind of surplus in play. The production of “homespun” yarn and doth was one of the first cottage industries, pin money was women’s earliest source of independent cash, and women were selling surplus yarn and cloth and working as small-scale entrepreneurs long before the emergence of factories, organized patterns of trade, and any of the mechanisms which now define the textiles industry. Even when cloths and clothes can be bought off the rack, women continue to absorb themselves in fibrous fabrications.
There is an obsessive, addictive quality to the spinning of yarn and the weaving of cloth; a temptation to get fixated and locked in to processes which run away with themselves and those drawn into them. Even in cultures assumed to be subsistence economies, women who did only as much cooking,

dialling, and childcare as was necessary tended to go into overdrive when it came to spinning and weaving cloth, producing far more than was required to clothe and furnish the family home. With time and raw materials on their hands, even “Neolithic women were investing large amounts of extra time into their textile work, far beyond pure utility,” suggesting that not everything was hand to mouth. These prehistoric weavers seem to have produced cloths of extraordinary complexity, woven with ornate designs far in excess of the brute demand for simple cloth. And wherever this tendency to elaboration emerged, it fed into a continual exploration of new techniques of dyeing, color combination, combing, spinning, and all the complications of weaving itself.
Even in Europe there had been several early and sophisticated innovations. Drawlooms had been developed in the Middle Ages, and while many of Leonardo da Vinci’s “machines for spinning, weaving, twisting hemp, trimming felt, and making needles” were never made, he certainly introduced the flyer and bobbin which brought tension control to the spinning wheel. Unlike “the spinster using the older wheel,” she now “slackened her hold on the yarn to allow it to be wound on to the bobbin as it was being twisted.”
It is often said that Leonardo’s sixteenth-century work anticipated the industrial revolution “in the sense that his ‘machines’ (including tools, musical instruments, and weapons) all aspired toward systemic automation.” But it was his intuition that textiles machines were “more useful, profitable, and perfect than the printing press” which really placed him ahead of his time. If printing had spread across the modern world, textiles led the frantic industrialization of the late eighteenth and early nineteenth centuries. “Like the most humble cultural assets, textiles incessantly moved about, took root in new re-

gions . . The first manufactory was a silk mill on an island in the Derwent near Derby built early in a century which also saw the introduction of the spinning jenny, the water frame, the spinning mule, the flying shuttle, the witches’ loom, and the power loom. A spiral of “inventions in both spinning and weaving (interacting and mutually stimulating) had attracted capital, concentrated labour, increased output and swollen imports and exports.” This was cloth capitalism, a runaway process which quite literally changed the world. In the 1850s, it was said that “if Providence had never planted the cotton shrub those majestic masses of men which stretch, like a living zone, through our central districts, would have felt no existence; and the magic impulse which has been felt ... in every department of national energy, our literature, our laws, our social condition, our political institutions, making us almost a new people, would never have been communicated.” Textiles had not merely changed the world: they seemed to have mutated its occupants as well. ‘‘Almost a new people ...” “I was surprised at the place but more so at the people,” wrote one commentator of Birmingham, the site of the first cotton-spinning mill. “They were a species I had never seen.”
While the industrial revolution is supposed to have made the break between handheld tools and supervised machines, the handmade and the mass-produced, the introduction of technology to more primitive textiles techniques is both a break with the old ways and a continuation of the lines on which the women were already at work. Even before its mechanization, the loom was described as the “most complex human engine of them all,” not least because of the extent to which it “reduced everything to simple actions: the alternate movement of the feet worked the pedals, raising half the threads of the warp and then the other, while the hands threw the shuttle carrying the thread

of the woof.” When John Heathcote, who patented a lace- making machine just after Jacquard built his loom, first saw “a woman working on a pillow, with so many bobbins that it seemed altogether a maze,” his impression was that lace was a “heap of chaotic material.” In an attempt to unravel the mystery, he “drew a thread, which happened to draw for an inch or two longitudinally straight, then started off diagonally. The next drew out straight. Then others drew out in various directions. Out of four threads concurring to make a mesh, two passed one way, the third another and the fourth another still. But at length I found they were in fact used in an orderly manner . . .” It was then a matter of producing “a fabric which was an exact imitation of the thread movements of handmade lace.” This is both the ordering of chaos, and also how its networks replicate themselves.
There were other spin-offs from textiles too. The weaving of complex designs demands far more than one pair of hands, and textiles production tends to be communal, sociable work allowing plenty of occasion for gossip and chat. Weaving was already multimedia: singing, chanting, telling stories, dancing, and playing games as they work, spinsters, weavers, and needle- workers were literally networkers as well. It seems that “the women of prehistoric Europe gathered at one another’s houses to spin, sew, weave, and have fellowship.” Spinning yarns, fabricating fictions, fashioning fashions . . . : the textures of woven cloth functioned as means of communication and information storage long before anything was written down. “How do we know this? From the cloth itself.” This is not only because, like writing and other visual arts,Nveaving is often “used to mark or announce information” and “a imiemonic device to record events and other data.” Textiles doi communicate in terms of the images which appear on the righfside of the cloth,

but this is only the most superficial sense in which they process and store data. Because there is no difference between the process of weaving and the woven design, cloths persist as records of the processes which fed into their production: how many women worked on them, the techniques they used, the skills they employed. The visible pattern is integral to the process which produced it; the program and the pattern are continuous.
Information can be stored in cloth by means of the mean- ingfid messages and images which are later produced by the pen and the paintbrush, but data can also be woven in far more pragmatic and immediate ways. A piece of work so absorbing as a cloth is saturated with the thoughts of the people who produced it, each of whom can flash straight back to whatever they were thinking as they worked. Like Proust’s madeleines, it carries memories of an intensity which completely escapes the written word. Cloths were also woven “to ‘invoke magic’—to protect, to secure fertility and riches, to divine the future, perhaps even to curse,” and in this sense the weaving of spells is far more than a metaphorical device. “The weaver, chose warp threads of red wool for her work, 24 spun one direction, 24 spun the other way. She divided the bunch spun one way into 3 sets of 8, and the other bunch into 4 sets of 6, and alternated them. All this is perhaps perfectly innocent, but ...”
If the weaving of such magical spells gives priority to the process over the completion of a task, this tendency is implicit in the production of all textiles. Stripes and checks are among the most basic of colored and textured designs which can be woven in. Both are implicit in' the grids of the woven cloth itself. Slightly more complex, but equally integral to the basic web, are the lozenges, or diamonds, still common in weaves across the world. These open diamonds are said to indicate fertility and tend to decorate the aprons, skirts, and belts which

are themselves supposed to be the earliest forms of clothing. “These lozenges, usually with little curly hooks around the edge, rather graphically, if schematically, represent a woman’s vulva.” These images are quite unlike those which are later painted on the canvas or written on the page. The lozenge is emergent from the cloth, diagonal lines implicit in the grids of the weave. And even the most ornate and complex of woven designs retains this connection to the warps and wefts. When images are later painted, or written in the form of words on a page, patterns are imposed on the passive backdrop provided by the canvas or the page. But textile images are never imposed on the surface of the cloth: their patterns are always emergent from an active matrix, implicit in a web which makes them immanent to the processes from which they emerge.
As the frantic activities of generations of spinsters and weaving women makes abundandy clear, nothing stops when a particular piece of work has been finished off. Even when magical connections are not explicidy invoked, the finished cloth, unlike the finished painting or the text, is almost incidental in relation to the processes of its production. The only incentive to cast off seems to be the chance completion provides to start again, throw another shutde, cast another spell.
As writing and other visual arts became the privileged bearers of memory and messages, weaving withdrew into its own screens. Both canvases and paper reduce the complexities of weaving to raw materials on which images and signs are imposed: the cloths from which woven patterns once emerged now become backcloths, passive matrices on which images are imposed and interpreted as if from on high. Images are no longer carried in the weave, but imprinted on its surface by the pens and brushes with which shutdes become superficial carriers of threads. Guided by the hand-eye coordinations of what

are now their male creators, patterns become as individuated and unique as their artists and authors. And whereas the weave was once both the process and the product, the woven stuff, images are now separated out from matrices to which they had been immanent. The artist sees only the surface of a web which is covered as he works; the paper on which authors now look down has no say in the writing it supports.
The processes themselves become dematerialized as myths, legends, and metaphors. Ariadne’s thread, and the famous contest in which the divine Athena tore mortal Arachne’s weaving into shreds, are among the many mythical associations between women and webs, spinsters and spiders, spinning yarns and storylines. For the Greeks, the Fates, the Moirai, were three spinsters—Klotho, Lachesis, and Atropos—who produced, allotted, and broke the delicate contingency of the thread of life. In the folktales of Europe, spindles become magic wands, Fates become fairies, and women are abandoned or rescued from impossible spinning and weaving tasks by supernatural entities, godmothers and crones who transform piles of flax into fine linen by means more magical than weaving itself, as in “Rum- pelstiltskin,” “The Three Spinsters,” and “The Sleeping Beauty.” “European folktales are frill of references to the making of magical garments, especially girdles, in which the magic seems to be inherent in the weaving, not merely in special decoration.”
As for the fabrics which persist: evaluated in these visual terms, their checks and diagonals, diamonds and stripes become insignificant matters of repeating detail. This is why Freud had gazed at work which was so literally imperceptible to him. Struggling only to interpret the surface effects of Anna’s work as though he was looking at a painting or a text, the process of weaving eluded him: out of sight, out of mind, out of his world.

This was a process of disarmament which automation should have made complete. But if textiles appear to lose touch with their weaving spells and spans of time, they also continue to fabricate the very screens with which they are concealed.
And because these are processes, they keep processing. “Behind the screen of representation,” weaving wends its way through even the media which supplant it. While paper has lost its associations with the woven fabrics with which it began, there are remnants of weaving in all writing: yarns continue to be spun, texts are still abbreviated textiles, and even grammar— glamor—and spelling retain an occult connectivity. Silkscreens, printing presses, stencils, photographic processes, and typewriters: by the end of the nineteenth century images, texts, and patterns of all kinds were being processed by machines which 6
still used matrices as means to their ends, but also repeated the 9
repeating patterns downgraded by the one-off work of art. And while all these modes of printing were taking technologies of representation to new heights, they were also moving on to the matrices of times in which these imprinting procedures would reconnect with the tactile depth of woven cloth.

casting on
Spinning is “a perilous craft” wrote Mircea Eliade. “The moon ‘spins’ Time and ‘weaves’ human lives. The Goddesses of Destiny are spinners.” When he looks at the seclusion of pubescent girls and menstruating women, often the occasion for the spinning of both actual and fictional yarns, he detects “an occult connection between the conception of the periodical creations of the world . . . and the ideas of Time and Destiny, on the

aa

one hand, and on the other, nocturnal work, women’s work, which has to be performed far horn the light of the sun and almost in secret. In some cultures, after the seclusion of the girls is ended they continue to meet in some old woman’s house to spin together.” And wherever spinning is ubiquitous, there is often “a permanent tension, and even conflict, between the groups of young spinning girls and the men’s secret societies. At night the men and their gods attack the spinning girls and destroy not only their work, but also their shuttles and weaving apparatus.”
If the psychoanalysts provide the only accounts of hysteria, the only records of the witch-hunting which swept three centuries of premodern society are written by the hunters and from their point of view. “The voices of the accused reach us strangled, altered, distorted; in many cases, they haven’t reached us at all.” What “really happened” has left the scene. Historians of witchcraft “have implicitly or explicidy derived the subject of their research from the interpretative categories of the demonologists, the judges or witnesses against the accused,” and “with very few exceptions,” most scholarly studies “have continued to concentrate almost exclusively on persecution, giving litde or no attention to the attitudes and behaviour of the persecuted.” Even feminist scholars have endorsed this approach. “Clearly,” writes Mary Daly of those on trial, “the supposed sexual fantasies of these women were (are) archetypi- cally male fantasies,” and the accused were nothing more than “projection screens for these hallucinations.”
If everything remaining of the witch cults is circumscribed by those who define and prosecute their crimes, anyone “declining to restrict himself to recording the results of this historical violence can find fragments, relatively immune from distortions, of the culture that the persecution set out to eradicate.”

The prosecution evidence is riddled with gaps: there are holes in the stories, twists to the plots. “Hence—for anyone unresigned to writing history for the nth time from the standpoint of the victors—the importance of the anomalies, the cracks that occasionally (albeit very rarely) appear in the documentation, undermining its coherence.”
All God’s children could be led astray, and many men met their deaths at the stake. As in the case of hysteria, the witches were not necessarily male. Persecutors testified to “the existence of an actual sect of female and male witches,” who “met at night, generally in solitary, places, in fields or on mountains. Sometimes, having anointed their bodies, they flew, arriving astride poles and broom sticks; sometimes they arrived on the backs of animals, or transformed into animals themselves . . .” But the Malleus Malejkarum, a fifteenth-century witch-hunter’s guide, also reported that “a greater number of witches is found in the fragile feminine sex than among men.” It argued that women were particularly predisposed to an “addiction to witchcraft” and considered them “to be of a different nature from men,” especially “as regards intellect, or the understanding of spiritual things.” Women were said to have “weak memories,” so that “it is a natural vice in them not to be disciplined, but to follow their own impulses without any sense of what is due; this is her whole study, and all that she keeps in her memory.”
The hunters tied themselves in terrible knots in an attempt to prove both that the witches’ activities were real enough to merit the prosecutions, and also that they were simply fantasies. “It cannot be admitted as true that certain wicked women, perverted by Satan and seduced by the illusions and phantasms of devils, do actually, as they believe and profess, ride in the night-time on certain beasts with Diana a goddess of the Pagans,

or with Herodias and an innumerable multitude of women, and in the untimely silence of night pass over immense tracts of land, and have to obey her in all things as their Mistress, etc.” Flight was simply a delusion: the witches didn’t really get in touch with the “innumerable multitude of women” they thought they met. They have believed they went hunting with Diana, Artemis, the Amazon queen, but it was all in the mind, it wasn’t happening. “Awakening from sleep, she began a long raving story of crossing seas and mountains, and she brought forth false responses. We denied her story, but she insisted upon it.” But, on the other hand, it was this tendency to ascribe the witches’ activities to “imagination and illusion” which also suggested that “they were really harmless.” And according to the Malleus Maleficarum, “For this reason many witches remain unpunished, to the great dispraise of the Creator, and to their own most heavy increase.”
“Under the repeated play of movement In the fingers a membrane grows between teem that seems to JoTn teem, then prolong them, until eventually It extends beyond the hand and descends along the mm. It grows. It lengthens. It gives the women a sort of wing on either side of their body. When they resemble giant bate, with transparent wings, one of teem comes up and, taking a kind of scissors from her belt, hastily divides tee two great flaps of silk. The fingers Immediately recommence their movement."

Monique Wlttlg, Les GuSrtlleres

flight
Ada Lovelace loved all forms of communication. She sometimes wrote several letters each day, and much of her surviving writing survives in this form. “Think what a delight,” she wrote in a letter when she learned that the electrical telegraph was coming to town in 1844. “Wheatstone says that sometimes friends hold conversations from one terminus to the other; that one can send for anyone to speak to one . . . Wonderful agent and invention!”
At the age of twelve she had entertained hopes of “writing a book of Flyology illustrated with plates,” and told her mother she would “be able to fly about with all your letters and messages and shall be able to carry them with much more speed than the post or any other terrestrial contrivances and to make the thing quite complete a part of the flying accoutrement shall be a letter bag, a small compass & a map which the two last articles will enable me to cut across the country by the most direct road without minding either mountains, hills, valleys, rivers, lakes &c, &c, &c. My book of Flyology shall contain a list of the advantages resulting from flying and it shall also contain a complete explanation of the anatomy of a bird.” Ada had plans to build her wings from paper or silk stiffened with wire, and also imagined “a thing in the shape of a horse with a steamengine in the inside so contrived as to move an immense pair of wings, fixed on the outside of the horse, in such a manner as to carry it up into the air while a person sits on its back.”

virtual aliens
"They speak together of the threat they have constituted towards authority, they tell how they were burned on pyres to prevent them from assembling In future.”
Monique Wlttig, Les GuSrilleres
The “overwhelming majority of electronics assembly jobs are occupied by young female workers on relatively low wages. In this respect, there are clear parallels with the situation in the textiles and clothing industries . . Most of these women do “assembly, the bonding of hair-thin wires to semiconductor chips, and the associated packaging. Though the work requires good eyesight and dexterity, little training is required . . .” Silicon Valley, Silicon Glen, Bangalore, Jakarta, Seoul, and Taipei provide dispersed networks of what U.S. multinationals call “virtual aliens” to fabricate the wafers, assemble the circuits, set up the keyboards and the screens, make the chips that make the chips that turn the computers on. They work in the global factory of the new transnationals: “On the west coast, Filipinas, Thais, Samoans, Mexicans and Vietnamese have made the electronics assembly line a microcosm of the global production process.”
Microprocessing has always been low status, poorly paid, sometimes dangerous. The terms and conditions of life in the factories and offices may be the smallest of improvements on those of compulsory service in the home. To those who already have room of their own, such moves seem paltry when compared to the rhetoric with which rights are declared and equal-

ity is sought. But these infiltrations won their spaces too. The work of these virtual aliens is the latest in the long and twisted line of microprocesses which emerge from a tangle of telephone lines, dials, operators, cables, tones, switches, and plugs; the keys, carriages, and cases of typewriters; the punched-card programs of calculators, pianolas, and looms; flying shuttles, spinning wheels. If she hasn’t had a hand in anything, her fingerprints are everywhere.
Left and right, base and superstructure, proletariat and bourgeoisie: like every reproductive system, industrial capitalism was itself supposed to function along the clear-cut binary lines. Often to the great detriment of the working class, the antagonism between forces and modes of production has been played out as a personal argument between the men: a matter of political consciousness, a struggle between bosses and workers, firms and unions, states and revolutionary cells. Organized and organizing factions have confronted each other as two sides of a split identity struggling to reconcile itself in some great climactic moment of revolution, and theories, critiques, and statistics have concentrated on male employment and the fate of the male worker who, together with modern capitalism and its critiques, has been largely engaged in matters of hand-eye coordination. Manual work and man’s work have been more or less synonymous, both for the workers—hired hands required to work with their hands, hand tools, handles, and other hand-size components—and the bosses—the ones who manage and manipulate the manufactories, and assume it’s all in their hands. This is the binary machine again: two hands and two sides of a game which is supposed to be conducted by another single hand: the invisible hand of capital, perfectly integrated with the supervising eye of the state.
Women, either their own or the proletariat’s proletarians,

as Engels called them, have been the least of the bourgeoisie’s concerns. Immersed in the low-status microprocesses of textiles production, secretarial work, and the production of miniature components, women are supposed to be the most inconspicuous and insignificant of cogs in the wheels of industry. Women have been off the productive map, out of the dialectical loop: no desire, no agency, not even the alienation of the male worker. Kept apart by the demands of home work, housework, and heterosexual monogamy, the women couldn’t get together to organize themselves after the fashion of the men. But for all the instabilities and crises it induced, the industrial proletariat was never the only carrier of revolutionary change, if it was ever such a thing at all. Perhaps its campaigns even served to distract bourgeois man from the really dangerous guerrillas in his midst, those apparently inconspicuous, well-behaved litde creatures who spent their time making lists, detailing procedures, typing, sorting, coding, folding, switching, transmitting, receiving, wrapping, packaging, licking the envelopes, fingers in the dll.
Women, children, and migrant workers have always been poorly paid, last in, and first out, a reservoir of labor which can be brought on stream as required. They are brought into the factories, the mills, and the new bureaucracies only in response to the demands of booming or war economies, and always under the strict supervision of their male superiors. Both the bosses and the male workers ensure that they are kept away from the important jobs. Managers treat them just like the men, only worse: they are paid, but they are paid less; their work is valued but not as highly as that of their male counterparts. As for their coworkers, the line adopted by America’s late-nineteenth century tobacco unions has been repeated time and again: “we have combated from its incipiency the movement of the introduction

of female labor in any capacity whatever,” they declared. “We can not drive the females out of the trade, but we can restrict this daily quota of labor through factory laws.”

cocoons
It has long been assumed in the Western world that technologies are basically tools, means to ends decided in advance by those who make them and put them to use. Whatever the particular purposes for which they are designed and employed, the overriding rationale has always been the effort to secure and extend the powers of those whose interests they are supposed to serve. And their interests have in turn been defined as the exercise of control over something variously defined as nature, the natural, the rest of the world. This crude model of the user and the used has legitimized the scientific projects, colonial adventures, sexual relations, and even the artistic endeavors of the modern world. It continues to inform the deployment of even the most complex machines.
But both man and his tools exist “only in relation to the interminglings they make possible or that make them possible.” The user and the used are merely the perceptible elements, the identifiable components which are thrown up by—and serve also to contain—far more complex processes. The weaver and the loom, the surfer and the Net: none of them are anything without the engineerings which they both capture and perpetuate.
These are processes which mock all grandiose attempts to name names and identify great moments of invention and dis-

covery. It is, as Braudel points out, “patient and monotonous efforts” which lead machinery on. Technical development is not only a matter of “the brisk changes we are a little too quick to label revolutions,” he writes, “but also the slow improvements in processes and tools . . . those innumberable actions which certainly have no innovating significance but which are the fruit of accumulating knowledge: the sailor fixing his ropes, the miner digging his gallery, the peasant behind his plough, the smith at his anvil.” These are the artisans, technicians, engineers whose work is more akin to “a collection of recipes drawn from craftsmen’s experience” than a tale of steady progress to some well-established end, and has “somehow or other evolved unhurriedly” by means of its own peculiar trials and errors, improvisations and accidents. .Until the publication oPBernard Forest’s The Engineer's Pocket Handbookjn 1755, engineering didn’t even have a name, and it has never quite found its place within the modern disciplines of sciences and arts.
While it dates from the engines of the mechanical age, engineering is not confined to the use and manufacture of machines in factories dedicated to the task. As its subsequent associations with electronics, chemicals, software, and genetics imply, it was merely passing through the tools and devices of the mechanical age. Nor is it a process which began at this point: engineering may have been newly defined among the levers, cogs, and automata of the eighteenth century, but the line on which it runs was not invented here.
Engineering travels on experimental routes which throw back to the skills of lost shamanic cultures, the trials and errors of alchemy, and brews condemned for witchcraft in the centuries before the Enlightenment. When Freud wrote his essay on Leonardo da Vinci, often said to be the West’s first engineer, it was not his ability to capture “the essence of femininity” in his

art which really interested the psychoanalyst. Even Leonardo’s penchant for hermaphrodites and the charges of homosexuality which, unlike a later engineer, he successfully denied, did not hold as much fascination as what Freud defines as his “alien interest—in experimentation.” This brought him “close to the despised alchemists, in whose laboratories experimental research had found some refuge at least in those unfavourable times.”
Such Renaissance hackers were on lines of enquiry entirely at odds with the Catholic Church. The “work of the ‘perspectors’ was still a matter of curiosity and artistic innovation” through the sixteenth and seventeenth centuries, and even subsequent engineering carries traces of these earlier, darker paths. In spite of the triumphs of the Victorian engineers, they 7 were still considered to have dirty hands. Pragmatism and tech- 9 nical skill were poor relations to the supposed creativity of sciences and arts, and the status of engineers fell far short of that accorded to those whose theories and visions they followed through. Engineers are not the authors of anything, but simply technicians and caretakers, carrying out instructions written elsewhere and looking after the machines entrusted to their care.
If they were never the masters of their destiny, engineers also do a great deal more than simply following orders from above. They may pay homage to the scientists and deliver their goods to the state, but “even today ‘wildcat’ activities of technical invention, sometimes related to bricolage, still go on outside the imperatives of scientific argumentation” and quite regardless Of social demand. This is not a straight but an “eccentric science,” wandering in its own queer streets and using “a hydraulic model, rather than being a theory of solids treating fluids as a special case.” It does not seek new theories, but new problems,

and emphasizes “becoming and heterogeneity, as opposed to the stable, the eternal, the identical, the constant.” And if both the sciences and the arts separate their authors from their instruments, engineering always remains embroiled in the entanglements of machines.
This is the diagonal route which feels a way through the binaries of one and the other, master and slave. Those who pick up on it are neither in charge of their materials nor are their materials enslaved to them. Neither random nor deliberate, this is a diagonal route, “determined in such a way as to follow a flow of matter, a machinic phylum,” a line which is “materiality, natural or artificial, and both simultaneously; it is matter in movement, in flux, in variation, matter as a conveyor of singularities and traits of expression. This has obvious consequences: namely, this matter-flow can only be followed. Doubtless, the operation that consists in following can be carried out in one pace: an artisan who planes follows the wood, the fibres of the wood, without changing location. But this way of following is only one particular sequence in a more general process. For artisans are obliged to follow in another way as well, in other words, to go find the wood where it lies, and to find the wood with the right kind of fibres . . .” They are “intuition in action.”
Culture and nature are scrambled with these inter- minglings. When sun-dried fibers are spun by hand, the spinsters’ fingers and the spinning wheel follow a trend set by the way the plants have already curled and died. When weavers interlace their threads, they jump into the middle of techniques which have already emerged among tangled lianas, interwoven leaves, twisted stems, bacterial mats, birds’ nes«.s and spiders’ webs, matted fleeces, fibers, and furs. When the silkworm goddess, variously known as Lei Zu and Lady Hsi-Ling, and said to

be the first sericulturist, farmed the worms and put their threads to human use, she too was prolonging the processes with which they were already weaving their cocoons. Folding, plying, multiplying threads: plaiting, weaving, and the spinning they imply draw on threads which are already assembling themselves in ways which far exceed any of Freud’s fantasies about his daughter’s pubic hair. And if Freud thought there was only one step involved in “making the threads adhere to one another,” the processes are rather more complex.
Long before the weaving can begin, threads must be combed and spun, plied and dyed, and measured out before they are wound onto the back beam, and through the rattle, or tension box. Stretched to the right tension, each warp thread must then be passed through the eyes of the heddles, the string or metal loops; then drawn between the harnesses; slayed through the dents in the reads; bunched, and finally tied to the apron. Combinations of color and texture must be worked out in advance: the order of the warps must be exacdy right, and the lifting sequence perfecdy prepared. Shutdes must be loaded with what might be a thousand different colors and threads, and the order of their traverse must be arranged. Only now can they begin to fly.
If this is the beginning of the process, everything is also over at this point. All the weaver now has to do is run the program woven in advance. The patterns are already as good as made. The fabrication might as well already be complete. The softwares are virtually real.

diagrams

Just before the outbreak of the Second World War, Alan Turing published a theoretical model of a machine which was to constitute the basis of all postwar computing. With a tape drive and a. computation unit, this hypothetical, abstract machine was capable of reading, erasing, and writing digits on a single line of type. It processed zeros and ones on a tape of infinite length which passed through the drive, and followed a series of basic commands.

The Turing machine
0
i
Config
i
move right config 1
move right config 2
Config
2
write 1 move right config 3
move right config 2
Config
3
move left config 4
move right config 3
Config
4
no move config 4
erase no move config 4
﻿AIxDesign is an independent community of practice, where we gather to delve into the intersection of Design and AI/ML/Data. We believe that AI is for everyone, and we work with the following principles:
Creativity over productivity – co-creating with AI to connect with our human selves
Democratizing access – all our events are accessible for beginners and don’t require expensive hardware
Making and learning with joy


In 2022 we are running four programs. Our programs are aimed at creating a welcoming space for exploration of AI/ML tools by and for independent creatives, exploring the use of AI for social good and introducing feminist and humanist perspective and methods


AI Playground
AI Playground, led by Computational Mama is an event series to play, build, craft and experiment with Creative AI tools and code. The series is divided in four themes: Image, Text, Music and Body. Through hands-on workshops and artist talks, this series invites creatives to experiment, offering the first steps into adopting AI into your creative practice.
Bottom-up Data Activism
Bottom-up Data Activism with Abdelrahman Hassan is a series of workshops where we engage in critiques of data-driven processes. We will focus on concepts of agency, privacy, and anti-hegemony, with an intersectional approach!⁠
Mapping AIxDesign landscape
In the research aspect of our practice, we are busy mapping the entry points and professional opportunities in the field of AI & Design. The research will produce a podcast series, and an illustrated depiction of the ecosystem, outlining overlaps, opportunities, challenges, key players and organizations, tools, and subjects covered in academic papers.
AI Tooling
AI Tooling led by Yasmin Morgan is a program where we experiment with generative design and AI tools to support our own creative process and content creation. The results are implemented into our own communication channels, and we will be sharing the messy and fun process of arriving there.⁠
After successfully funding through Kickstarter and your help, the AI Ideation Cards are now also available in our shop!
1. Get lost in the AIxDesign Resource Library
A constantly-evolving database of resources for designers and creatives embarking on their AI journey. Including articles, online courses, code snippets, job opportunities, and much more!
2. Attend one of our upcoming events
We host keynotes and workshops on a regular basis. Mapping the AIxDesign landscape one session at a time, they range from accessible coding workshops to keynotes about the role of UX designers and tools for applied ethics to informal networking spaces. Join us to see for yourself.
3. Download the AI meets Design toolkit
The predecessor of this community, the AI meets Design toolkit is a set of tools to support designers in developing AI products & services. It was created by Nadia Piet in 2019 and you can download it here!
4. Sign up for our monthly newsletter
We send out a monthly newsletter packed with resources & community updates to 1100+ curious minds just like you. Click here to browse the archive, or sign up in the footer below to receive the next edition in your inbox.
5. Join the AIxDesign community on Slack
Our Slack space brings together ±210 practitioners in the field. To join us, you can fill the signup form here. It will send you an invite link once you submit (please check your SPAM folder as well). See you in there!
6. Help us make it all happen
We hope to be a platform for others and invite you to get involved. We’re open to any ideas you might have about how to contribute. If you’re not sure, this page can help.


Collaboration is at the heart of what we do. Whether you’re an individual looking to contribute, an expert looking to speak or join forces on a project, an organization looking for talent, or other offerings, we’re here for it. Please check below for more info, or reach out to talk.

Recl(AI)ming Pleasure through F(AI)lure
The tools we use to create narratives with AI, furthering and expanding our human language, are modern in their performance but simultaneously stuck in the past carrying our ingrained biases and stereotyping. How can one reclaim agency and fight biases with (AI) storytelling?
In their work, Emily Martinez focuses on critically looking, un/learning, un/making, speaking and dreaming about queer imaginings and AI. During the behind-the-scenes talk, Emily will walk us through their projects such as Queer AI, Ultimate Fantasy, and Unsupervised Pleasures.
The talk is open to curious individuals from all fields and disciplines.
This event is part of AI Playground by AIxDesign. AI Playground is a series of events where we want to showcase the most interesting, forward-thinking and mind-blowing ways to use AI&ML tools in creative practice. The goal of these sessions is to inspire and activate creatives in a playful and accessible way to take the first step into exploring and adopting AI methods into their toolkit.
This event is part of AI Playground program led by Computational Mama a.k.a. Ambika and funded by Stimuleringsfonds Creative Industrie.
The talk will take around 60 minutes including the Q&A and discussions, and you will need to bring:
-a reasonable internet access
-An open mind :) <3
(The talk will happen on Gathertown and will be recorded for future reference and documentation. )
About the Artist
Emily Martinez (they/she) is a new media artist working with machine learning, queer technologies, new economies, and consensual tech. They are a 1st generation immigrant/refugee (Cuba > Miami) and a self-taught coder who believes in the tactical misuse of technology. Emily's work has been exhibited internationally, mostly as a collaborator with Anxious to Make and Queer AI. Their latest project, Unsupervised Pleasures is a DIY community library and practice space that uses queer methodologies, glitch feminism, decolonial and other non-normative frameworks to un/make things with AI. When Emily is not working, they are learning to love and doing their energy work.
About AIxDesign
AIxDesign is a studio and global network exploring & shaping practices emerging at the intersection of Design and AI/ML/Data. Embracing industry, academic, and artistic approaches equally, our goals are to invite critical & creative approaches to AI, make it more accessible, and collectively unpack and uphold what it means to develop AI systems in line with human values. For more info about our work, have a look at our website at aixdesign.co.


Thinking of AI, we are drawn to its potential when it comes to imagining our future. But in the creative uses of this technology, AI is as powerful to help us reminisce and create new memories and narratives of our past. In their work, interdisciplinary artist Aarati Akkapeddi. does just that – using AI to work with the subjects of family, memory and childhood, the artist creates new memories and a space for reflection. During their workshop, Aarati will introduce the workshop and take us on an immersive and exploratory deep dive into working with AI and images. In this hands-on workshop, we will understand how to combine AI and ML with our personal image archives and watch new memories emerge.
The workshop is open to curious individuals from all fields and disciplines and requires no specific technical skills.
This event is part of AI Playground by AIxDesign. AI Playground is a series of events where we want to showcase the most interesting, forward-thinking and mind-blowing ways to use AI&ML tools in creative practice. The goal of these sessions is to inspire and activate creatives in a playful and accessible way to take the first step into exploring and adopting AI methods into their toolkit.
This event is part of AI Playground program led by Computational Mama a.k.a. Ambika and funded by Stimuleringsfonds Creative Industrie.
About the Workshop Flow
Does the machine understand the human face as an average of many faces? What does it mean to build a collage with AI? Can your photo gallery be used as a dataset? The very beginning of exploring AI as a creative tool is understanding the importance of datasets. Instead of struggling with “big data” we will learn to build tiny datasets. With the tiny image datasets, we’ll begin our journey of algorithm based creative explorations.
The Good Old D(AI)s workshop will get you comfortable with:
-redefining what an image dataset can mean to a creative practitioner
-running google colab with small python scripts to realign your image dataset based on a AI facial recognition algorithm
-developing a portrait of averaged faces using your dataset or others available freely on the internet
The workshop will take 120 minutes and you will need to bring:
-10-20 portrait images with humans in them from your own collection of photos or from a copyright free resource. While the algorithm is meant for facial recognition there is no restriction or compulsion to come in to the workshop with other images
-All images should be less than 1mb and around 500kb is ideal.
-Join on a computer with reasonable internet access
-An open mind :) <3
(The workshop will happen on Gathertown and will be recorded for future reference and documentation. )
About the Artist
Aarati Akkapeddi is a coder, interdisciplinary artist and educator based in Lenapehoking (Brooklyn, NY). They work for The Experimental Humanities Collaborative Network, where they create digital spaces and tools. In their creative practice, they combine archival material, code, machine learning and analog techniques (photography & printmaking) to create artwork about intergenerational/collective memory. They currently teach creative coding in the Design & Technology department at Parsons. Learn more about Aarati's work on aarati.me/.


Learn about the exciting world of whales and how the Whales for Climate team is shining a light on the urgent environment crisis through AI, Generative Adversarial Networks, and new ways of generative image making.
This event is part of the AI Playground program led by Computational Mama a.k.a. Ambika.


Have you been thinking about playing with AI?
Have you tried learning AI but quit because it seemed too complex?
Would like to learn and explore AI in simple friendly formats?
Have you been exploring and playing with AI tools?
Has AI become part of your creative side projects?
Have you been able to convince your clients to play with it?
Have you convinced your friends to play with AI?
**Bring your own (AI) toys will be a quick, convivial and fun free play session. Come with your toys, share your joys, build things together, consider friendships over collaborations.**


What do AI and design have to do with each other? How may designers contribute to the trajectory? How may we claim AI as a creative material? Where might we put ourselves – jobs, schools, communities – if we want to commit to exploring these questions?
With the goal to put on paper the broad scope of practices that lay on the intersection of AI and Design, we are inviting you to join our generative workshop and ponder these questions together 💭 In our exploration, we will be making a map to create a variety of entry points for curious individuals and collectives who are interested in getting to know this area and exploring professional opportunities it offers. Join us in our first session to collectively start roaming and mapping the AI & Design landscape!
This event is hybrid, running both online and at Digital Society School in Amsterdam. If you wish to participate physically, please RSVP hello@aixdesign.co.


Our community hangouts are a space to come meet other practitioners at the intersection of AI and Design and we're hosting our next on the 14th of December!
We'll kick off with a round of speed networking. We'll randomise 1-to-1 meetings of 5 minutes each. No pressure, good times, 10/10 Would recommend.
From there on, simply come-as-you-are. We'll open up gather.town for you to explore and interact as you'd like. Continue conversations from your 1:1s or gather around our virtual gallery to meet people interested in the same things as you.
The room will be hosted by the inspiring Ploipailin Flynn: business strategist, digital right activist, founder of antiracistby.design, and long-term AIxDesign member.
Sign up here through Eventbrite and we'll send you a message with everything you need. See you then!


About the Session
ML5.js is an open-source and user-friendly library that aims to make machine learning approachable for designers, artists, and other members of the creative community.
In this session, we will train a custom Neural Network to recognize and respond to a body pose of your choice. This can be a dance move, a yoga posture, or any other body pose or gesture you'd like to play with! We will be using the ML5.js library, the PoseNet model, and Glitch to create a user interface.
This session is the second of two introductory workshops to ML5.js. While coding experience is not required, a basic understanding of Javascript will be helpful.
About the Speaker
Erik Katerborg is a creative technologist and lecturer in creative media at the Rotterdam University for Applied Sciences. He is especially interested in making technology and coding more accessible to people with a creative or design background. Learn more about Erik’s work on his Linkedin.
Now more than ever, advanced technologies are being put into practice around us. Evolving technologies have brought about a change in human behaviour when interacting with AI. For example, certain products are shaped in a certain way around us to accommodate certain technologies. This is also shaped the interface delivering such technology (Think: Google Home or Fitbit).
In this talk, Sofie will dive into the risks, opportunities and application areas of how design and UX shape human decision making when it comes to AI and products that harness the capabilities of AI.
Human behavior results not only from intentions and deliberate decisions, but also from its interaction with technological artifacts. She will also touch upon the basics of captology which is the study of computers as persuasive technologies. Here, persuasion broadly covers behaviour change and change in user motivation brought about by evolving technologies.
About the Speaker
Meet Sofie Nabseth, part of the core team at the AI company Sana Labs.
Over the last years, Sofie has headed Sana Labs’ global marketing activities and expanded their international footprint. From brand and positioning to global AI summits and lead generation, bringing the benefits of AI in learning to tens of thousands of individuals across the globe. Sofie is now responsible for business development, helping organizations in various industries leverage AI in learning.
Sofie is also leading the Swedish chapter of the non-profit Women In AI - helping more girls and women understand and apply AI, decreasing the gender gap in the AI field.
Learn more about Sofie and her work as well as Sana Labs through her LinkedIn.


AIxDesign is a community-led platform exploring and promoting the use of AI/ML for creative purposes, practice-based research and activism.
What is AIxDesign - for the website (up to 5 sentences)
1. At AIxDesign we are celebrating the power of human-machine creativity for social good. Through our hands-on workshops on AI, ML and Data tools and community-led research, we create wacky and imaginative use cases for these technologies. By working with independent creatives and combining hands-on play with a critical approach, we are developing feminist and humanist gatherings, materials and methods.

2. At AIxDesign we focus on the combination of AI, ML and Data tools with designerly and artistic practices, developing wacky and imaginative use cases. We believe that AI is for everyone, and it is our goal to democratize access to AI and expand the typology of an AI-professional. It is our goal to use our creativity, capacity to reflect and the desire to tweak our tools to help create better AI and an inclusive professional community.

3. At AIxDesign we believe that AI is for everyone, and it is our goal to radically democratize access to AI, ML and Data tools with the focus on create uses of these technologies. We want a future where artists, designers, and activists can freely use AI for reasons other than corporate benefit. By working with independent creatives and combining hands-on play with a critical approach, we are developing feminist and humanist gatherings, materials and methods.
4. 5. AIxDesign is a collaborative effort to democratize access to AI&ML tools and computational literacy, with the focus on imaginative and creative use of these technologies. Furthering the use and exploration of AI in non-commercial context, AIxDesign creates learning opportunities and showcases the wide array of practices and professionals working with AI. Joy, and working and learning with joy, is our core value. Our goal is to make AI&ML tools less intimidating and lower the access barrier. Through our non-hierarchical structure and with a team to support a thriving community of like-minded individuals, we work to create joyful encounters between humans and tech and between humans and other humans. Destabilising AI as a fixed construct – with a fixed set of ties to the goal of automation and optimization, big tech and wast amounts of computational power – we want to radically expand the typology of an AI-practitioner. We see a lot of opportunities to decentralize AI and we promote it as a tool everyone can use. While we are intrigued and impressed by all creative uses of AI, be it design or music creation or writing, we currently call ourselves AIxDesign, as we want to use the word ‘design’ as an umbrella term for intentionally using AI/ML as a creative tool and designing creative processes where AI/ML is involved. At AIxDesign, we work with joy, but we also work with difficult subjects. The tools we are working with are not neutral, and we acknowledge the fact that technologies that fascinate us came into existence with built-in biases, discriminating on the basis of race, gender and class. We do our part to work against (algorithmic) discrimination and towards a more just society, and create a comfortable setup for difficult conversations.
THE WHY - Vision
We believe that AI is for everyone, and it is our goal to radically democratize access to AI, ML and Data tools. We want a future where artists, designers, and activists can freely use AI for reasons other than corporate benefit. By taking AI outside of big tech, we want to expand the typology of AI-professional, simultaneously expanding the range of subjects for AI to work with. Outside of chasing productivity and optimization, what goals are out there? AI-technologies have built-in biases and often facilitate injustice. We want to use our creativity, capacity to reflect and the desire to tweak our tools to help create better AI and an inclusive professional community.
THE WHAT - Mission
At AIxDesign we are harnessing the power of human-machine creativity for social good. Through our hands-on workshops on AI, ML and Data tools and community-led research, we create wacky and imaginative use cases for these technologies. By working with independent creatives and combining hands-on play with a critical approach, we are developing feminist and humanist gatherings, materials and methods.
   * We focus on AI, ML and Data and the intersection of these tools with design and creative practices.
   * We engage in field-building. As a nascent field, we aim to both explore, shape, and advocate for practices emerging at the intersection of AI and Design.
   * We produce workshop programs, event series, one-off events and collaborative event production – AIxDesign community is at its best when we gather to play, learn, explore and chat.
   * We gather: We started AIxDesign because we struggled to find like-mindeds to talk to - people that understood and valued both technology, creativity, data, and design. We value our community, and we are working to ensure that every member has the opportunity of networking and professional development.
   * We acknowledge that AI & ML landscape of today is far from being an inclusive space, and we aim to change that by creating a welcoming community, playful learning experiences and accessible formats. AIxDesign started online, and while we are beginning to have events on location we will continue keeping all our events hybrid, for everyone to join.
   * We aim to overcome the ‘techie/non-techie’ binary. Our practice embraces equal parts data and design smarts, valuing industry and academic contributions, and looking to marry both analytical and creative approaches to build meaningful intelligent systems to co-exist with.
   * We are an independent community of practice. When it comes to AI/ML landscape, existing communities of practice live within the big tech and not independently. AIxDesign aims to change that.
THE HOW - Values & Guiding principles
   * Imagination & creativity
   * Plurality: We are, and we always will be, featuring a multitude of perspectives.
   * Joy
   * Inclusive community
   * Practice-based without sacrificing theory: We created AIxDesign partly out of our frustration with existing learning materials on AI & ML tools. While being technically excellent, many don’t introduce the students to complex concepts and problematic history of AI. We wanted to create hands-on learning situations in which it is also possible to learn about those.
   * Not for profit but for gainful employment AIxDesign is a non-profit organization, but by focusing on non-commercial activities we don’t mean that we don’t cover any corporate uses of AI & ML. We are interested in seeing how playful exploration can lead to job opportunities, and we are happy to share job postings for positions where AI & ML skills are valued in our network.

Medium articles
Today’s world generates data at unbelievably rapid rates. It is essential to leverage the available data to understand the bigger picture better. Data Science is changing the world, and user research needs to get on board to understand business and user needs better. User Experience can be a tool that can be essential to frame how data science conveys critical insights. This talk gives an overview of how Data Science can complement UX research, including quantitative and qualitative methods. It introduces the Data Science pipeline and describes useful UX research applications, like identifying users to interview, finding different customer segments, and generating data for usability studies.

This session is the first of our keynote series, and it took place over Zoom. The session was interactive yet educational. For this event, our speaker was Grishma Jena — Data Scientist with the Research Ops team for User Research and Design, Cloud & Data Platform, at IBM in San Francisco. She works across portfolios in conjunction with user research and design teams and uses data to understand users’ struggles and opportunities to enhance their experiences. Grishma kicked off the session by sharing her role at IBM and how we could get involved in their user research program.

The keynote covered relevant topics in Data science, such as data, Machine Learning models, and steps to be taken to incorporate data science into user research. We have summarized the key six takeaways from the keynote below:

1. Addressing Misconceptions
There are common misconceptions around data science and user research. Grishma addressed that the misconceptions are mainly related to practitioners in these fields. User researchers often think of data scientists as a glorified numbers person, and data scientists confuse user researchers for designers and may not be considered important to a product development process. She mapped out her motivation for the talk by sharing what data science can do in user research.
2. Asking the Right Questions
Grishma emphasized asking the right question, i.e., formulate a question the stakeholder is trying to answer. Questions like “Are we offering the right things to the right people?” or “How likely is it the user will buy our product?” aid in finding answers to the problem you’re trying to solve.

After you ask the right question and find corresponding data, the next step is data wrangling or data cleaning. Data wrangling is when you gather, select, and transform data for easy access and analysis. A couple of methods to do this is by scaling or normalizing data, deduplicating records, interpolating values, and standardizing multiple sources.

The next step is data exploration — the initial investigation of data to explore essential variables and how they are distributed. Grishma urges covering any initial patterns or points of interest. This helps form hypotheses about the defined problem.

3. Creating a Meaningful Model
The next step after data exploration is model building. She provided the following steps:

Feature engineering: Select important features and construct more meaningful ones, using domain knowledge
Preparation: Divide the data into training and test sets
Training: Choose supervised or unsupervised learning, Tune model parameters and Monitor against overfitting
Evaluation: Evaluate model on unseen data, i.e., a test set
One relevant use case to the topic is Airbnb. She provided them as an example of doing a tremendous job in integrated data science in user research. Here are a couple of ways Airbnb has managed to do so:

Used data to determine host preferences: Airbnb used supervised learning and classification to answer critical questions like “Would a host accept or decline a booking?” They also found trends about how hosts in small and big cities behaved based on demands and availability of properties.
Reduced bounce rates with a redesign: A data scientist at Airbnb discovered that some Asian countries' bounce rates are relatively high. To fix this issue, they approached UX researchers and decided to show top-selling destinations to ensure users stay on the site. This led to a redesign and a 10% increase in conversions. Here, they used supervised learning and regression.
Showed skewed search results to users: They let users drive the search based on past data (bookings) available to them. They created a model to assess the probability of booking and used it to skew search results. As a result, results showed properties that were most likely to be booked past on past booking trends.
Here is more information about how Airbnb uses Data Science.

Moreover, Grishma discussed model validation, where you assess your model’s quality, where you can use cross-validation for robustness or use metrics like accuracy, precision, recall, F1 score, or confusion matrix.

A good example is ABN AMRO: they want to help their customer service representatives tag better and faster based on the support ticket. They came up with a robust system with a high level of accuracy and precision in tagging/labeling. However, on deployment, they found that the representatives’ time to file support tickets didn’t decrease but instead increased.
This happened because the model landed up presenting the representatives with 20–30 different tags, which led to them spending more time going through the list of tags — this is a case where UX researchers could have helped data scientists produce better results by running some usability tests before implementation. The big takeaway from this is that while a system may look good on paper, it may not solve the problem at hand in person.
4. Telling a Story with Data
The last step of the data science pipeline is data visualization and storytelling. One can tell a story with data to answer the original questions, communicate findings to stakeholders, and humanize the numbers at hand. It uses a combination of narrative, visuals, and data to drive change. Narrative and visuals engage the users, visuals and data enlighten the user while narrative and data explain the data to the users. To learn more about data storytelling, check this link out.

Grishma presented us with Google’s outlook on usability testing as they believe that it is time-consuming and expensive. To solve this, Google AI used deep learning for usability testing to predict the tap-ability of elements.

Lastly, she then presented how Spotify uses data science and user research. Spotify is one of the few companies that have used the combination of Data Science and User Research to find peculiarities of users’ listening habits. In other words, find outliers and use it for their 2016 ad campaign such as the one shown below.
5. Data Scientist + User Researcher = Dream Team
Grishma believes that Data Scientists and User Researchers are better together and are a dream team. As data scientists usually focus on quantitative data and User Researchers on qualitative data, they would bring different perspectives to the table. This will help in blending people and data to get close to the truth. Integrating these teams will provide a holistic understanding of multiple forms of data and mitigate the cons of a single research method alone. Furthermore, these teams will keep biases in check and find correlations that help develop a better hypothesis and hyper-specific personae.

“All data created by people. And all people create data…Today we divorce people from their data, and that gives companies a license to forget about the people behind the data…It allows us to divorce ourselves from the responsibility of what that data can do.”

– Ovetta Sampson, Microsoft

Embracing Fair Practices
The talk was concluded by addressing the most critical topic relevant to data and user research, which is Ethics. All involved in handling data should have an ethical discussion about the way the data is used. She discussed how tech could attack people or be misused by people that have used home assistant systems to gaslight their partners.

She urges to ensure that training data is fair and representative and understanding possible bias sources. It is crucial to ensure fairness over time, especially for the different user groups. It is essential to have a diverse team working in integrated teams to ensure that varied opinions, backgrounds, and thoughts come forward.
Swiping cramps
I’ve been using Tinder for the last 2 years and since my first swipes, I’ve had a hunch that Tinder loads the deck against me to keep me on the app for longer. I promise I’m not salty about not getting enough matches, I just feel I could be getting these matches with less swiping. Currently, I average 9 left swipes (rejection) to 1 right swipe (hopeful acceptance), and would argue I don’t have a specific “type”.

However, a definite side effect of an app that shows you the world and values quantity over quality, is that you develop some conscious and unconscious bias on which way you’re swiping. Whether that would be profiles with sporty photos that irrationally make me feel lazy or profiles with bios that are only used to plug Instagram profiles, there are certain things I always half-consciously swipe left on. All this may sound shallow to non-Tinder users, but online daters will all agree this is the reality of dating in 2020.

So I asked myself:

Why is it that, in an age where Spotify can accurately create playlists based on my music taste and Youtube is able to feed me video after video I will passively consume, Tinder only shows me profiles I want to swipe right on 10% of the time?

It’s important to say here that Tinder does use an algorithm to help curate peoples experience on the app. They claim it helps people match more frequently, get off the app and meet IRL. However, they are very vague on how this algorithm actually works, but they do say that the more you use the app, the more matches you get. Who would’ve guessed?

So, if Tinder has an algorithm that is so focused around matches, why does it show me profiles I don’t want to match with 90% of the time. Is Tinder’s algorithm just bad? Am I more random and spontaneous than I think? Or am I just weirdly picky and hard to please?

Recommendations matter
My hypothesis/conspiracy theory is that Tinder knows who I am likely to swipe right on and rations out those profiles, and in an attempt to gamify the experience, keep me swiping for longer and hook me to their app. A tactic akin to slot machines, video game loot boxes called “variable rate reinforcement”, and one of the widely recognized dark design patterns.

“The player is basically working for reward by making a series of responses, but the rewards are delivered unpredictably… Dopamine cells are most active when there is maximum uncertainty, and the dopamine system responds more to an uncertain reward than the same reward delivered on a predictable basis.”

Dr Luke Clark, director at the Center for Gambling Research at the University of British Columbia

Methods of investigation
To test this hypothesis, I decided to assess whether there was a distinct pattern of my “Matches” compared to my “Left Swipes”.

To work out these patterns, I trained various machine learning models with data from both my “Matches” and random “Left Swipes”. I split them into Pictures, Bios, Music and in the end, the full profile.

Consequently, I tested the generated results made by these models side by side, choosing my favourite of the two options presented each time. If I choose the result generated by the model trained on my “Matches” more often, then it would prove my match preferences were learnable by an AI and hence, Tinder could consistently fill my feed more with people I would like to swipe right on.
Generating (Frankenstein’s) Profile Pictures
Profile pictures are a key part of the Tinder experience — it’s the first thing you see and I would be lying if I didn’t say it’s probably the most important aspect of a profile. Tinder evidently thinks so too, as throughout the apps short history it has continued to increase the size of the profile picture in its UI.

So to generate fake profile pictures, I trained a Stylegan2 Model on RunwayML with 212 images of the faces of my “Left Swipes” and a separate model on 212 “Match” faces, both for 5000 steps.

Due to my small data set, the resulting generated images were very abstract. However, going into the test I was pretty confident I would be easily able to discern which generated images were based on “The Perfect Match” model, based on the colour, composition and frankensteined facial features.

This was not the case — during the test, I only selected “The Perfect Match”-generated picture 5 out of 10 times. Not too promising.
Generating Profile Bios
Bios are a variable part of a Tinder profile: some users writing only their height, others a thesis on their perfect partner and a good proportion of users have no bio at all. For me, a bio is an important part of my swiping decision, but no bio is definitely better than a bad bio. Hence, out of my 212 matches, only 116 had bios. I also found 116 bios from random people I would normally swipe left on. These were used to train the Tensor flow-based Textgenrnn word-level model for 100 steps.

Again, as in the previous step, the generated results were very abstract but definitely recognisable as Tinder bios, even if they made no sense.

During the test, I went for “The Perfect Match”-generated bio 7 out of 10 times. However, 2 were easy to spot because I clearly recognised some generated text from the datasets, so the real result was 5 out of 8.
Generating “Our Song”
The final piece of a Tinder profile is music, as users have the option to add an Anthem song, as well as a feed of their current most-listened artists. For my test, I focused solely on the user’s Anthems as these are more common and they would give me cleaner data points. For me, these anthems are quite important and help to add detail to my picture of someone.

Through brute force, I compiled a playlist on Spotify of the 99 songs from my “Matches” and 99 others from random “Left Swipes”. I then used Spotify’s similar playlist feature via Skiley to generate new playlists. I was worried that the diverse data sets used would lead to very random playlists but the resulting playlist felt very close to the input data.

During the test, I was shown the song name, artist and album artwork but not allowed to listen to the song because the majority of the time this is how I engage with Tinder profile Anthems.

As a result, I picked the “The Perfect Match”-generated song 6 out of 10 times.
The Final Test — Comparing fully generated profiles
For the last part of the test, I compared full profiles of generated photos, bio and anthems. This was most true to life of the test, as the combination of all the different points gave me a really clear picture of a profile. As such, I managed to pick “The Perfect Match” generated profile 8 out of 10 times. However, because of the bios, I again clearly recognised 2 of the generated bios from the data sets — the real result was 6 out of 8.
Conclusion
In the end, I managed to correctly pick “The Perfect Match” generated profile 61.1% of the time. This was all done with very crude models (except Spotify) and tiny data sets. Whereas Tinder would have access to more sophisticated models, a lot more data and real data scientists. Hence I would tentatively say my hypothesis is correct. If Tinder wanted to, I believe they could learn my preferences and show me profiles I would match with at a much more consistent rate.

There are many reasons why I think they choose not to implement a more accurate and user-focused algorithm — perhaps it would ruin the fun of the app or maybe people wouldn’t opt for subscription plans if they found matches so easily. Cynically, I believe they tune their algorithm to balance out the outcomes of Matches, Fun and Profit. They are a company after all, not a benevolent cupid armed with digital arrows.

Closing Thoughts
At the end of the day, I feel my results were quite obvious and entirely predictable. However, I feel that fact I was able to create this experiment and put Tinder on trial with no Machine Learning and very little coding experience, tells us of a new paradigm emerging. One where companies or governments can no longer hide behind or outright blame their algorithms for anti-user features or more importantly discrimination. People (Companies or Governments) make choices about what they want their algorithm to do, and now with new easy to use the software we can check that we agree with their choices.

On a personal level, what I am also taking away from this experiment is a greater understanding of Machine Learning but most importantly a deeper awareness and questioning of my conscious and unconscious biases.
Both 
Nadia Domide
’s and my (yes we’re both called Nadia haha) day-to-day work is focussed on Artificial Intelligence and Design. Intrigued by creative AI practices, we decided to not leave this excitement to client requests and to start experimenting.

In this article, we’ll walk you through the entire process of how we created our first generative AI visuals using Runway ML.

Runway: ML for Artists
Runway ML is a free software making Machine Learning accessible to artists and creatives. They offer the option to (re-)train your own models such as StyleGAN which we were most eager to try out. Around the time we were having these talks, Runway ML put out an open call for their residency program and we decided to apply. While we weren’t chosen — by the time we finished writing our application, we were so excited we decided to do it anyway.

These places do not exist(.com)
After some brainstorming and discussions, different streams of inspiration started colliding. We were really inspired by the parody projects on thispersondoesnotexist.com, such as thisartworkdoesnotexist.com & thisworddoesnotexist.com — so we asked ourselves: Can we create places that don’t exist? Can AI dream up real presentations of unreal places? Can we generate imaginary landscapes? Long story short: yes, we can.

The complete step-by-step process of generating visuals by (re-)training a StyleGAN in Runway ML
1. Data collection
One of the important parts of an ML project is collecting the training data. We started curating a dataset of approximately 3000 images from Google Earth View. These images were displayed on the world’s largest billboard to bring a bit of zen to New York’s hectic Times Square during the holidays. They are both stunning and a well-curated (AKA lazy) dataset to obtain.

2. Picking the right pre-trained model to start with
Runway ML currently offers an easy way to do image synthesis, by using StyleGAN to generate photorealistic images. We chose to start with their available pre-trained model, called Landscapes (see image below).
By doing transfer learning on the StyleGAN model it allowed us to train our algorithm in a shorter time. We preprocessed the images to squared and centered ones, opted for 3000 training steps, and hit the exciting purple ‘Start Training’ button.

3. Training on our dataset
We trained the model for a couple of hours. During this training time, we could observe how the FID (Frechet Inception Distance) score was changing (see image below). This metric simply computes how similar a generated image is from the real ones in the dataset. Having a low value is an indicator that the real data and the generated one have similar characteristics.
4. Generating outputs from the latent space
Ta-da! Once the training was completed, we could use it to generate new images and videos using random points in the latent space.

What is a latent space? It is a magical multi-dimensional hidden space with no meaning, filled with points. The beauty of this space is that the generative model learns to map these points to output images. And a space walk is simply a series of images that show a transition between two or more generated images.

Runway gives you the option to export images and a video of a so-called latent space walk.

Join us in our collective imagination
We’ve generated 120 of these images. Astonished by their beauty and our brains seeking to think up stories, we questioned what we should do with these fictional places. Might these landscapes have value when put in the hands of people? How can you create memories of places you have never been to (and never will)?

If you like the idea of generating imaginary landscapes & assigning meaning to places that don’t exist, please join us in this small experiment of collective imagination.

We’ll start with a shot of our fictional historical landmarks — the Dustry Blue meets Sandy View and the Coral Candy Pickle Grove place — pictured in the images below.
Big thanks to Runway ML, and everyone that has contributed to the open-source files that made this experiment possible. Now go out to craft your own StyleGAN models and imaginaries. Thank you for reading with us!
The next generation of storytelling
How can AI help us discover new ways to create and tell stories?
Explaining AI with visual narratives
I heard a song recently on one of Spotify’s curated “Made for You” playlists, and it hit me that I hadn’t heard that particular song in ages. I used to listen to it constantly and Spotify didn’t even exist back then. How could its algorithm possibly have known it was relevant to me? Yep, that’s a rhetorical question. Surprise, surprise: it isn’t only historical data that feeds algorithms.

So, you might wonder — What other data are being used to predict my interests in music?

That, I’m sorry to say, is a question for engineers building algorithms and training models, but it is a good question. A question I’d like to find an answer to every time that, for example, Netflix recommends me a new show that I end up loving like crazy or, speaking of less frivolous topics when I hear a friend of mine complaining because she did not qualify for a second credit card.
It would be useful to have a behind-the-scenes window to peek at the algorithms making the decisions that impact our lives and see what’s behind the final output we get. In fact, AI is so pervasive in our lives that people don’t always realize when it’s being employed. Often the use of this technology is so subtle that it could be easily hidden behind a simple picture or a phone notification. After all, if you don’t know the rules, how could you possibly understand the game?

Having a clear explanation of the data that has been used to train a model, how the model works, and how it is being deployed is so crucial to gain full control of how AI impacts us. This refers to everyone, but especially to those who are not technically savvy and know very little about how AI works.

To enhance people’s awareness of AI, we have to first explain the meaning of a model’s recommendations and the context surrounding that final output. My background in data journalism and information design inspired me to question the methods we usually employ to communicate the results of models and algorithms.

How can we reimagine the ways we communicate the AI’s outcome?
I don’t know about you — you can probably still sleep at night without having an answer to this — but this question is stuck in my mind.
The curiosity about the inherent complexity of models led me to experiment with novel ways to illuminate the richness of the data provided by AI technologies. For example, the output of a machine-learning model is not only about its final predictions.

Underlying the ML’s predictions is a huge variety of other information that is integral to the final result: the data used for training the model, the understanding of that data lineage and provenance, the bias that might be detected, the new data created over a feature engineering process, the features’ score, and more and more data… This is only a small part of the meta-information that we might think of that underlying the final result of an ML model. And yes, if your mind is about to blow up, you are on the right track, dude. 🤯 🧠

Think of all the algorithms governing our lives and decisions: AI opened up an entirely new world of information that represents the simultaneous combination of multiple processes, tools, and data.

As you might have started guessing, the complexity of a final model’s result is enormous, and we, as humans, need a better way not to only access this information, but to comprehend it. Therefore, understanding how AI works and impacts our lives is crucial. To do so, every time we work with AI, we have to put a genuine effort into unlocking the context of the data behind its outcome.

Many of my projects focus on envisioning new ways to represent the results of AI models and facilitating public understanding of AI. The foundation of these projects is research and experimentation with the use of data storytelling applied to AI practices.
I believe that, without experimenting, we can’t come up with new ideas. Even if sometimes it means working on projects that might look more like a moonshot without any immediate valuable result. It’s by doing and practising that we can advance our knowledge of what works and what doesn’t when it comes to interaction between humans and AI.

This brings me to a project I started last year in an effort to reinvent the content we can create with NLP and NLU’s data. I named this project Interviews Visualized and it was a fun way to combine the art of storytelling, data design, NLP algorithms, and IBM Watson Natural Language Understanding. Let me bring you straight into my workshop to explain what this project was about. I hope it will spark some new ideas for your future work… follow me!
AI reveals the stories hidden in our words
Last year, I happened to run a newsletter for the team I worked for at IBM. While brainstorming the content for the newsletter and designing its layout, I remember wanting to integrate data storytelling into it. Yet the question was: how? Over the years, working with data on a daily basis has become so rooted in my mindset and working process that I often can’t think of any ideas without being driven by data first. I try to include data storytelling in everything I do because there is no better way of revealing the insights hidden in data. However, it wasn’t until I wrote the very first issue of the newsletter that I had the idea of how to incorporate data into it.
As I logged the transcripts of the interviews of the people I’d spoken to, I began to wonder about the actual meaning of their words: which ones were the most relevant, and why? How did those words relate to specific sentences and passages in the text? What was their context, their connotation, and so on… It was at that point that I started writing an NLP algorithm to find answers to my questions in order to reveal the hidden connections existing in the story’s sentences and words.

By extracting additional meaning from the stories I would write, I wanted to provide readers with a second layer of information: all the information they could not see in the text itself. To give readers the ability to see the patterns existing in the text’s words, I combined NLP techniques, IBM Watson NLU and data design to visualize the structure and semantics of the words used in the newsletter. In this way, I transformed the data generated as the output of a model into a compelling visual story.
As soon as I created and applied the first NLP algorithm to the text, I realized how many overlooked patterns existed between the words used by the people I interviewed: NLP illuminated the human side of the words chosen and returned people a new way to explore a written text more in-depth.

Following this concept, I came up with a visual way to represent all this information and attached a legend to every visual story to encourage people to take the time to explore the data and go deeper into the details.
The ultimate purpose of this experiment, which kept going for several weeks with several visual data narratives, was to find new ways to make AI and ML technologies more accessible to a broader audience of non-experts and to create new strategies to communicate the content generated by algorithms and models.

I felt so much potential in this project that I scaled up the initial algorithm that I created by partnering with a fantastic data scientist and colleague of mine, Erika Agostilli, to leverage the capabilities of Watson NLU to allow my visual stories to express even more details and rich context than I could do with my own algorithm.

Scaling up the project in such a way allowed us to create a little engine, which Erika and I called Interviews Processor, that I used to parse the text of each story I published in the newsletter.
Numbers “can help remedy our human fallibilities. What’s easy to forget is that statistics can amplify these fallibilities, too.” —Hannah Fry

It’s not algorithms’, data’s, or numbers’ fault if sometimes human judgment is mistaken. Our mistakes have more to do with how we communicate the numbers provided by algorithms and models: how we manage to communicate the context those numbers relate to, the level of uncertainty they bring with them, how that uncertainty reflects into the world, and how those numbers affect people’s lives.

I believe that we will never even get close to painting the complexity of this AI in its entirety.
But we can try, every time we deal with data, by exerting a genuine effort to connect data to people and explain its richness and complexity as best as we can. Visual data stories are the most powerful means we have to do that.
Together with 
Benjamin Flader
 and 
Harits Abdurrohman
, we did a little AI x Design Community collaborative session to evaluate the state of No-code and Low-code ML tools. This article documents our process, the metrics that we assessed the tools on, and insights we were able to draw from our assessment.

The process of decluttering tools
Everything started with a very long list of no-code tools. We filtered out the ones that we believed were not relevant — for example, if it was a data analytics tool rather than a machine learning tool. Initially, although our focus was on no-code tools, but we also included low-code tools as well. In the end, we ended up with 26 tools altogether.
We defined no-code tools as tools that do not require any coding, but also don't require prior knowledge of programming or software development concepts.

Our next step was to discuss the assessment metrics. We chose coverage of ML process and tool simplicity. In two rounds, we split up to briefly research the tools on our list, and to make sure that the assessment is not based on a sole opinion, there were always at least 2 of us looking into each tool.

After browsing through tool features, documentation, tutorials, and company landing page, we came back together to discuss and assess the tools based on the two metrics we had decided upon. Evaluations of the tools were from first impressions, although we were all familiar with about half the tools and we’ve evaluated from personal experience.
We noted that some of the tools were more process-focused, while some were use-case focused. The ones that focus on the process helps users easily manage the machine learning process or parts of it. The ones that focus on use cases seem to be more "out of the box", allowing for people to quickly train and deploy a model for a specific use case. We classified the tools into these 3 categories of focus: process building, single use case, and multiple use cases.

Aside from tool simplicity, we also assessed how much Data Science knowledge a user would need to have in order to be able to navigate these tools and use it properly (without wanting to throw their computers out the window).

After mapping the tools in a quadrant chart, we analyzed the results of our assessment and grouped the quadrants as Power Tools, Accessibility Promotors, Expert Tools, and Pocket Tools.

Results & Presentation of assessment factors
Coverage of ML process: From data wrangling to model training to deployment, how much of the machine learning process does the tool cover? If the tool is focused on a specific use case, how much of the process is behind the tool's "black box"?
Tool simplicity: How simple is it for a user with minimal to no coding experience to use the tool? Are the results shown by the tool easy to interpret? How is UX/UI? Does it require download or is it plug-and-play?
Tool focus: Is the tool designed for specific use case(s) or is it meant to aid the machine learning development process?
Scale of Data Science knowledge needed: At which level of data science knowledge should the user have to be able to effectively use the tool?
Quadrant groups
Power Tools: Tools found in this quadrant are mostly intended for data scientists and engineers (or those with a good understanding of data science concepts). These tools will often aid users in the ML development process, either by providing in-depth insights or helping to increase productivity.
Accessibility Promotors: Tools here often aim to make AI accessible to the general public.
Expert Tools: If a tool finds itself in this quadrant, then it would be considered underdeveloped for a no-code tool and would most likely be a tool used by industry experts to perform specific tasks in the ML process.
Pocket Tools: These tools are designed with a very specific focus of certain parts of the ML process in mind.
Assessment analysis & Tool recommendations
There is no no-code ML tool that covers both the whole ML process from data collection to integration and is extremely simple to use. All tools have different benefits, but the one that most stood out to us was Obviously AI (that we are proudly announcing that we are hosting our future event with). As it is in an early stage, we'll have to keep an eye on it. We are looking forward to see how these tools will progress!

If you already have a specific use case in mind there are specific tools for one or several use cases available. Try out Teachable Machine for image or audio recognition. It is easy to use but lacks representation of the whole ML process, although it does come close. Similarly, try out Runway ML for image generation.

To get some ideas, check out our other articles in which some of these tools are used:
Lastly, while there are a lot of tools trying to be the tool for everything (like H2O), the market for no-code ML tools is still young and does not provide many options for specific phases of the ML process. There is lots of room here for the development of tools intended for testing, data preparation, or deployment and integration of ML models in other tools.

Overview of assessed tools
AI Builder from Microsoft
AutoML Tables from Google
Azure AutomatedML from Microsoft
BigML
Create ML from Apple
DataRobot
Fiddler
H2O
Lobe
MakeML
Metaranx
mljar
Obviously AI
Orange
PerceptiLabs
RapidMiner
Weka
What-If Tool
Use-case specific tools
Chatfuel for Facebook Messenger (chatbots)
Dialogflow from Google (conversational AI)
Cognigy (conversational AI)
textengine.io (language generation)
Playform (image generation)
RunwayML (image and text generation)
Teachable Machine from Google (recognition & classification)
Wekinator (recognition & classification)
Computational creativity: 14 AI Artists whose work to explore
A list of our favourite artists who are using AI as a tool for their creative work
AI is being used in many professions — but what about the creative industry? How do artists use AI in their work? Currently, it’s not a very common tool for artists, but the good news is that this number is increasing. You can see bits of AI in many parts of the creative fields: from visual arts to dance, music, and even photography. In this article we’ll introduce some of our favourite artists who are using AI in their practice.

1.SOUGWEN CHUNG is a multidisciplinary artist exploring the communication between humans and machines. She is using a robot hand to create her artworks. This robot mimics her brush/marker strokes while she is painting with her hand. Sougwen’s art practice includes different art fields, including installation, sculpture, still image, drawing, and performance.
2.WAYNE MCGREGOR is a choreographer and director. In the “Living Archive” project, Wayne and Google Arts & Culture collaborated to experiment with AI and dance dialogue. The AI tool generates real-time new original movements for/with professional dancers. They trained the AI model with thousands of hours of Wayne’s archive videos, collected for 25 years.
PINDAR VAN ARMAN is, as he introduced himself, an AI artist collaborating with his painting robots’ creative mind. He teaches his robot assistants how to paint by deconstructing his artistic process, critiquing the results, modifying the algorithms in a loop. His last project, called “Artonomous” is a collaboration with Kitty Simpson, photographer, which draws portraits inspired by a set of reference photographs using AI, feedback loops, and deep learning.
REFIK ANADOL is a new media artist, creating parametric data sculpture and live visual/audio performance in his installations. He is exploring the space between the digital and physical world using machine intelligence by demonstrating a hybrid relationship between space design and media arts.
ANNA RIDLER is an artist and researcher that is using her own handmade datasets for creating artworks. Constructing personal datasets is helping her to find underlying concepts and themes while using them. Her datasets include selected captured and classified images and texts, which needs a laborious process to create. In “Mosaic Virus”, she uses her dataset and AI to display evolving videos of tulips based on bitcoin's price.
DANIEL AMBROSI is one of the creators of the AI art movement. In his project “Dreamscapes”, he created a series of landscape images using a unique form of computational photography, which he built back in 2011. Along with this dataset, he used an enhanced version of “DeepDream,” a computer vision program designed by Google in Dreamscapes.
HELENA SARIN is a software engineer and visual artist using GANs (Generative Adversarial Networks) to create artworks. “Neural Bricolage,” founded by Helena, is featuring her AI-assisted pieces. They are created using a dataset consists of her own drawings, sketches, and photographs as datasets.
.LAUREN LEE MCCARTHY is an artist that embodies machines to examine social relationships in today’s world. She works with performance, software, electronics, internet, film, photography, and installation, “trying to understand the distance between the algorithm and herself, between others and herself”.
SCOTT EATON is an artist and creative technologists, creating his artworks by combining traditional craft with contemporary digital tools. He uses deep neural networks in his work called “Hyperbolic Composition I: Genesis”, which is an expressive, novel figurative image of the human body.
SOFIA CRESPO is a generative artist focused on artificial lifeforms. She is interested in biology-inspired technologies. Her artworks are answers to the question of how new tech can help us connect with nature and love it more.
KYLE MCDONALD. “Artist working with code” is the way Kyle introduces himself. Besides using code and machine learning to create his own artworks, he contributes to developing new tools and open-source toolkits for artists to use them in creative ways.
.TARYN SOUTHERN is an artist, storyteller, and a producer with futuristic mindset working in the intersection of storytelling and technology. “I AM AI” is created by Taryn in 2018, which is the first album composed and produced by AI.
TOM WHITE is collaborating with AI systems to create abstract prints to see the machine perception through their eye. His artworks are creations by AI for AI to investigate around the question, how machines see, understand, and articulate the world.
BEN SNELL. Thinking of computational power as new raw material, Ben creates artwork (photographs, sculptures, and drawings) using contemporary materials and techniques in combination with traditional motifs. A computer program sculpted “Dio”, trained by a dataset of classical sculpture archives.
Boshra Javaheri
 is a designer and researcher with a diverse background in architecture, product design, and human-centered experience design. She is curious about people and has a passion for their experiences, emotions, and interactions with AI. You can find more about her here.
UX challenges for AI/ML products [1/3]: Trust & Transparency
Design considerations in Human-AI interactions. Part 1 of the series.
This article was originally developed and released as Chapter 4 of the AI-Driven Design e-book series ‘Design Challenges in Machine Learning Products’ in collaboration with AWWWARDS, Adyen, and Joel van Bodegraven available for download here.

I repurposed it into a Medium article so it can be a ‘living document’ I can edit and add to as I learn, develop new insights, and find better examples.

Introduction
Every design material comes with unique opportunities and challenges. In the same way that designing an event poster is different from designing a mobile app, designing AI/ML-driven applications is different from designing mobile apps.

As we begin to see AI features popping up in our day-to-day products and services, its challenges begin to materialize. They range from UX problems, such as explainability and user feedback mechanisms, to greater ethical challenges, such as echo chambers and data bias. Designing the user experience of adaptive, intelligent, and semi-autonomous systems present a range of new challenges for us designers to take on.

When thinking or talking about AI, we often imagine utopian or dystopian futures. Rarely, we dare to acknowledge its impact as something we have a hand in shaping (or even: a design challenge). Technology may be neutral and deterministic, but its development is not. As designers, we can take the raw material of AI and turn it into user, business, and social value.
This piece is by no means all-encompassing and only scratches the surface on the complexities of designing for AI. Instead, it aims to provide a starting point for building a shared understanding around some of the complexities of designing AI/ML interactions, spark discussion, and invite everyone to take part in (re-)imagining how to design positive user/human experiences in algorithmic systems.

3 x 3 challenges
This article, which shares my research on designing Machine Learning Products, will address 3 different themes Trust & Transparency, User Autonomy & Control and Value Alignment, highlighting 9 of the challenges that can arise within them, all supported with real-life examples.

Theme 1: Trust & Transparency
Not all AI features are (in)visible to the user, nor should we want them to be. When confronting our users with new systems, it is our job as designers to help them understand how they work, be transparent about their abilities, construct helpful mental models, and make the users feel comfortable in their interactions. Transparency is key to building trust in the system and respecting user trust in your organization.

1. EXPLAINABILITY
Making sense of the machine and communicating to the user why the system acts the way it does.

Design considerations: Show users what input data or algorithms were used to train the model, explaining and visualizing a gradual detailing of the model’s logic and how to interpret its presented result.

As HAI’s faculty member Nigam Shah points out, there are three main types of AI interpretability: “how a model works, why the model input produced the output, and an explanation that brings about trust in a model”.
→ Mixpanel
Mixpanel, the business analytics service company, uses machine learning to uncover user insights. The anomaly detection feature helps pick up on unusual behavior. The image shows an example of an anomaly, together with what data the prediction is based on and which segments drive the anomaly so that the user can make an informed decision about the next steps. It even offers a “share” function to consult with a colleague for a 3rd opinion.
→ Airbnb
When Airbnb introduced ‘smart pricing’ based on supply and demand, adoption wasn’t as high as expected. They learned users were happy to be informed by the algorithm but they still wanted to make the final decision for themselves. Airbnb then built an interface where hosts can evaluate price changes and accept or reject each of the algorithm’s recommendations.

Questions around Explainability

What is the right level of transparency?
Too little means the user doesn’t trust your system. Too much means the user might get confused with an overload of information.
What are good ways to explain predictions, confidence, and underlying logic underlying within the user interface?
What are useful mental models to help users understand and interact with the AI?
How to provide explainability to the user when model interpretability is low?
2. MANAGING EXPECTATIONS
Assisting the user to build helpful mental models of what the system can and cannot do by being transparent about abilities and limitations.

Design considerations: Proper onboarding during the first interaction with the application or feature where abilities and limitations are established.

→ Siri & Assistant
Each time the user calls upon Siri and it shows this screen, Siri has the opportunity to respond to queries and gradually introduce the user to its varied abilities. Onboarding and setting expectations become even more important in post-pixel interfaces because the user doesn’t have physical affordances nudging them where to go and what to do.
The Assistant chatbot doesn’t try to cover up its shortcomings but instead makes the most of its limited abilities by explicitly stating its abilities and how a user must communicate a query in order for it to be processed successfully.

Questions around Managing Expectations

Where and when do I communicate its abilities and limitations to the user?
What is the right level of trust?
Too little trust means the user doesn’t get any value from the system. Too much trust might lead to automation bias and poses risks for both the user and the organization.
How do I (continue to) onboard my user and make sure the systems’ adaptivity and resulting unfamiliarity don’t result in loss of trust?
3. FAILURE + ACCOUNTABILITY
When it comes to designing AI-driven user flows; assume failure and design graceful recoveries. Take accountability for mistakes and minimize the cost of errors for your user.

Design considerations: Apologizing, minimizing automation bias, allowing the user to indicate a mistake, suggesting alternatives, allowing feature requests, and taking accountability for mistakes.
→ Google Home & Alexa
I make requests to my Google Home quite frequently that are returned with “I’m sorry, I can’t do that yet.’’ While disappointing, the apologies and promise of future improvement keep me from losing trust. In the example of Siri below, we can see it also recommends an alternative — a query it understands to be similar to the one initially called upon and one it can perform. Both of them recognized there is no way to user test against adaptive systems so they must be designed for failure.


→ Fail! Chatbots
While we can not predict every possible scenario and ML’s adaptive nature makes testing a bit more tricky, we can anticipate obvious failures and prevent them from leading to awkward user experiences like below. Test your systems in real-life, out-of-the-lab context to bring to the surface common and obvious mistakes.


→ Nest

One day as B.J. May approached his Nest doorbell, it wouldn’t let him in because the model thought he was Batman. Fortunately, the designers anticipated failure and designed 2 back-up ways for him to intervene and still get inside. Consequently, the failure didn’t have many consequences other than a funny Twitter thread.

Questions around Graceful Failure + Accountability

How can your user report a mistake?
Under which conditions, for which goals and which users, even edge cases, might the system return undesired results?
How to design an interface that minimizes the cost to the user when the AI makes a mistake?
Considering the impact of mistakes in your use case, and how to retaliate from a bad prediction to not harm trust.
Who is responsible and liable for the consequences of mistakes?
3+ Ways AI and Design intersect (and Designers can get involved with AI)
Pointing out the spaces & practices emerging at this intersection.

Shaping the world around us
The role of a designer is to intentionally shape the world around us with the resources we have available. That world is increasingly shaped by data sets, algorithms, and pre-trained models.

As Artificial Intelligence(AI)/Machine Learning(ML) is finding its way into every industry and area of life, we can no longer afford to limit the development of these systems to the domain of engineering. As people are widely affected, both positively and negatively, we no longer get to excuse ourselves because it’s ‘too’ technical.

You may ignore technological developments and continue your design practice as-is for now. But if you recognize the influence and responsibility we have as designers or are simply curious to learn what AI and design have to do with one another, you came to the right place.

How do AI and design intersect?
While still in its infancy, the intersection of AI/ML and design is beginning to take shape and establish itself as a field. Universities form research groups and big tech expert teams committed to it. And as AI/ML is increasingly widespread, the need for designers to ensure the human(ity)-centeredness of these systems is urgent. The other day I even came across the first job opening for “AI designer”, and then another one — so yes, I guess it’s a real thing now.

Right now, I’m observing 3 major opportunity spaces for designers to work with AI/ML:

design with AI (human-machine collaborations with creative output)
design for AI (integrating human-centered design practices into the AI dev process)
design of AI (interaction and UX design for AI systems)
We’ll discuss each of these in more detail.

1) Design with AI — human-machine collaborations with creative output. AI as a design tool or partner.
How can AI assist us in the creative / design process? Design with AI is exploring collaboration between human creativity and computational logic to produce creative output. Creatives have begun exploring the abilities of AI to augment their images, videos, text, music, UI, product design, architecture, and any other format that can be rendered into computable data.

This practice seems most relevant for graphic designers, artists, and creatives.


Examples of what Design with AI might look like in the wild:

generative or parametric design in architecture like Autodesk did to design their new Toronto office
automating tedious design tasks like AirBnB’s tool for digitalizing wireframes or Netflix’s tool for localizing graphics
logo and brand asset generation tools that have learned from best practices for example from Brandmark
using machine learning models such as PoseNet and mapping its prediction to visual output like this collaboration by technologist Maya Man and dancer Bill T. Jones
Generative Adversarial Networks (GANs) to generate images imitating or warping a certain input style. Try the Ganbreeder yourself
Vera van der Seyp is one of my favorite AI designers working with generative type and graphics
Sofia Crespo is an AI artist focused on nature & neural nets
Resources to learn more about Design with AI:
algorithms.design — incredible directory of AI x design projects
Runway ML —the easiest way to get started with creative AI
AI Artist’s AI Art Tools — exhaustive collection of creative AI tools
autonomous.design — set of AI-powered design tools

Questions to ask and ponder around Design with AI:
Continuing a similar debate around intelligence, how do we define and measure creativity? Who owns, gets to take credit, and sells the art when it’s made in collaboration with an AI, from an often pre-existing dataset, and open-source model? Should we use computers to imitate best practices or incorporate randomness instead?


Image created using CLIP+VQGAN model to generate visuals based on text prompts
2) Design for AI — integrating human-centered design practices into the AI development process. AI as a problem-solver.
Why, when, and how should we build AI systems? Design for AI is about bringing elements and practitioners from the human-centered design approach into the AI development process. Spotting opportunities, considering user needs, and anticipating societal implications alongside engineering decisions will enable us to build systems that are considerate of the context they’re intended for. This requires a close and cross-disciplinary collaboration between designers and engineers.

This practice seems most relevant for design consultants, strategists, and researchers.


Examples of what Design for AI might look like in the wild:

Spotting opportunities for AI/ML to add value and help solve problems in a unique way
Being able to frame user needs as data exploration queries and machine learning problems
Informing trade-offs between different algorithms and optimization parameters based on the specific use case (e.g. recall vs precision)
Prototyping the experience through Wizard of Oz methods to validate user value before investing lots of resources
Interviewing domain experts for an early understanding of feature selection and decision logic
Some resources to learn more about Design for AI:
AI meets Design toolkit — my toolkit for designers to work with AI
AI Ideation Cards — card deck with 24 prompt cards and 100+ examples to help you brainstorm around AI capabilities and find opportunities
Lingua Franca — A Design Language for Human-Centered AI

Questions to ask and ponder around Design for AI:
What do designers need to learn about AI/ML? What do engineers need to learn about design? How can we make the most of both designers’ and engineers’ practice? How do we communicate and collaborate to design human(ity)-centered AI?


AI Ideation Card deck to support in finding meaningful opportunities for AI
3) Design of AI —shaping the UX, UI, and user interaction with AI systems. AI as a design material.
How will users interact with the AI systems we built? Design of AI is about designing interactions with adaptive, intelligent, and semi-autonomous systems. Every design material comes with unique opportunities and challenges. In the same way that designing an event poster is different from designing a mobile app, designing AI/ML-driven applications is different to designing mobile apps. To create helpful and holistic experiences, this requires designers to acquaint themselves with the materiality of data and AI/ML, and coin smart solutions.

This practice seems most relevant for UI, UX, and interaction designers.


Examples of what Design of AI might look like in the wild:

Leveraging new types of interactions available such as voice interfaces and computer vision for better user experiences
Designing interfaces to explain to users (with the right amount of detail) how the system works
Laying out an ongoing onboarding process that makes sense of changes
Building in the right user feedback mechanisms both implicit and explicit to help your model learn
Auditing datasets and models for inclusivity and bias
Anticipating potential unintended consequences
Some resources to learn more about Design of AI:
AI-Driven Design Ebook #4 — Design Challenges in Machine Learning Products — my ebook with AWWWARDS & Adyen about some of the unique design challenges

Questions to ask and ponder around Design of AI:
What are helpful mental models for users to have around understanding AI/ML-systems? How do we materialize high-level principles like explainability and user feedback loop into (post-pixel) interfaces? How can we build responsibility and ethics audits into every step of the development process to anticipate consequences and ensure inclusivity?


Image from SPACE10’s Everyday Experiments proposing a speculative computer vision chef
4) Design x AI
There are plenty more practices and design challenges surrounding AI/ML. While these may not fit into the categories above, they’re relevant spaces for us designers and this post wouldn’t be complete without mentioning them.

Data Design
On the periphery, we observe a practice that is less about AI/ML and more about the raw material it feeds off: data. This includes data visualization, data design, information design, even database design. For example Giorgia Lupi’s work.
ML Tools
With more low- and no-code tools entering the market, somebody is designing the applications that allow people to train, test, and deploy machine learning models through Graphical User Interfaces instead of code. For example Teachable Machine, AutoIBM, or RunwayML.
AI Education
One of the most important roles in this whole space is developing accessible educational content and experiences to improve data literacy and make AI/ML understandable to professionals and the general public.
… tell me what else is emerging in the comments!

Image from the project Explainable Artificial Intelligence: a Collection of Critical Essays
Forming a taxonomy around AI x Design
A very young field, this is a first attempt at forming some sort of taxonomy to organize the practices emerging at this intersection. It’s by no means complete or decided — rather, it’s here for you to argue and add onto! I’d love to hear from you with more examples, new categories, and alternate mental models.

Join the AIxDesign community
AI and design have a lot more to do with each other than a first glance may suggest. As a development that is so influential, it’s of crucial importance more designers, artists, social scientists, and hybrid minds work alongside the engineers building these systems.

That includes you! If you’re feeling in any way triggered, consider this a call. Get lost in some of the resources, ask your engineers if you can help, run off to make some weird AI art. Whatever it is, take that first step.

If you (want to) work in the AIxDesign space, I invite you to join our community of AIxDesign practitioners. We host events, publish content, congregate in a Slack channel, share opportunities, facilitate networking spaces, and more.

There’s so much to be done and this is just the beginning. I’m excited and honored to see it all unfold and contribute where I can.

I’d love to hear if you learned anything new, and what (if any) next steps you’re planning to take!

TL:DR
There are new practices emerging at the intersection of AI and design. For now, they seem to fall in 3 categories: 1) design with AI (creative human-AI collaboration), 2) design for AI (human-centered design for the AI dev process) and, 3) design of AI (interaction and UX design for AI systems). With plenty more practices popping up at the periphery, this young but rapidly growing field offers exciting opportunities for designers to get involved and make an impact.
Imagining narratives for preferable AI futures
How speculative design helped us visualize & prototype ways in which AI might contribute to the Sustainable Development Goals
As AI systems are often pretty invisible, most people imagine artificial intelligence in the way it often is portrayed in sci-fi and pop culture. The Terminator, Ex-Machina, Blade Runner: AI is basically the same as an evil robot, right?

This workshop aimed to explore the anxieties and prejudices we have around this technology and instead of imagining the easy-coming-to-mind dystopia, focus our gaze on preferable AI futures.

Speculative design helps us to imagine and visualise these explorations, so we can open up these topics for discussion. To focus on real life issues rather than the creation of utopias, we used the Sustainable Development Goals as a framework to thematize our anxieties and narrow down the focus of possibilities.


The event flyer :)
AIxDesign & Speculative Futures Rotterdam
In the span of the workshop, we aimed to imagine how AI technologies could positively contribute to the pursuit and realization of Sustainable Development Goals. We finished the event with a prototyping session of AI-driven solutions for better futures.

This workshop was a collaboration between AIxDesign and Speculative Futures Rotterdam. The three organizers and facilitators — Nadia Piet, Karolina Thakker, and Erik Peters — came about the theme for the event during a casual chat on the perception of artificial intelligence in popular media. We allowed the speculative design framework to take us into less explored paths and together imagine the more hopeful alternatives during a 2 hours workshop.

The setup of the workshop
The setup of the workshop
The event took place online with the help of Google Meet and Google Hangouts for group sessions. We used boards in Mural to create frameworks for exercises and provided an additional Google Docs document with instructions for the participants to refer to throughout the workshop.

The session was packed with exercises we found necessary in providing the structure that could help the participants narrow down the focus and stimulate lively discussions in groups. At the same time, the technical complexity of hosting a workshop with this many exercises within two hours would be a challenge. In hindsight, most participants navigated with ease through all the different online channels.

The workshop: How to use AI for the realization of Sustainable Development Goals?
Mapping and flipping AI anxieties
The event was divided into two main parts. In the first part, in an exercise called Anxiety Mapping, the participants were asked to list down all the anxieties and fears that they associate with the adoption of AI technologies in the current world. The map was filled with all sorts of worries within minutes in which we managed to identify main problem clusters. These challenges were to be used as a starting point for the second part of the workshop in which groups worked on developing AI-driven solutions for preferable futures through series of different exercises.

Filling the Anxiety Map
Filling the Anxiety Map
The participants formed groups based on their choice of the problem they found to be most interesting. Along the theme of our evening, we then asked them to reframe this problem into a Hope — a way how AI technologies could help us in creating better futures, although there is no opinion on what “better” actually means. For that reason, we decided to add Sustainable Development Goals as our reference point. We asked the groups to choose their design objective for the next exercise based on the SDG that they felt was most relevant to their AI-driven Hope.

It was a fast-paced event, so we made the decision not to intervene in the group chats. It turned out well but as facilitators, we missed hearing all the fun bits and weird ideas of group discussions.

How might we use our AI superpowers?
The next exercise was a brainstorming session. First, we asked participants to choose one domain within their focus area that they would like to work on. Besides, we briefly explained the main AI superpowers (add a short one sentence why AI superpower) and provided a simple prompt question to help groups with the idea generation phase of the workshop. People were engaged in the process. It was hard to break the discussions and ask everybody to move on to the next exercise.

We asked each person in a group to then choose their favourite ideas and place them onto the provided Futures Cone template. It was helpful to narrow down the choices and assess the probability of them existing in the future.

Prototyping the Bigotry Bias Buster
The last part of the event was the rapid artefact prototyping session. To make it easier and more fun, we provided the groups with ready-to-use templates. The templates included: an event page, a Kickstarter campaign, a place in Google maps, an app and a LinkedIn job profile. In the end, many shared with us that they enjoyed working in this way and had a good laugh trying to put their prototype together.

Unfortunately, we had only a few minutes left for groups to present their creative ideas for AI-powered futures (who wouldn’t like to own a cute digital double Patronus of their own?), but we were happy to see participants being proud and proactive in their projects.

SDG’s for achieving a better future for all
Team Orange reducing inequalities
From the clustered anxiety ‘reinforcing harmful bias, the group flipped this anxiety in the exploration of Law and Justice Systems, linking this to SDG #10: Reduced Inequalities. The artefact they presented, named ‘Bigotry Bias Buster’, suggests a decentralised system that spots the biased stuff in our algorithms. ‘BBB is a supreme entity that reigns with Fairness on Planet Earth. It condemns entities, nations, citizens, or companies who dares letting any bias into their algorithms.’

The canvas of Team Orange — SDG #10: Reduced Inequalities
The canvas of Team Orange — SDG #10: Reduced Inequalities
The artefact of team orange: A decentralised system that spots the biased stuff in our algorithms
The artefact of Team Orange: A decentralised system that spots the biased stuff in our algorithms
Team Purple guarding digital well-being
From the clustered anxiety ‘lack of privacy / surveillance’, the group flipped this anxiety in the exploration of Help Companions and Data Protection, linking this to SDG #3: Good Health and Well Being. They proposed a Kickstarter campaign for ‘Create Your Own Digital Double’, an avatar hologram that is the visualisation of your private data. The double is customisable and protects your data.


The canvas of Team Purple— SDG #3: Good Health and Well Being

The artefact of Team Purple: An avatar hologram that is the visualisation of your private data
Team Green promoting good health
From the clustered anxiety ‘loss or lack of abilities’, the group flipped this anxiety into Enhancing our abilities and helping us being more in touch with ourselves, linking this to SDG #3: Good Health and Well Being. Career OS is a lifelong coach powered by AI to help build your social and soft skills at work. The system provides ‘timely and proactive recommendations on your next social and work skills to achieve your career goals.’


The canvas of Team Green— SDG #3: Good Health and Well Being

The artefact of Team Green: A lifelong coach powered by AI to help build your social and soft skills at work
Learnings about manoeuvring the digital space
Regardless of time constraints and a heatwave outside our houses, we hope everybody had fun during the workshop and got inspired by the discussions. Many liked working with the worksheets and the artefact templates, but wished to have a bit more time for an initial introduction. As virtual workshops have their limits, we are looking forward to a moment to conduct this workshop again in real life.

We will continue to refine this version for future sessions, online and offline, and hope to set up another session in the near future! Join our Friendly Futures Bunch mailing list to be updated on public workshops, reach out if you’d like a version of this for your team or organization, and stay tuned for more near-future futures work!
Imagining narratives for preferable AI futures
How speculative design helped us visualize & prototype ways in which AI might contribute to the Sustainable Development Goals
As AI systems are often pretty invisible, most people imagine artificial intelligence in the way it often is portrayed in sci-fi and pop culture. The Terminator, Ex-Machina, Blade Runner: AI is basically the same as an evil robot, right?

This workshop aimed to explore the anxieties and prejudices we have around this technology and instead of imagining the easy-coming-to-mind dystopia, focus our gaze on preferable AI futures.

Speculative design helps us to imagine and visualise these explorations, so we can open up these topics for discussion. To focus on real life issues rather than the creation of utopias, we used the Sustainable Development Goals as a framework to thematize our anxieties and narrow down the focus of possibilities.


The event flyer :)
AIxDesign & Speculative Futures Rotterdam
In the span of the workshop, we aimed to imagine how AI technologies could positively contribute to the pursuit and realization of Sustainable Development Goals. We finished the event with a prototyping session of AI-driven solutions for better futures.

This workshop was a collaboration between AIxDesign and Speculative Futures Rotterdam. The three organizers and facilitators — Nadia Piet, Karolina Thakker, and Erik Peters — came about the theme for the event during a casual chat on the perception of artificial intelligence in popular media. We allowed the speculative design framework to take us into less explored paths and together imagine the more hopeful alternatives during a 2 hours workshop.

The setup of the workshop
The setup of the workshop
The event took place online with the help of Google Meet and Google Hangouts for group sessions. We used boards in Mural to create frameworks for exercises and provided an additional Google Docs document with instructions for the participants to refer to throughout the workshop.

The session was packed with exercises we found necessary in providing the structure that could help the participants narrow down the focus and stimulate lively discussions in groups. At the same time, the technical complexity of hosting a workshop with this many exercises within two hours would be a challenge. In hindsight, most participants navigated with ease through all the different online channels.

The workshop: How to use AI for the realization of Sustainable Development Goals?
Mapping and flipping AI anxieties
The event was divided into two main parts. In the first part, in an exercise called Anxiety Mapping, the participants were asked to list down all the anxieties and fears that they associate with the adoption of AI technologies in the current world. The map was filled with all sorts of worries within minutes in which we managed to identify main problem clusters. These challenges were to be used as a starting point for the second part of the workshop in which groups worked on developing AI-driven solutions for preferable futures through series of different exercises.

Filling the Anxiety Map
Filling the Anxiety Map
The participants formed groups based on their choice of the problem they found to be most interesting. Along the theme of our evening, we then asked them to reframe this problem into a Hope — a way how AI technologies could help us in creating better futures, although there is no opinion on what “better” actually means. For that reason, we decided to add Sustainable Development Goals as our reference point. We asked the groups to choose their design objective for the next exercise based on the SDG that they felt was most relevant to their AI-driven Hope.

It was a fast-paced event, so we made the decision not to intervene in the group chats. It turned out well but as facilitators, we missed hearing all the fun bits and weird ideas of group discussions.

How might we use our AI superpowers?
The next exercise was a brainstorming session. First, we asked participants to choose one domain within their focus area that they would like to work on. Besides, we briefly explained the main AI superpowers (add a short one sentence why AI superpower) and provided a simple prompt question to help groups with the idea generation phase of the workshop. People were engaged in the process. It was hard to break the discussions and ask everybody to move on to the next exercise.

We asked each person in a group to then choose their favourite ideas and place them onto the provided Futures Cone template. It was helpful to narrow down the choices and assess the probability of them existing in the future.

Prototyping the Bigotry Bias Buster
The last part of the event was the rapid artefact prototyping session. To make it easier and more fun, we provided the groups with ready-to-use templates. The templates included: an event page, a Kickstarter campaign, a place in Google maps, an app and a LinkedIn job profile. In the end, many shared with us that they enjoyed working in this way and had a good laugh trying to put their prototype together.

Unfortunately, we had only a few minutes left for groups to present their creative ideas for AI-powered futures (who wouldn’t like to own a cute digital double Patronus of their own?), but we were happy to see participants being proud and proactive in their projects.

SDG’s for achieving a better future for all
Team Orange reducing inequalities
From the clustered anxiety ‘reinforcing harmful bias, the group flipped this anxiety in the exploration of Law and Justice Systems, linking this to SDG #10: Reduced Inequalities. The artefact they presented, named ‘Bigotry Bias Buster’, suggests a decentralised system that spots the biased stuff in our algorithms. ‘BBB is a supreme entity that reigns with Fairness on Planet Earth. It condemns entities, nations, citizens, or companies who dares letting any bias into their algorithms.’

The canvas of Team Orange — SDG #10: Reduced Inequalities
The canvas of Team Orange — SDG #10: Reduced Inequalities
The artefact of team orange: A decentralised system that spots the biased stuff in our algorithms
The artefact of Team Orange: A decentralised system that spots the biased stuff in our algorithms
Team Purple guarding digital well-being
From the clustered anxiety ‘lack of privacy / surveillance’, the group flipped this anxiety in the exploration of Help Companions and Data Protection, linking this to SDG #3: Good Health and Well Being. They proposed a Kickstarter campaign for ‘Create Your Own Digital Double’, an avatar hologram that is the visualisation of your private data. The double is customisable and protects your data.


The canvas of Team Purple— SDG #3: Good Health and Well Being

The artefact of Team Purple: An avatar hologram that is the visualisation of your private data
Team Green promoting good health
From the clustered anxiety ‘loss or lack of abilities’, the group flipped this anxiety into Enhancing our abilities and helping us being more in touch with ourselves, linking this to SDG #3: Good Health and Well Being. Career OS is a lifelong coach powered by AI to help build your social and soft skills at work. The system provides ‘timely and proactive recommendations on your next social and work skills to achieve your career goals.’


The canvas of Team Green— SDG #3: Good Health and Well Being

The artefact of Team Green: A lifelong coach powered by AI to help build your social and soft skills at work
Learnings about manoeuvring the digital space
Regardless of time constraints and a heatwave outside our houses, we hope everybody had fun during the workshop and got inspired by the discussions. Many liked working with the worksheets and the artefact templates, but wished to have a bit more time for an initial introduction. As virtual workshops have their limits, we are looking forward to a moment to conduct this workshop again in real life.

We will continue to refine this version for future sessions, online and offline, and hope to set up another session in the near future! Join our Friendly Futures Bunch mailing list to be updated on public workshops, reach out if you’d like a version of this for your team or organization, and stay tuned for more near-future futures work!
About volumetric interviews, art school collaborations, and algorithmic systems for artistic expression
To share a variety of voices and perspectives in our growing community, we’ve decided to cast a monthly spotlight on one of our members in the form of mini-interviews 👀 This month, we are very happy to introduce you to Leo Scarin: an artist, creative technologist, and our brand new Social Media Lead!


AIxD: Hi Leo! First question 🎤 where are you now and what does the view from your window look like? (We would love to see a picture 🏙)

L: I am in the TodaysArt headquarters in The Hague, where my studio is based. Attached is a scratchy cat in the backyard 🐈


AIxD: Thank you! We love to see a cat 😻 Off to the next question: tell us briefly about yourself. What do you currently do and what kind of background and experiences have led you to where you are now?

L: I am a creative technologist! My main interest is the use of new technologies to design meaningful and tangible digital interactions. I recently graduated at the Royal Academy of Arts in Interactive / Media / Design, a bachelor program that incorporates diverse and critical approaches to Art, Design, Technology and Society. For a couple years I have worked with TodaysArt, a media art platform and festival based in The Hague.

AIxD: As a creative technologist, it is actually part of your work to be trying new tools. Could you tell us more about a tool or a method you have recently worked with and are excited about?

L: I have a strong fascination for volumetric capture technologies, which I believe to be the most intimate way to archive a memory. In my work, I often adopt this technique through the use of Kinect sensors, photogrammetry softwares (OpenMVG, Brekel), and realtime render engines (Unity3D, Touchdesigner). The most exciting part is the application of volumetric technologies into interactive experiences: an inspiring example is Planet of People at the Lithuanian Space Agency. PS — For the most technical geeks, Instant Neural Graphics Primitives seems to be the latest and fastest AI tool in the game of photogrammetry, a technique to be very curious about, in the future.


Volumetric Interviews, RGBdog
AIxD: to put it simply, what is volumetric capture? Is it recording real-life events in a 3D format?

L: Correct! Volumetric capture renders a digital 3D replica of real-life objects or bodies. Photogrammetry, for example, uses different multi-angle photos to digitally recreate the 3D form of an object. Kinect sensors do this by capturing the real-time depth of a moving image through infrared lights.

AIxD: Thank you for sharing some insights into your toolkit with us 🛠 We also want to use this space to promote your work and ideas. Is there anything you are working on at the moment you would like to share with us?

L: I am now working on a super exciting project by Rotterdam-based artist and fellow KABK alum Kexin Hao for the upcoming Rewire Festival in The Hague. Future Dance of Nostalgia is an interactive performance and installation incorporating pre-industrial heavy labour gestures into dance and bodily knowledge. As the creative technologist of this project, I am developing a customised Just Dance-like motion-capture videogame for the visitors to play! The installation will be running at the entrance of Amare during the whole Rewire Festival (get your tickets here).

AIxD: Fantastic! We can’t wait to see your work 🙌 and to learn more about your vision and perspective. What do you find interesting/attractive/worth looking into at the AI x Design crossover?

L: As accessible as AI tools have become for artists and designers, what excites me the most is how algorithmic computational systems will enhance artistic, creative expression. There is some sort of poetry in the way machines manifest creativity, and how new aesthetics arise from it. Those fragmented attempts of computers to replicate the complexity of human situations are the limbo, the intermediate state of the image that I am attracted by, both as a creative medium and as a context of technological research. Furthermore, I find worth looking into the social justice scenarios and how they are shaped by computational systems. Kavita Philip writes “rights, ethics, speech, freedoms […] cannot be predicted or deduced from axioms or a priori rules […] searching for a pure, clean signal entails wiping out all noise. It involves shaping humans in the dumb models for which computational systems have been built”. As she wisely suggests, I am interested in embracing the noise and explore its expressive outcome.

AIxD: Wow, this is a wonderful quote, and also very urgent, as we need to look not only at playful aspects of tech but also we aware of its harms and violations. Last question: If you could pick one text/book or a podcast which is an absolute must-read or must-listen, what would you recommend?

L: Your computer is on fire (the quote above is from this book, from the chapter ‘Afterwords: How to Stop Worrying about Clean Signals and Start Loving the Noise’)


Variations on a Remote Room, Leo Scarin, 2021
AIxD: Thank you! And thank you so much for chatting with us 🙏 we are looking forward to having more conversations throughout the year and working together 🎆
Shall we meet more of our community members? This month, let us introduce you to Catarina, our content lead and creator for AI Playground guides.


AIxD: Hi Catarina, super excited to interview you this month! I have been enjoying a lot of the content you have been sharing at AIxDesign. For our audience, could you tell a bit about yourself? What do you feel passionate about and what aspired you to do what you are doing right now?

Catarina: I’m a London-based digital designer with a great interest in generative visuals, mixed realities, and the human psyche.


Originally from a small town in the north of Portugal, I moved to England 8 years ago. Before the big move, I was certain that I would work in the biomedical field. I studied one year of biomedical sciences and realized that it was never intended for me — so I then decided to pursue what has always been my biggest passion: film.

After graduating, I was determined to work as a cinematographer during my career in film. I worked as a freelance photographer for big companies but soon realized how limited the photography medium could be if I stuck to the same old ways of working.

I became fascinated with the world of creative coding and interactive technologies, which led me to a master’s degree in Computational Arts. Since then, I’ve had the opportunity to expand my creative practice and exhibit projects publicly. The world of design came very spontaneously to me as nowadays I work full-time as a digital designer and also freelance on other design & tech projects.

I’m a firm believer that it’s totally okay to change paths, as I’m now able to look back and see how all of these experiences have shaped the work I currently do.


Project 25 Abril_Sempre, about Portuguese revolution witnessed by a machine
AIxD: I really admire how fearlessly you have been pursuing your passion. Are you currently working on any projects? 🖋

Catarina: I’m collaborating with inspiring people in the holistic field using intuitive design and creating interactive live visuals for their events. These collaborations help give a bigger purpose to what I do in my specific field.


AIxD: Since you have been our content lead, I am curious what do you find interesting in the AIxDesign crossover? 💾

Catarina: As the AIxDesign crossover becomes more widely accessible, I’m excited to see what the future will bring to art-making, especially filmmaking. I find it exhilarating to learn about new tools/software and hear about how other designers and artists are expanding their creative practice with these tools.


AIxD: When you are working in the field of creative technologies, is there any tools or methods that intrigue you currently?

Catarina: I’ve recently been reading into applying Jungian archetypes to my design work and how ancestral wisdom (I like to call it AI — ancestral intelligence) can influence the reasons behind why we design. I’m excited to find ways to weave the old and the new beyond the human-machine-future ideology.


AIxD: Ancestral Intelligence! How fascinating! To close our chat, is there any books, authors, or artists you would like to share with us? 📚

Catarina: I seem to always go back to Be Here Now by one of my main references, Ram Dass. It is fascinating and insightful, I find its visual journey a source of inspiration. The fact that it can look a bit chaotic ironically brings a sense of calm as you follow the path the book takes you on. It might not be for everyone, but I still find it one of the most beautiful books.


AIxD: Well thank you so much for the recommendation and thank you for taking the time for the chat! Can’t wait for the future projects that you will be working on🌸
Meet the community: Karina Zavidova
To share a variety of voices and perspectives in our growing community, we’ve decided to cast a monthly spotlight on one of our members in the form of mini-interviews 🎉 This month, let us introduce you to Karina, our communication lead.


AIxD: Hi Karina, glad to be able to interview you this month! Although we have chatted before, could you share a little background about yourself with our readers? What are you currently working on, and what aspired you to pursue your passion?

KZ: Hi! I have a background in graphic-design-slash-fine arts, so I am trained to work with visuals, but I ended up working with words. Right now, I am a freelancer and I do four types of work: I work as a fundraiser in the cultural field, mainly with autonomous practitioners and small collectives (it’s called ‘grant writer’ in the Dutch English-speaking arts & culture circle, but when I tried to explain what I do to a corporate recruiter they say this is called fundraiser — go figure), I do graphic design from time to time, I do communications at AIxDesign, and I also work in event catering.

Right now I also took a job as a ghostwriter — this is something I have never done before, but of course I said yes. My goal is to stop being a freelancer by 2023 and streamline all my writing and content-making activities into an actual full-time job. I am taking my time to figure out what this job is.

I (surprise surprise) love tech and see myself as a communications professional in tech, but I also want to talk to more people from various fields and do a more dedicated search — that’s not something I could think about in the last few years during the pandemic and now I feel that I am finally having enough space in my life to start exploring options.

I am looking for a challenging environment I care about, where I can grow, and also an environment that is not starved of money and cares about its people (the reason I’m drifting away from the arts, an environment designed to grow superstars and care little about the rest).

AIxD: That’s really fascinating. I am with you on being in a challenging environment could sometimes bring out the best side of us💪. Are you currently working on any projects that you would like to share with us?

KZ: Nothing, I signed NDAs 😅🤝.

But I have a quite an interesting personal project going on — after ten years in the Netherlands with six temporary residence permits, I am going to switch to a permanent permit (by 2023 unless there are delays with processing my application) and eventually become a Dutch citizen in 2023, changing nationality from Russian to Dutch.

I am very curious how my professional situation will change — since my residence permit first was tied to studies (where employment is not allowed) and then to self-employment (when it is not allowed to work as an employee, only as a freelance contractor), it will be the first time I have full access to the job market.

Will my employability improve? Would it be feasible to transition to a new field? In the Dutch cultural field, more than 50% are freelancers, so it didn’t feel very odd to be one. But now I can also try finding my place in the fields where freelancing is not the norm. What is out there? It feels great to begin to realise how many options are there.


AIxD: Congratulations! That’s very exciting and I’m glad that there are a lot more opportunities awaiting you ❤️. Since you are our communication lead and have been collaborating with the AIxDesign team for a bit, may I ask what you find interesting/attractive/worth looking into at the AIxDesign crossover?

KZ: I am interested in the current situation where we have a combination of super-powerful/realistic simulations and a lot of wacky and wonky low-tech-looking stuff and the power dynamic between the two. What I see is that a lot of big brands are embedding the ‘wonky stuff’ into their visual language, the streetwear + high fashion combination that is also translatable to visuals or text.

What I ask myself, as a communications professional who loves tech, is what can we do to make sure that us embracing the ‘wonky stuff’ will lead to the appreciation of creatives who made it and the wider acknowledgment of individual makers and collectives (also more chances for all — moving away from ‘superstars and the rest’ mentality) and not to just subconsciously embedding the ‘wonky stuff’ into the neo-corporate speak.

You asked me if I can share some images to illustrate my point about the ‘wonky stuff’ and to be honest there was nothing particular that came to mind. I mean this visual style where we see images that could be…uhm, a visual analogue of a crocheted top? It is somewhat in the air, in the Instagram stories.

Yet, this mass-produced wonkiness made me think of something else. I am intrigued by the possibility to produce (visuals, texts, anything) in huge amounts, and I can see the statement ‘I can produce something you (or your consumers) like in an indefinite amount’ as a way for artists to negotiate with established entities or to lure.

To illustrate this, I want to put side by side two examples: Metabirkin from 2022, where digital bags were created (in a relatively large amount), subsequently creating the conversation between the artist and the brand, and Female Extension from 1997, where an artist generated 127 (!) net art pieces by fictional female net artists, also to test how the authority would respond (And how would these fictional pieces inform her on her sense of belonging in the world of 90s net art?)

Hermès Allegedly Sends Cease and Desist to MetaBirkins NFT Creator
Hermès has allegedly sent a cease and desist letter to Mason Rothschild, the artist behind the MetaBirkins NFTs…
hypebeast.com


MetaBirkins by Mason Rothschild, which aren’t by Hermès or even really bags, initially sold for $42,000 in December. (Mason Rothschild. Business Of Fashion, 2022)
AIxD: Love that observation. For our readers, could you share a tool or method that you have recently worked with and are very excited about recently?

KZ: I love software, and I get genuinely hyped about using good software and project management tools. But I also appreciate it when a piece of software has an element of beauty built into it.

I want to mention Ulysses, my favorite writing app. It is functional, it is beautiful and it is also made by a small team — the subscription costs only €50 a year (half if you are a student) and the support is better than for products that cost ten times more. Oh, and yes, it is in Markdown. I love writing in Markdown. First I was weirded out by it, but then I got used to it and now I am trying to write in Markdown in any other app

AIxD: Thank you for sharing the software with us ✍️! To close our chat, if you could pick one text/book or a podcast which is an absolute must-read or must-listen, what would you recommend?

Hacker, Hoaxer, Whistleblower, Spy
The Many Faces of Anonymous
by Gabriella Coleman
To share a variety of voices and perspectives in our growing community, we’ve decided to cast a monthly spotlight on one of our members in the form of mini-interviews 👀 This month, we are very happy to introduce you to Sinem Görücü: a data and design justice advocate and our event lead!


AIxD: Hi Sinem! Great to have the opportunity to chat with you 🙌. Please tell us briefly about yourself. What do you currently do, and what kind of background and experiences have led you to where you are now? 🎤
S: I was actually trained as an architect and an urban designer, and I slowly got interested in data and design justice during my studies. Right now, I am working freelance on a variety of creative things at the intersections of data, AI, design, feminism, and social justice.

AIxD: That’s amazing! I think data literacy and data science have become incredibly important topics to focus on in recent years, especially with the rise of machine learning. Could you tell us more about something you are interested in or excited about recently?

S: In general, I am very interested in community-initiated and -led data practices and lately, I have been interested in data storytelling and communal data story construction. Works by Giorgia Lupi (datavizscrapbook) have always inspired me.



Selected work from DataVizScrapbooks by Giorgia Lupi
AIxD: Thank you for sharing!😍 Is there anything you are working on at the moment you would like to share with us?
S: Right now, I am finalizing an article named “My Grandma Is Not A Cyborg” that I wrote for futuress.org. It is a critical text that merges the daily experiences of design oppression with design justice theory.

AIxD: That sounds extremely intriguing! Looking forward to the article. As the event lead for AIxD, what do you find interesting/attractive/worth looking into at the AIxDesign crossover?
S: I think what is most exciting to me is examining the potential and future of AI through the design justice perspective and exploring the democratized AI ways of doing/designing.

AIxD: Definitely. Hopefully, we could have more opportunities to learn more about design justice in AI-driven design in our future events! If you could pick one text/book or a podcast which is an absolute must-read or must-listen, what would you recommend?📚
S: I would definitely pick Data Feminism by Catherine D’ignazio and Lauren F. Klein, which has been the highlight of the last couple of years for me. I just keep it on my bedside table.


Data Feminism by Catherine D’ignazio and Lauren F. Klein
AIxD: Thank you! And thank you so much for chatting with us 🙏 we are looking forward to the future events you have planned for all of us in the upcoming months. 🎫
Meet the community: Computational Mama
To share a variety of voices and perspectives in our growing community, we’ve decided to cast a monthly spotlight on one of our members in the form of mini-interviews 🎉 This month, we are excited to introduce you to Ambika Joshi, also known as Computational Mama: a creative technologist and one of our project leads.


AIxD: Hi Computational Mama! So amazing to have the time to sit down and chat with you! I’ve known you from our AI Playground project but I am wondering if you could tell us a bit about yourself? Very curious about your background.

A: I’m co-founder of Ajaibghar and we build products and experiences for the arts and culture sector. I also teach and explore creative coding and the idea of coding as a form of care.

Ajaibghar
Website
www.ajaibghar.com

AIxD: That sounds amazing. 👏 The mission of Ajaibghar (“Builds product and strategic solutions in Arts & Culture, driven by creative technology and new media”) is particularly interesting. Could you shed some light on the projects that you have been working on?

A: At Ajaibghar we are building a mobile-friendly web app for museums and brand experiences called MuseSkôp. It uses image recognition AI on products, artworks, and spaces and adds an augmented layer of digital content in the physical world!

MuseSkôp | AI-driven Journeys through Artworks - Ajaibghar
Website
www.ajaibghar.com



MuseSkôp
AIxD: That is super fascinating! As a project lead at AIxDesign, what do you find interesting to look into at the AIxDesign crossover?

A: As creators, we all have one eye and one step in the future. It’s only natural for the design community to embrace and mold the future of AI. Design brings nuance to AI through experience, empathy, and care.

AIxD: Well said! When working with design and AI, there are many tools we could utilize to collaborate together. Is there any tool or methods you have recently worked with that you are particularly excited about? 🖌

A: I’m so excited about Gather Town. It’s a very nice platform for events and community meet-ups and we have been exploring it for our exhibitions and museum projects too! We also held the first AIxDesign AI playground meet-up on it.

Gather | A better way to meet online.
Centered around fully customizable spaces, Gather makes spending time with your communities just as easy as real life.
www.gather.town

AIxD: It is such a cute platform to gather online! 😍 To close our conversation, is there any book or podcast you would like to recommend to our readers?

A: In line with the responses and thoughts around care I’d recommend this very old podcast episode from Design Matters with Debbie Millman interviewing the artist Amy Sherald.

Drip - Debbie Millman
Hello my dear Drip friends! This is the last official episode of my 13th season of Design Matters! And it is a special…
d.rip

AIxD: Thank you for everything you have shared with us today! Looking forward to future AI Playground events and exciting things you have planned for us! 🔥

AIxD: Hi Abdo! Really nice to get to chat with you this month! I know that you have been up to many exciting projects so why don’t you share a little bit about yourself and your background with our subscribers?

Abdo: My name is Abdelrahman (Abdo) Hassan, and I’m a creative technologist, data practitioner, and poet.


Abdo. Image courtesy of @Imaginationofthings
Originally from Egypt, I was inspired massively by the January 2011 revolution and how it prompted an interplay between technical and social systems. I'm now based in Amsterdam, working full-time on the interdisciplinary practice of responsible AI.

Over time, it became clear that data-driven systems aren't only becoming omnipresent but also increasingly invasive and extractive. My background in critical theory made it clear that there are power imbalances that are often amplified by technological systems. Decoding harm and decolonizing technology then aren’t just technical tasks, but rather outcomes of a shared choreography. I then work at the intersection of tech, design, literacy, and play to create this shared choreography. I love working within both academia and industry since my overarching mission is to bridge the theory and practice of data work.

AIxD: That’s really fascinating. Love your remark on a delicate balance and choreography between decoding harm and decolonizing tech. I wonder if there’s any project that you are working on at the moment that touched upon a similar topic?

Abdo: My next project is with AIxDesign, called Everyday Data (H)activism.


In this project, I would like to invite the community of data practitioners/designers/researchers to curate with me ways that would enable the public to engage in more responsible data practices. We’re often reactive as data consumers, unaware of how invasive technologies shape our everyday. The project aims to provide a playful manual for individuals to fight back against information asymmetries.

AIxD: For those who are reading, stay tuned for more updates on the project! As a project lead, I wonder what you find interesting at the AIxDesign crossover?

Abdo: AIxDesign is a much-needed crossover between the technical and human sides of Artificial intelligence. We’re living in a time where the adoption of AI is skyrocketing, only to realize that the tech isn’t as innocent as we once thought. Technical systems never operate in a vacuum. There has been an ever-increasing gap between people who build tech, those that design it, and those affected by it. Platforms like AIxDesign provide fertile ground to fill in those gaps, allowing for data work to be a communal world-building practice.


Image courtesy of @Imaginationofthings
My collaboration with AIxDesign is an intuitive evolution of my practice and a continuation of an earlier project I worked on called Atlas of Algorithmic (in)Equality.

AIxD: Absolutely. As technology becomes more democratized, we will need to provide the platform for conversations like these and connect like-minded people together. Is there any technology or tool you have been using or are excited about?

Abdo: I try to be tool-agnostic as much as possible, but one technology I’ve recently worked with is Augmented Reality using Snap AR.

I collaborated with Imagination of Things on a Poetic AR project, where we worked with poets on turning some of their poetic expressions into playful experiences. In the process, we were able to explore shifting identities, create new senses of space, and create meaning.


Image courtesy of @Imaginationofthings
I find that one of the biggest dilemmas in the field is finding a critical language that allows us to subvert dominant structures. Think of an AR piece that exposes alternate histories of space, or one which helps us explore shifting identities or connections to a colonial past.

I love when tech is used in a way that is beyond solutionist; to show us that different futures are ultimately possible. I often think of Responsible Tech as not only a deconstructive practice but also a constructive and innovative one.

AIxD: To close our chat, could you share one book that has strongly influenced you or that you would recommend to our readers?

Abdo: It's hard to pick one, but it would be a mix of “Data Feminism” by Catherine D’Ignazio and Lauren Klein and “Psychopolitics: Neoliberalism and New Technologies of Power” by Byung-Chul Han.



Left: Sketch notes of Data Feminism book. Right: Image courtesy of After 8 Books
AIxD: Thank you for taking the time to share with our readers. We can’t wait for Everyday Data H(activism) program to begin and are looking forward to sharing the knowledge collaboratively developed during the program